<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>sherwinNG&#39;s blog</title>
  
  <subtitle>遇见更好的自己</subtitle>
  <link href="http://sherwinzhang.com/atom.xml" rel="self"/>
  
  <link href="http://sherwinzhang.com/"/>
  <updated>2022-06-03T08:48:18.424Z</updated>
  <id>http://sherwinzhang.com/</id>
  
  <author>
    <name>sherwin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>目前使用不错软件整理</title>
    <link href="http://sherwinzhang.com/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/"/>
    <id>http://sherwinzhang.com/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/</id>
    <published>2022-06-03T08:44:34.000Z</published>
    <updated>2022-06-03T08:48:18.424Z</updated>
    
    <content type="html"><![CDATA[<h3 id="办公"><a class="markdownIt-Anchor" href="#办公"></a> 办公</h3><ul><li>日事清：计划管理</li><li>screenbrush:屏幕画笔、标记</li><li>pdf阅读器：pdf expert<ul><li>做笔记功能，生态能力（ipad做笔记，可以在其他端查看）</li></ul></li></ul><h3 id="图文笔记"><a class="markdownIt-Anchor" href="#图文笔记"></a> 图文笔记</h3><ul><li>sublime（替代mac文本文档）</li><li>typora（markdown书写）<ul><li>搭配ipic使用（截图上传到云端）</li></ul></li><li>onenote(学习类笔记)</li><li>有道云（写随笔的地方）</li><li>xmind(脑图制作)</li><li>agenda(带分类窗口和时间线的笔记记录平台)</li></ul><h3 id="图片"><a class="markdownIt-Anchor" href="#图片"></a> 图片</h3><ul><li>截图软件：snipaste, 截图效果一般，但是贴图功能非常强大</li></ul><h3 id="影音"><a class="markdownIt-Anchor" href="#影音"></a> 影音</h3><ul><li>视频录制：obs</li><li>视频剪辑:screenflow</li></ul><h3 id="系统"><a class="markdownIt-Anchor" href="#系统"></a> 系统</h3><ul><li>bartender:mac bar管理工具，让你的bar没有那么杂乱的软件</li><li>magnet:屏幕窗口管理：<ul><li>尤其适合多屏用户（分享自己的快捷键）</li></ul></li><li>MonitorControl<ul><li>一键调节外置显示器亮度及音量的软件</li></ul></li></ul><h3 id="网络"><a class="markdownIt-Anchor" href="#网络"></a> 网络</h3><ul><li>浏览器<ul><li>Safari(为了苹果生态使用)</li><li>chrome(当Safari无法发挥作用时使用)</li></ul></li></ul><h3 id="学习"><a class="markdownIt-Anchor" href="#学习"></a> 学习</h3><ul><li>bob:非常好用的翻译软件，支持截图翻译、输入翻译、复制翻译</li></ul><hr><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">苹果生态</span><br><span class="line">iCloud云盘(存储必要文档)</span><br><span class="line">阅读书籍（苹果生态阅读+笔记同步）</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;办公&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#办公&quot;&gt;&lt;/a&gt; 办公&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;日事清：计划管理&lt;/li&gt;&lt;li&gt;screenbrush:屏幕画笔、标记&lt;/li&gt;&lt;li&gt;pdf阅读器：pdf expert&lt;ul&gt;&lt;l</summary>
      
    
    
    
    <category term="apple生态" scheme="http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"/>
    
    
    <category term="software" scheme="http://sherwinzhang.com/tags/software/"/>
    
  </entry>
  
  <entry>
    <title>流言猛如虎</title>
    <link href="http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E6%B5%81%E8%A8%80%E7%8C%9B%E5%A6%82%E8%99%8E/"/>
    <id>http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E6%B5%81%E8%A8%80%E7%8C%9B%E5%A6%82%E8%99%8E/</id>
    <published>2022-05-14T15:41:15.000Z</published>
    <updated>2022-05-14T15:57:15.909Z</updated>
    
    <content type="html"><![CDATA[<p>最近发生的几件事情，作为一个旁观者，让我深刻体会到在信息爆炸的今天，流言的可怕性。暂且在此记录，同时告诫自己以后类似情况应对之法。</p><ul><li>1.本周四下午，突然传出北京市因为疫情原因，新闻发布会将会宣布北京进入静默管理3天。该消息一经流传，就在各大短视频平台迅速蔓延，然后大部分市民都加入了蔬菜等日常物资的抢购潮。但是没过多久，随着疫情新闻发布会的结束，并没有如大家所传需要进入静默期，而是严惩假消息传播的始作俑者。</li><li>2.最近在刷微信小视频或者抖音的时候，总可以发现北大韦神的日常，有他授课的、日常生活被别人拍摄的。其中有一条是几个教授花数月解不开的难题，被韦神几分钟解决。这个视频经过几天发酵后，最后韦东奕亲自澄清，并没有如此事情，只是流言。</li></ul><p>我们每个人都处在信息爆炸的今天，说不定你某个无意的举动，经过蝴蝶效应的发酵，就会把你推上风口浪尖。甚至在你受到别人关注的时候，即使很多和你并不相关的事情，都会被杜撰出来。</p><p>我们暂且不讨论那些杜撰此类消息的人们，为了博人眼球，无所不用其极。我们能做的是，在流言中如何让自己尽可能成为一名“智者”。</p><p>现在的我们，每天都会接受来自外界的很多信息，其传播之广、传播之快，远超常人想象，在这些未经证实的“消息”被推送到我们面前，我们需要做的是：</p><ul><li>1.不做任何消息都照单全收的“接收者”，需要加上自己的判断。</li><li>2.流言止于智者。</li></ul><blockquote><p>于2022年05月14日23:29:33家中</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近发生的几件事情，作为一个旁观者，让我深刻体会到在信息爆炸的今天，流言的可怕性。暂且在此记录，同时告诫自己以后类似情况应对之法。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;1.本周四下午，突然传出北京市因为疫情原因，新闻发布会将会宣布北京进入静默管理3天。该消息一经流传，就在各大短视频平台迅</summary>
      
    
    
    
    <category term="程序人生" scheme="http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"/>
    
    
    <category term="随感" scheme="http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据分割介绍</title>
    <link href="http://sherwinzhang.com/ML/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/"/>
    <id>http://sherwinzhang.com/ML/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-06-12T01:14:21.000Z</published>
    <updated>2022-05-14T05:52:18.114Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习中，我们可<strong>通过实验测试来对学习器的泛化误差进行评估并进而做出选择</strong>。</p><p>为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后<strong>以测试集上的“测试误差” (testing error)作为泛化误差的近似。</strong></p><p>通常我们假设测试样本也是<strong>从样本真实分布中独立同分布采样而得</strong>。但需注意的是，<strong>测试集应该尽可能与训练集互斥。</strong></p><blockquote><p>互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。</p></blockquote><p>测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景:</p><blockquote><p>老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？</p><p>答案是否定的，可能有的同学只会做这10道题却能得高分。</p></blockquote><p>回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，<strong>若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。</strong></p><p>可是，我们只有一个包含m个样例的数据集<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa7rhg07vj30js01iweh.jpg" alt="image-20191226164011364" style="zoom:50%"></p><p>既要训练，又要测试，怎样才能做到呢？</p><ul><li>答案是:<strong>通过对D进行适当的处理，从中产生出训练集S和测试集T。（这个也是我们前面一直在做的事情）。</strong></li></ul><p>下面我们一起总结一下几种常见的做法：</p><ul><li>留出法</li><li>交叉验证法</li><li>自助法</li></ul><hr><h2 id="1-留出法"><a class="markdownIt-Anchor" href="#1-留出法"></a> 1 留出法</h2><p>“留出法”(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazesd4ngwj309401g745.jpg" alt="image-20200117114218755" style="zoom:50%">。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p><p>大家在使用的过程中，<strong>需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响</strong>，例如在分类任务中至少要保持样本的类别比例相似。</p><p>如果从采样( sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为**“分层采样”( stratified sampling)。**</p><blockquote><p>例如通过对D进行分层样而获得含70%样本的训练集S和含30%样本的测试集T，</p><p>若D包含500个正例、500个反例，则分层采样得到的S应包含350个正例、350个反例，而T则包含150个正例和150个反例；</p><p>若S、T中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。</p></blockquote><p>另一个需注意的问题是，即便在给定训练测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。</p><p>例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中，这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。</p><p>因此，单次使用留出法得到的估计结果往往不够稳定可靠，<strong>在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。</strong></p><blockquote><p>例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。</p></blockquote><p>此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:</p><ul><li>若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；</li><li>若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性( fidelity)。</li></ul><p>这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。</p><p>使用Python实现留出法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment">#使用train_test_split划分训练集和测试集</span></span><br><span class="line">train_X , test_X, train_Y ,test_Y = train_test_split(</span><br><span class="line">        X, Y, test_size=<span class="number">0.2</span>,random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>在留出法中，有一个特例，叫：<strong>留一法( Leave-One-Out，简称LOO）</strong>，即每次抽取一个样本做为测试集。</p><p>显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集一每个子集包含个样本；</p><p>使用Python实现留一法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line"></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> loo.split(data):</span><br><span class="line">    print(<span class="string">"%s %s"</span> % (train, test))</span><br><span class="line"><span class="string">'''结果</span></span><br><span class="line"><span class="string">[1 2 3] [0]</span></span><br><span class="line"><span class="string">[0 2 3] [1]</span></span><br><span class="line"><span class="string">[0 1 3] [2]</span></span><br><span class="line"><span class="string">[0 1 2] [3]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>留一法优缺点：</p><p>优点：</p><ul><li>留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，<strong>留一法的评估结果往往被认为比较准确</strong>。</li></ul><p>缺点：</p><ul><li>留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，则需训练1百万个模型，而这还是在未考虑算法调参的情况下。</li></ul><h2 id="2-交叉验证法"><a class="markdownIt-Anchor" href="#2-交叉验证法"></a> 2 交叉验证法</h2><p>“交叉验证法”( cross validation)先将数据集D划分为k个大小相似的互斥子集，即<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazet6n0dwj30ig01g74a.jpg" alt="image-20200117114308300" style="zoom:50%">。每个子集$$D_i$$都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。</p><p>然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</p><p>显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”(k- fold cross validation)。k最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。下图给出了10折交叉验证的示意图。</p><p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa0qilguhj31cg0m2q5k.jpg" alt="image-20191226121521481"></p><p>**与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。**为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有<br>“10次10折交叉验证”。</p><p>交叉验证实现方法，除了咱们前面讲的GridSearchCV之外，还有KFold, StratifiedKFold</p><h3 id="kfold和stratifiedkfold"><a class="markdownIt-Anchor" href="#kfold和stratifiedkfold"></a> KFold和StratifiedKFold</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,StratifiedKFold</span><br></pre></td></tr></table></figure><ul><li><p>用法：</p><ul><li>将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果</li><li>StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。</li></ul></li><li><p>注意点：</p><ul><li>对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本</li></ul></li><li><p>参数说明：</p><ul><li>n_splits：表示划分几等份</li><li>shuffle：在每次划分时，是否进行洗牌<ul><li>①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同</li><li>②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的</li></ul></li></ul></li><li><p>属性：</p><ul><li>①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,StratifiedKFold</span><br><span class="line"></span><br><span class="line">X = np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],</span><br><span class="line">    [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],</span><br><span class="line">    [<span class="number">31</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>],</span><br><span class="line">    [<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>],</span><br><span class="line">    [<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>],</span><br><span class="line">    [<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,<span class="number">64</span>],</span><br><span class="line">    [<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">folder = KFold(n_splits = <span class="number">4</span>, random_state=<span class="number">0</span>, shuffle = <span class="literal">False</span>)</span><br><span class="line">sfolder = StratifiedKFold(n_splits = <span class="number">4</span>, random_state = <span class="number">0</span>, shuffle = <span class="literal">False</span>)</span><br><span class="line">   </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> folder.split(X, y):</span><br><span class="line">    print(<span class="string">'train:%s | test:%s'</span> %(train, test))</span><br><span class="line">    print(<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sfolder.split(X, y):</span><br><span class="line">    print(<span class="string">'train:%s | test:%s'</span>%(train, test))</span><br><span class="line">    print(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个for，输出结果为：</span></span><br><span class="line">train:[<span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>] | test:[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">1</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>] | test:[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">6</span> <span class="number">7</span>] | test:[<span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>] | test:[<span class="number">6</span> <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个for，输出结果为：</span></span><br><span class="line">train:[<span class="number">1</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>] | test:[<span class="number">0</span> <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>] | test:[<span class="number">1</span> <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">7</span>] | test:[<span class="number">4</span> <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">train:[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">6</span>] | test:[<span class="number">5</span> <span class="number">7</span>]</span><br></pre></td></tr></table></figure><p>可以看出，sfold进行4折计算时候，是平衡了测试集中，样本正负的分布的；但是fold却没有。</p><h2 id="3-自助法"><a class="markdownIt-Anchor" href="#3-自助法"></a> 3 自助法</h2><p>我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此**实际评估的模型所使用的训练集比D小，**这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。</p><p>有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？</p><p>“自助法”( bootstrapping)是一个比较好的解决方案，它<strong>直接以自助采样法( bootstrap sampling)为基础</strong>。给定包含m个样本的数据集D，我们对它进行采样产生数据集D:</p><ul><li>每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；</li><li>这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。</li></ul><p>显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率是$$(1-\frac{1}{m})^m$$，取极限得到</p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa4wbuxlvj30ge03m3yq.jpg" alt="image-20191226150103208" style="zoom:50%"><p>即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。</p><p>于是我们可将D′用作训练集，D\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。</p><p>这样的测试结果，亦称**“包外估计”(out- of-bagestimate）**</p><p>自助法优缺点：</p><ul><li>优点：<ul><li>自助法在数据集较小、难以有效划分训练/测试集时很有用；</li><li>此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。</li></ul></li><li>缺点：<ul><li>自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。</li></ul></li></ul><h2 id="4-总结"><a class="markdownIt-Anchor" href="#4-总结"></a> 4 总结</h2><p><strong>综上所述：</strong></p><ul><li>当我们数据量足够时，选择留出法简单省时，在牺牲很小的准确度的情况下，换取计算的简便；</li><li>当我们的数据量较小时，我们应该选择交叉验证法，因为此时划分样本集将会使训练数据过少；</li><li>当我们的数据量特别少的时候，我们可以考虑留一法。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在机器学习中，我们可&lt;strong&gt;通过实验测试来对学习器的泛化误差进行评估并进而做出选择&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后&lt;strong&gt;以测试集上的“测试误差” (testing </summary>
      
    
    
    
    <category term="ML" scheme="http://sherwinzhang.com/categories/ML/"/>
    
    
    <category term="ML" scheme="http://sherwinzhang.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>分类中解决类别不平衡问题</title>
    <link href="http://sherwinzhang.com/ML/ML/%E5%88%86%E7%B1%BB%E4%B8%AD%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/"/>
    <id>http://sherwinzhang.com/ML/ML/%E5%88%86%E7%B1%BB%E4%B8%AD%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/</id>
    <published>2019-06-10T02:24:21.000Z</published>
    <updated>2022-05-14T05:52:14.607Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类中解决类别不平衡问题"><a class="markdownIt-Anchor" href="#分类中解决类别不平衡问题"></a> 分类中解决类别不平衡问题</h1><hr><p>其实，在现实环境中，采集的数据（建模样本）往往是比例失衡的。比如网贷数据，逾期人数的比例是极低的（千分之几的比例）；奢侈品消费人群鉴定等。</p><h2 id="1-类别不平衡数据集基本介绍"><a class="markdownIt-Anchor" href="#1-类别不平衡数据集基本介绍"></a> 1 类别不平衡数据集基本介绍</h2><p>当遇到数据类别不平衡的时候，我们该如何处理。在Python中，有Imblearn包，它就是为处理数据比例失衡而生的。</p><ul><li>安装Imblearn包</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install imbalanced-learn</span><br></pre></td></tr></table></figure><p>第三方包链接：<a href="https://pypi.org/project/imbalanced-learn/" target="_blank" rel="noopener">https://pypi.org/project/imbalanced-learn/</a></p><ul><li>创造数据集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用make_classification生成样本数据</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">5000</span>, </span><br><span class="line">                           n_features=<span class="number">2</span>,  <span class="comment"># 特征个数= n_informative（） + n_redundant + n_repeated </span></span><br><span class="line">                           n_informative=<span class="number">2</span>,  <span class="comment"># 多信息特征的个数</span></span><br><span class="line">                           n_redundant=<span class="number">0</span>,   <span class="comment"># 冗余信息，informative特征的随机线性组合</span></span><br><span class="line">                           n_repeated=<span class="number">0</span>,  <span class="comment"># 重复信息，随机提取n_informative和n_redundant 特征 </span></span><br><span class="line">                           n_classes=<span class="number">3</span>,  <span class="comment"># 分类类别</span></span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>,  <span class="comment"># 某一个类别是由几个cluster构成的</span></span><br><span class="line">                           weights=[<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.94</span>],  <span class="comment"># 列表类型，权重比</span></span><br><span class="line">                           random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>查看各个标签的样本</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看各个标签的样本量</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">Counter(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Counter(&#123;2: 4674, 1: 262, 0: 64&#125;)</span></span><br></pre></td></tr></table></figure><ul><li>数据集可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gae1okhof5j30l20dojth.jpg" alt="image-20191230001202944" style="zoom:50%"><p>可以看出样本的三个标签中，1，2的样本量极少，样本失衡。下面使用imblearn进行过采样。</p><p>接下来，我们就要基于以上数据，进行相应的处理。</p><hr><p>关于类别不平衡的问题，主要有两种处理方式：</p><ul><li>过采样方法<ul><li>增加数量较少那一类样本的数量，使得正负样本比例均衡。</li></ul></li><li>欠采样方法<ul><li>减少数量较多那一类样本的数量，使得正负样本比例均衡。</li></ul></li></ul><h2 id="2-解决类别不平衡数据方法介绍"><a class="markdownIt-Anchor" href="#2-解决类别不平衡数据方法介绍"></a> 2 解决类别不平衡数据方法介绍</h2><h3 id="21-过采样方法"><a class="markdownIt-Anchor" href="#21-过采样方法"></a> 2.1 过采样方法</h3><h4 id="211-什么是过采样方法"><a class="markdownIt-Anchor" href="#211-什么是过采样方法"></a> 2.1.1 <strong>什么是过采样方法</strong></h4><p>对训练集里的少数类进行“过采样”（oversampling），<strong>即增加一些少数类样本使得正、反例数目接近，然后再进行学习。</strong></p><h4 id="212-随机过采样方法"><a class="markdownIt-Anchor" href="#212-随机过采样方法"></a> 2.1.2 <strong>随机过采样方法</strong></h4><p>随机过采样是在少数类 <img src="https://www.zhihu.com/equation?tex=S_%7Bmin%7D" alt="[公式]"> 中随机选择一些样本，然后**通过复制所选择的样本生成样本集 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> ，**将它们添加到 <img src="https://www.zhihu.com/equation?tex=S_%7Bmin%7D" alt="[公式]"> 中来扩大原始数据集从而得到新的少数类集合 <img src="https://www.zhihu.com/equation?tex=S_%7Bnew-min%7D" alt="[公式]"> 。新的数据集 <img src="https://www.zhihu.com/equation?tex=S_%7Bnew-min%7D%3DS_%7Bmin%7D%2BE" alt="[公式]"> 。</p><ul><li>通过代码实现随机过采样方法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用imblearn进行随机过采样</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> RandomOverSampler</span><br><span class="line">ros = RandomOverSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = ros.fit_resample(X, y)</span><br><span class="line"><span class="comment">#查看结果</span></span><br><span class="line">Counter(y_resampled)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#过采样后样本结果</span></span><br><span class="line"><span class="comment"># Counter(&#123;2: 4674, 1: 4674, 0: 4674&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X_resampled[:, <span class="number">0</span>], X_resampled[:, <span class="number">1</span>], c=y_resampled)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gae1va7kupj30la0e0gnf.jpg" alt="image-20191230001829494" style="zoom:50%"><hr><ul><li>缺点：<ul><li>对于随机过采样，由于需要对少数类样本进行复制来扩大数据集，<strong>造成模型训练复杂度加大</strong>。</li><li>另一方面也容易<strong>造成模型的过拟合问题</strong>，因为随机过采样是简单的对初始样本进行复制采样，这就使得学习器学得的规则过于具体化，不利于学习器的泛化性能，造成过拟合问题。</li></ul></li></ul><p>为了解决随机过采样中造成模型过拟合问题，又能保证实现数据集均衡的目的，出现了过采样法代表性的算法SMOTE算法。</p><h4 id="213-过采样代表性算法-smote"><a class="markdownIt-Anchor" href="#213-过采样代表性算法-smote"></a> 2.1.3 <strong>过采样代表性算法-SMOTE</strong></h4><p>SMOTE全称是Synthetic Minority Oversampling即合成少数类过采样技术。</p><p>SMOTE算法是对随机过采样方法的一个改进算法，由于随机过采样方法是直接对少数类进行重采用，会使训练集中有很多重复的样本，容易造成产生的模型过拟合问题。而SMOTE算法的基本思想<strong>是对每个少数类样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> ，从它的最近邻中随机选择一个样本 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> （ <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> 是少数类中的一个样本），然后在 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> 之间的连线上随机选择一点作为新合成的少数类样本。</strong></p><p>SMOTE算法合成新少数类样本的算法描述如下：</p><ul><li><ol><li>对于少数类中的每一个样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> ，以欧氏距离为标准计算它到少数类样本集 <img src="https://www.zhihu.com/equation?tex=S_%7Bmin%7D" alt="[公式]"> 中所有样本的距离，得到其k近邻。</li></ol></li><li><ol start="2"><li>根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> ，从其k近邻中随机选择若干个样本，假设选择的是 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> 。</li></ol></li><li><ol start="3"><li>对于每一个随机选出来的近邻 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> ，分别与 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 按照如下公式构建新的样本。</li></ol></li></ul><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaztjwh1k9j30fa022mx4.jpg" alt="image-20200117201311799" style="zoom:50%"><hr><p>我们用图文表达的方式，再来描述一下SMOTE算法。</p><ol><li>先随机选定一个少数类样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 。</li></ol><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gadzj9rsb6j30cy08gta1.jpg" alt="image-20191229225742800" style="zoom:50%"><ol start="2"><li>找出这个少数类样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 的K个近邻（假设K=5），5个近邻已经被圈出。</li></ol><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gadzjr88hkj30d0086myn.jpg" alt="image-20191229225812157" style="zoom:50%"><ol start="3"><li>随机从这K个近邻中选出一个样本 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> （用绿色圈出来了）。</li></ol><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gadzk53hm3j30c8088jss.jpg" alt="image-20191229225834558" style="zoom:50%"><p>4)在少数类样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 和被选中的这个近邻样本 <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx_%7Bi%7D%7D" alt="[公式]"> 之间的连线上，随机找一点。这个点就是人工合成的新的样本点（绿色正号标出）。</p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gadzkjad2pj30ca07w402.jpg" alt="image-20191229225858077" style="zoom:50%"><p>SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。</p><ul><li>代码实现：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SMOTE过采样</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line">X_resampled, y_resampled = SMOTE().fit_resample(X, y)</span><br><span class="line">Counter(y_resampled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样后样本结果</span></span><br><span class="line"><span class="comment"># [(0, 4674), (1, 4674), (2, 4674)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X_resampled[:, <span class="number">0</span>], X_resampled[:, <span class="number">1</span>], c=y_resampled)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gae1xnw2wvj30lq0e676f.jpg" alt="image-20191230002046690" style="zoom:50%"><h3 id="22-欠采样方法"><a class="markdownIt-Anchor" href="#22-欠采样方法"></a> 2.2 欠采样方法</h3><h4 id="221-什么是欠采样方法"><a class="markdownIt-Anchor" href="#221-什么是欠采样方法"></a> 2.2.1 什么是欠采样方法</h4><p>直接对训练集中多数类样本进行“欠采样”（undersampling），即去<strong>除一些多数类中的样本使得正例、反例数目接近，然后再进行学习。</strong></p><h4 id="222-随机欠采样方法"><a class="markdownIt-Anchor" href="#222-随机欠采样方法"></a> 2.2.2 随机欠采样方法</h4><p>随机欠采样顾名思义即从多数类 <img src="https://www.zhihu.com/equation?tex=S_%7Bmaj%7D" alt="[公式]"> 中随机选择一些样样本组成样本集 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 。然后将样本集 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 从 <img src="https://www.zhihu.com/equation?tex=S_%7Bmaj%7D" alt="[公式]"> 中移除。新的数据集 <img src="https://www.zhihu.com/equation?tex=S_%7Bnew-maj%7D%3DS_%7Bmaj%7D-E" alt="[公式]"> 。</p><ul><li>代码实现：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机欠采样</span></span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> RandomUnderSampler</span><br><span class="line">rus = RandomUnderSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = rus.fit_resample(X, y)</span><br><span class="line">Counter(y_resampled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样后结果</span></span><br><span class="line">[(<span class="number">0</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>), (<span class="number">2</span>, <span class="number">64</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X_resampled[:, <span class="number">0</span>], X_resampled[:, <span class="number">1</span>], c=y_resampled)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gae20fk2zyj30lu0ds0v4.jpg" alt="image-20191230002326732" style="zoom:50%"><hr><ul><li>缺点：<ul><li>随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但是这也存在一些问题。对于随机欠采样，<strong>由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。</strong></li></ul></li></ul><hr><p>官网链接：<a href="https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html" target="_blank" rel="noopener">https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;分类中解决类别不平衡问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#分类中解决类别不平衡问题&quot;&gt;&lt;/a&gt; 分类中解决类别不平衡问题&lt;/h1&gt;&lt;hr&gt;&lt;p&gt;其实，在现实环境中，采集的数据（建模样本）往往是比例失衡的。比如网贷数据，逾期</summary>
      
    
    
    
    <category term="ML" scheme="http://sherwinzhang.com/categories/ML/"/>
    
    
    <category term="ML" scheme="http://sherwinzhang.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>独立同分布</title>
    <link href="http://sherwinzhang.com/ML/ML/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83/"/>
    <id>http://sherwinzhang.com/ML/ML/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83/</id>
    <published>2019-06-08T02:24:21.000Z</published>
    <updated>2022-05-14T05:52:33.600Z</updated>
    
    <content type="html"><![CDATA[<h1 id="独立同分布iidindependent-and-identically-distributed"><a class="markdownIt-Anchor" href="#独立同分布iidindependent-and-identically-distributed"></a> 独立同分布IID(independent and identically distributed)</h1><h2 id="1-独立同分布iid"><a class="markdownIt-Anchor" href="#1-独立同分布iid"></a> 1 独立同分布(i.i.d.)</h2><p>在概率统计理论中，<strong>如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。</strong></p><p>在西瓜书中解释是：<strong>输入空间中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。</strong></p><h2 id="2-简单解释-独立-同分布-独立同分布"><a class="markdownIt-Anchor" href="#2-简单解释-独立-同分布-独立同分布"></a> 2 简单解释 — 独立、同分布、独立同分布</h2><p>（1）<strong>独立</strong>：每次抽样之间没有关系，不会相互影响</p><p>举例：给一个骰子，每次抛骰子抛到几就是几，这是独立；如果我要抛骰子两次之和大于8，那么第一次和第二次抛就不独立，因为第二次抛的结果和第一次相关。</p><p>（2）<strong>同分布</strong>：每次抽样，样本服从同一个分布</p><p>举例：给一个骰子，每次抛骰子得到任意点数的概率都是六分之一，这个就是同分布</p><p>（3）<strong>独立同分布</strong>：i.i.d.，每次抽样之间独立而且同分布</p><h2 id="3-机器学习领域的重要假设"><a class="markdownIt-Anchor" href="#3-机器学习领域的重要假设"></a> 3 机器学习领域的重要假设</h2><p>IID独立同分布即假设<strong>训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。</strong></p><h2 id="4-目前发展"><a class="markdownIt-Anchor" href="#4-目前发展"></a> 4 目前发展</h2><p>机器学习并不总要求独立同分布，在不少问题中要求样本数据采样自同一个分布是因为希望用训练数据集得到的模型可以合理的用于测试数据集，使用独立同分布假设能够解释得通。</p><p>目前<strong>一些机器学习内容已经不再囿于独立同分布假设下，一些问题会假设样本没有同分布</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;独立同分布iidindependent-and-identically-distributed&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#独立同分布iidindependent-and-identically-distributed</summary>
      
    
    
    
    <category term="ML" scheme="http://sherwinzhang.com/categories/ML/"/>
    
    
    <category term="ML" scheme="http://sherwinzhang.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>完整机器学习项目的流程</title>
    <link href="http://sherwinzhang.com/ML/ML/%E5%AE%8C%E6%95%B4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
    <id>http://sherwinzhang.com/ML/ML/%E5%AE%8C%E6%95%B4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%B5%81%E7%A8%8B/</id>
    <published>2019-06-04T01:14:21.000Z</published>
    <updated>2022-05-14T07:25:01.121Z</updated>
    
    <content type="html"><![CDATA[<h1 id="完整机器学习项目的流程"><a class="markdownIt-Anchor" href="#完整机器学习项目的流程"></a> 完整机器学习项目的流程</h1><h4 id="1-抽象成数学问题"><a class="markdownIt-Anchor" href="#1-抽象成数学问题"></a> <strong>1</strong> <strong>抽象成数学问题</strong></h4><p>明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。</p><p>这里的抽象成数学问题，指的明确我们可以获得什么样的数据，抽象出的问题，是一个分类还是回归或者是聚类的问题。</p><h4 id="2-获取数据"><a class="markdownIt-Anchor" href="#2-获取数据"></a> <strong>2</strong> <strong>获取数据</strong></h4><p>数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。</p><p>数据要有代表性，否则必然会过拟合。</p><p>而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数量级的差距。</p><p>而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。</p><h4 id="3-特征预处理与特征选择"><a class="markdownIt-Anchor" href="#3-特征预处理与特征选择"></a> <strong>3</strong> <strong>特征预处理与特征选择</strong></h4><p>良好的数据要能够提取出良好的特征才能真正发挥作用。</p><p>特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。</p><p>筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。</p><h4 id="4-训练模型与调优"><a class="markdownIt-Anchor" href="#4-训练模型与调优"></a> <strong>4</strong> <strong>训练模型与调优</strong></h4><p>直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。</p><h4 id="5-模型诊断"><a class="markdownIt-Anchor" href="#5-模型诊断"></a> <strong>5</strong> <strong>模型诊断</strong></h4><p>如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。</p><p>过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。</p><p>误差分析 也是机器学习至关重要的步骤。通过观察误差样本全面分析产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……</p><p>诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。</p><h4 id="6-模型融合"><a class="markdownIt-Anchor" href="#6-模型融合"></a> <strong>6</strong> <strong>模型融合</strong></h4><p>一般来说，模型融合后都能使得效果有一定提升。而且效果很好。</p><p>工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。</p><h4 id="7-上线运行"><a class="markdownIt-Anchor" href="#7-上线运行"></a> <strong>7</strong> <strong>上线运行</strong></h4><p>这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。</p><p>这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;完整机器学习项目的流程&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#完整机器学习项目的流程&quot;&gt;&lt;/a&gt; 完整机器学习项目的流程&lt;/h1&gt;&lt;h4 id=&quot;1-抽象成数学问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; h</summary>
      
    
    
    
    <category term="ML" scheme="http://sherwinzhang.com/categories/ML/"/>
    
    
    <category term="ML" scheme="http://sherwinzhang.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>2019-春节假期生活杂感</title>
    <link href="http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/2019%E5%B9%B4%E6%98%A5%E8%8A%82%E5%81%87%E6%9C%9F%E7%94%9F%E6%B4%BB%E6%9D%82%E6%84%9F/"/>
    <id>http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/2019%E5%B9%B4%E6%98%A5%E8%8A%82%E5%81%87%E6%9C%9F%E7%94%9F%E6%B4%BB%E6%9D%82%E6%84%9F/</id>
    <published>2019-02-05T05:32:15.000Z</published>
    <updated>2022-05-14T16:04:47.503Z</updated>
    
    <content type="html"><![CDATA[<p>又是和往年一样的春节放假，依旧是常规的操作，放假回来，在家懒着，然后腊月底和爸一起去上坟，最后几天贴春联，挂灯笼等操作。</p><hr><ul><li><strong>【三十而立，面包杂想】</strong></li></ul><p>现在是腊三十，不管从阳历还是阴历，自己总算要又长一岁了，虚岁要步入三十大关。</p><p>古代三十而立，在此插入，非常有必要解释一下，到底什么是三十而立</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">孔子所说的“三十而立”，是指他在这个时候做事合于礼，言行都很得当。 [1] </span><br><span class="line">言：谦卑 中 传递祥和！</span><br><span class="line">行：举止 彬彬有礼！</span><br><span class="line">现常用来指人开始有所成就。</span><br><span class="line">三十而立：三十岁的时候就可以自立于世。</span><br></pre></td></tr></table></figure><p>2019-02-05(正月初一)</p><p>接着上次的内容继续扯东扯西，三十而立，就自己理解，立的内容是两方面吧，一个是精神层面的面包，一个是真实世界的面包。关于精神层面，具体而言，自从从事计算机相关的工作，这个并没有什么大的进步，关于真实世界的面包，可能限于环境的限制，更多的考虑的是这方面的内容吧，有房吗，有车吗。。。</p><p>有时候醒悟，面包不仅仅一直局限于真实世界的单一口味，还需要充实自己的精神世界。最好的目标彼岸，真实环境，精神层面都有面包收获，这应该是自己期望达到的目标吧。</p><hr><ul><li><strong>【2019-沉淀再想】</strong></li></ul><p>这个假期，说实在的，过得不是那么心安理得，因为有很多事情需要自己去完成，但是都没有去行动，比如一直说要去完成的那篇论文，比如说要准备机器学习，再比如说对自己人工智能的充电。就那么没心没肺的过着，但是当回想的时候，还是蛮有愧疚感的。可能春节回家继续办公、学习，这件事情本身就比较反人类，哈哈。既然回家之前把目标确定了，那么就在接下来的这几天努力尽量完成目标，别加深自己的罪恶感。</p><p>关于假期目标，其实是希望达到高度的折射，也从另一方面说明了2019必须干的事情。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如何沉淀自己人工智能相关领域的知识？</span><br><span class="line">需要自己计划好，然后去坚持完成</span><br></pre></td></tr></table></figure><p>关于2019的基调，需要主要精力去做的事情，都需要自己定位好，然后去严格的执行，只有一步步的去执行，才能在最后获取到相应的高度，加油！！</p><hr><ul><li><strong>【关于潮流变化】</strong></li></ul><p>假期回家，被父母每天观看的快手充斥着，自己也为了体验一下究竟什么样产品可以勾住几乎所有人，下载浏览，快手给我带来更大的吃惊之处是，我妈竟然在昨天从快手中提出20元，这个是她这段时间直播的收入，瞬间觉得，这个社会发展的和自己之前的理解有些偏差。这个社会在追求快速发展的同时也失去了一些东西。</p><p>之前自己是一直不下载观看快手之类的app的，因为觉得里面的内容，大部分都没有营养，看的过程中，也就是当时博君一笑，尔尔。但是当这个东西还可以赚钱，在利益的驱动下，是否会有更多没有营养的内容被鼓励产生。</p><p>产品的制作，宣传已经没有了底线可言，怎么让产品更符合大众，更能吊起大众的胃口，怎么设计。如此产品，怎么经得起深究思考呢？</p><p>或者，产本设计者自己就没有设计一个底线，只是在追求利益。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;又是和往年一样的春节放假，依旧是常规的操作，放假回来，在家懒着，然后腊月底和爸一起去上坟，最后几天贴春联，挂灯笼等操作。&lt;/p&gt;&lt;hr&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;【三十而立，面包杂想】&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在是腊三十，不管从阳历还是阴历，自己总</summary>
      
    
    
    
    <category term="程序人生" scheme="http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"/>
    
    
    <category term="随感" scheme="http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
</feed>
