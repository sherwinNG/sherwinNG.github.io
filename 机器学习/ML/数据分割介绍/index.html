<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="sherwin"><meta name="keywords" content=""><meta name="description" content="在机器学习中，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。互斥，即测试样本尽量不在训练集中出现、未在训练过程中使"><meta property="og:type" content="article"><meta property="og:title" content="数据分割介绍"><meta property="og:url" content="http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/index.html"><meta property="og:site_name" content="sherwinNG&#39;s blog"><meta property="og:description" content="在机器学习中，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。互斥，即测试样本尽量不在训练集中出现、未在训练过程中使"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gaa7rhg07vj30js01iweh.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1gazesd4ngwj309401g745.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1gazet6n0dwj30ig01g74a.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gaa0qilguhj31cg0m2q5k.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gaa4wbuxlvj30ge03m3yq.jpg"><meta property="article:published_time" content="2021-06-12T01:14:21.000Z"><meta property="article:modified_time" content="2022-06-04T03:47:45.895Z"><meta property="article:author" content="sherwin"><meta property="article:tag" content="ML"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gaa7rhg07vj30js01iweh.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><title>数据分割介绍 - sherwinNG&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"sherwinzhang.com",root:"/",version:"1.9.3",typing:{enable:!0,typeSpeed:70,cursorChar:"--",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="sherwinNG's blog" type="application/atom+xml"></head><body><header><div class="header-inner" style="height:40vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>【文言】</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> 友链</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><i class="iconfont icon-books"></i> 文档</a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="https://hexo.fluid-dev.com/" target="_blank" rel="noopener">主题博客 </a><a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/guide/" target="_blank" rel="noopener">配置指南 </a><a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/icon/" target="_blank" rel="noopener">图标用法</a></div></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="数据分割介绍"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-06-12 09:14" pubdate>2021年6月12日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 25 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">数据分割介绍</h1><div class="markdown-body"><p>在机器学习中，我们可<strong>通过实验测试来对学习器的泛化误差进行评估并进而做出选择</strong>。</p><p>为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后<strong>以测试集上的“测试误差” (testing error)作为泛化误差的近似。</strong></p><p>通常我们假设测试样本也是<strong>从样本真实分布中独立同分布采样而得</strong>。但需注意的是，<strong>测试集应该尽可能与训练集互斥。</strong></p><blockquote><p>互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。</p></blockquote><p>测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景:</p><blockquote><p>老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？</p><p>答案是否定的，可能有的同学只会做这10道题却能得高分。</p></blockquote><p>回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，<strong>若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。</strong></p><p>可是，我们只有一个包含m个样例的数据集<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa7rhg07vj30js01iweh.jpg" srcset="/img/loading.gif" lazyload alt="image-20191226164011364" style="zoom:50%"></p><p>既要训练，又要测试，怎样才能做到呢？</p><ul><li>答案是:<strong>通过对D进行适当的处理，从中产生出训练集S和测试集T。（这个也是我们前面一直在做的事情）。</strong></li></ul><p>下面我们一起总结一下几种常见的做法：</p><ul><li>留出法</li><li>交叉验证法</li><li>自助法</li></ul><hr><h2 id="1-留出法"><a class="markdownIt-Anchor" href="#1-留出法"></a> 1 留出法</h2><p>“留出法”(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazesd4ngwj309401g745.jpg" srcset="/img/loading.gif" lazyload alt="image-20200117114218755" style="zoom:50%">。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p><p>大家在使用的过程中，<strong>需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响</strong>，例如在分类任务中至少要保持样本的类别比例相似。</p><p>如果从采样( sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为**“分层采样”( stratified sampling)。**</p><blockquote><p>例如通过对D进行分层样而获得含70%样本的训练集S和含30%样本的测试集T，</p><p>若D包含500个正例、500个反例，则分层采样得到的S应包含350个正例、350个反例，而T则包含150个正例和150个反例；</p><p>若S、T中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。</p></blockquote><p>另一个需注意的问题是，即便在给定训练测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。</p><p>例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中，这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。</p><p>因此，单次使用留出法得到的估计结果往往不够稳定可靠，<strong>在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。</strong></p><blockquote><p>例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。</p></blockquote><p>此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:</p><ul><li>若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；</li><li>若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性( fidelity)。</li></ul><p>这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。</p><p>使用Python实现留出法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-comment">#使用train_test_split划分训练集和测试集</span><br>train_X , test_X, train_Y ,test_Y = train_test_split(<br>        X, Y, test_size=<span class="hljs-number">0.2</span>,random_state=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>在留出法中，有一个特例，叫：<strong>留一法( Leave-One-Out，简称LOO）</strong>，即每次抽取一个样本做为测试集。</p><p>显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集一每个子集包含个样本；</p><p>使用Python实现留一法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> LeaveOneOut<br><br>data = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>loo = LeaveOneOut()<br><span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> loo.split(data):<br>    print(<span class="hljs-string">"%s %s"</span> % (train, test))<br><span class="hljs-string">'''结果</span><br><span class="hljs-string">[1 2 3] [0]</span><br><span class="hljs-string">[0 2 3] [1]</span><br><span class="hljs-string">[0 1 3] [2]</span><br><span class="hljs-string">[0 1 2] [3]</span><br><span class="hljs-string">'''</span><br></code></pre></td></tr></table></figure><p>留一法优缺点：</p><p>优点：</p><ul><li>留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，<strong>留一法的评估结果往往被认为比较准确</strong>。</li></ul><p>缺点：</p><ul><li>留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，则需训练1百万个模型，而这还是在未考虑算法调参的情况下。</li></ul><h2 id="2-交叉验证法"><a class="markdownIt-Anchor" href="#2-交叉验证法"></a> 2 交叉验证法</h2><p>“交叉验证法”( cross validation)先将数据集D划分为k个大小相似的互斥子集，即<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazet6n0dwj30ig01g74a.jpg" srcset="/img/loading.gif" lazyload alt="image-20200117114308300" style="zoom:50%">。每个子集$$D_i$$都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。</p><p>然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</p><p>显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”(k- fold cross validation)。k最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。下图给出了10折交叉验证的示意图。</p><p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa0qilguhj31cg0m2q5k.jpg" srcset="/img/loading.gif" lazyload alt="image-20191226121521481"></p><p>**与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。**为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有<br>“10次10折交叉验证”。</p><p>交叉验证实现方法，除了咱们前面讲的GridSearchCV之外，还有KFold, StratifiedKFold</p><h3 id="kfold和stratifiedkfold"><a class="markdownIt-Anchor" href="#kfold和stratifiedkfold"></a> KFold和StratifiedKFold</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold,StratifiedKFold<br></code></pre></td></tr></table></figure><ul><li><p>用法：</p><ul><li>将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果</li><li>StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。</li></ul></li><li><p>注意点：</p><ul><li>对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本</li></ul></li><li><p>参数说明：</p><ul><li>n_splits：表示划分几等份</li><li>shuffle：在每次划分时，是否进行洗牌<ul><li>①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同</li><li>②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的</li></ul></li></ul></li><li><p>属性：</p><ul><li>①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold,StratifiedKFold<br><br>X = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">13</span>,<span class="hljs-number">14</span>],<br>    [<span class="hljs-number">21</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>,<span class="hljs-number">24</span>],<br>    [<span class="hljs-number">31</span>,<span class="hljs-number">32</span>,<span class="hljs-number">33</span>,<span class="hljs-number">34</span>],<br>    [<span class="hljs-number">41</span>,<span class="hljs-number">42</span>,<span class="hljs-number">43</span>,<span class="hljs-number">44</span>],<br>    [<span class="hljs-number">51</span>,<span class="hljs-number">52</span>,<span class="hljs-number">53</span>,<span class="hljs-number">54</span>],<br>    [<span class="hljs-number">61</span>,<span class="hljs-number">62</span>,<span class="hljs-number">63</span>,<span class="hljs-number">64</span>],<br>    [<span class="hljs-number">71</span>,<span class="hljs-number">72</span>,<span class="hljs-number">73</span>,<span class="hljs-number">74</span>]<br>])<br><br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><br>folder = KFold(n_splits = <span class="hljs-number">4</span>, random_state=<span class="hljs-number">0</span>, shuffle = <span class="hljs-literal">False</span>)<br>sfolder = StratifiedKFold(n_splits = <span class="hljs-number">4</span>, random_state = <span class="hljs-number">0</span>, shuffle = <span class="hljs-literal">False</span>)<br>   <br><span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> folder.split(X, y):<br>    print(<span class="hljs-string">'train:%s | test:%s'</span> %(train, test))<br>    print(<span class="hljs-string">""</span>)<br>    <br><span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> sfolder.split(X, y):<br>    print(<span class="hljs-string">'train:%s | test:%s'</span>%(train, test))<br>    print(<span class="hljs-string">""</span>)<br></code></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一个for，输出结果为：</span><br>train:[<span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">0</span> <span class="hljs-number">1</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>] | test:[<span class="hljs-number">6</span> <span class="hljs-number">7</span>]<br><br><span class="hljs-comment"># 第二个for，输出结果为：</span><br>train:[<span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">0</span> <span class="hljs-number">2</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">1</span> <span class="hljs-number">3</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">7</span>] | test:[<span class="hljs-number">4</span> <span class="hljs-number">6</span>]<br><br>train:[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">6</span>] | test:[<span class="hljs-number">5</span> <span class="hljs-number">7</span>]<br></code></pre></td></tr></table></figure><p>可以看出，sfold进行4折计算时候，是平衡了测试集中，样本正负的分布的；但是fold却没有。</p><h2 id="3-自助法"><a class="markdownIt-Anchor" href="#3-自助法"></a> 3 自助法</h2><p>我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此**实际评估的模型所使用的训练集比D小，**这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。</p><p>有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？</p><p>“自助法”( bootstrapping)是一个比较好的解决方案，它<strong>直接以自助采样法( bootstrap sampling)为基础</strong>。给定包含m个样本的数据集D，我们对它进行采样产生数据集D:</p><ul><li>每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；</li><li>这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。</li></ul><p>显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率是$$(1-\frac{1}{m})^m$$，取极限得到</p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaa4wbuxlvj30ge03m3yq.jpg" srcset="/img/loading.gif" lazyload alt="image-20191226150103208" style="zoom:50%"><p>即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。</p><p>于是我们可将D′用作训练集，D\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。</p><p>这样的测试结果，亦称**“包外估计”(out- of-bagestimate）**</p><p>自助法优缺点：</p><ul><li>优点：<ul><li>自助法在数据集较小、难以有效划分训练/测试集时很有用；</li><li>此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。</li></ul></li><li>缺点：<ul><li>自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。</li></ul></li></ul><h2 id="4-总结"><a class="markdownIt-Anchor" href="#4-总结"></a> 4 总结</h2><p><strong>综上所述：</strong></p><ul><li>当我们数据量足够时，选择留出法简单省时，在牺牲很小的准确度的情况下，换取计算的简便；</li><li>当我们的数据量较小时，我们应该选择交叉验证法，因为此时划分样本集将会使训练数据过少；</li><li>当我们的数据量特别少的时候，我们可以考虑留一法。</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/ML/">#ML</a></div></div><div class="license-box my-3"><div class="license-title"><div>数据分割介绍</div><div>http://sherwinzhang.com/机器学习/ML/数据分割介绍/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>sherwin</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年6月12日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/tmux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" title="tmux命令总结"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">tmux命令总结</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/DeepFM/" title="DeepFM"><span class="hidden-mobile">DeepFM</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="lv-container" data-id="city" data-uid="MTAyMC81NzExNy8zMzU4MQ=="><script type="text/javascript">Fluid.utils.loadComments("#lv-container",function(){Fluid.utils.createScript("https://cdn-city.livere.com/js/embed.dist.js")})</script><noscript>Please enable JavaScript to view the comments</noscript></div></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing;(t=t.getElementById("subtitle"))&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible"))}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>