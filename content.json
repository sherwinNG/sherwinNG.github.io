{"meta":{"title":"sherwinNG's blog","subtitle":"遇见更好的自己","description":"The way of the future!","author":"sherwin","url":"http://sherwinzhang.com","root":"/"},"pages":[{"title":"404","date":"2020-02-19T01:33:16.829Z","updated":"2020-02-19T01:33:16.829Z","comments":true,"path":"404.html","permalink":"http://sherwinzhang.com/404.html","excerpt":"","text":"这是网页标题"},{"title":"","date":"2022-06-03T07:16:01.465Z","updated":"2022-06-03T07:15:12.823Z","comments":true,"path":"google13fc42b677fff64a.html","permalink":"http://sherwinzhang.com/google13fc42b677fff64a.html","excerpt":"","text":"google-site-verification: google13fc42b677fff64a.html"},{"title":"关于","date":"2022-06-03T16:22:32.664Z","updated":"2022-05-13T09:28:31.824Z","comments":false,"path":"about/index.html","permalink":"http://sherwinzhang.com/about/index.html","excerpt":"","text":"个人详细介绍12&#123;你好&#125;"},{"title":"分类","date":"2022-06-03T16:22:28.226Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"categories/index.html","permalink":"http://sherwinzhang.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-06-03T16:22:17.542Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"tags/index.html","permalink":"http://sherwinzhang.com/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-06-03T16:22:23.877Z","updated":"2022-03-21T07:18:46.000Z","comments":true,"path":"links/index.html","permalink":"http://sherwinzhang.com/links/index.html","excerpt":"","text":""},{"title":"书单","date":"2022-06-03T16:22:30.631Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"books/index.html","permalink":"http://sherwinzhang.com/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-06-03T16:22:20.686Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"repository/index.html","permalink":"http://sherwinzhang.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"目前使用不错软件整理","slug":"apple生态/目前使用不错软件整理","date":"2022-06-03T08:44:34.000Z","updated":"2022-06-03T16:22:54.587Z","comments":true,"path":"apple生态/apple生态/目前使用不错软件整理/","link":"","permalink":"http://sherwinzhang.com/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/","excerpt":"","text":"办公日事清：计划管理screenbrush:屏幕画笔、标记pdf阅读器：pdf expert做笔记功能，生态能力（ipad做笔记，可以在其他端查看） 图文笔记sublime（替代mac文本文档）typora（markdown书写）搭配ipic使用（截图上传到云端）onenote(学习类笔记)有道云（写随笔的地方）xmind(脑图制作)agenda(带分类窗口和时间线的笔记记录平台) 图片截图软件：snipaste, 截图效果一般，但是贴图功能非常强大 影音视频录制：obs视频剪辑:screenflow 系统bartender:mac bar管理工具，让你的bar没有那么杂乱的软件magnet:屏幕窗口管理：尤其适合多屏用户（分享自己的快捷键）MonitorControl一键调节外置显示器亮度及音量的软件 网络浏览器Safari(为了苹果生态使用)chrome(当Safari无法发挥作用时使用) 学习bob:非常好用的翻译软件，支持截图翻译、输入翻译、复制翻译123苹果生态 iCloud云盘(存储必要文档) 阅读书籍（苹果生态阅读+笔记同步）","categories":[{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"}],"tags":[{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"}]},{"title":"流言猛如虎","slug":"程序人生/流言猛如虎","date":"2022-05-14T15:41:15.000Z","updated":"2022-06-04T03:48:10.861Z","comments":true,"path":"程序人生/程序人生/流言猛如虎/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E6%B5%81%E8%A8%80%E7%8C%9B%E5%A6%82%E8%99%8E/","excerpt":"","text":"最近发生的几件事情，作为一个旁观者，让我深刻体会到在信息爆炸的今天，流言的可怕性。暂且在此记录，同时告诫自己以后类似情况应对之法。1.本周四下午，突然传出北京市因为疫情原因，新闻发布会将会宣布北京进入静默管理3天。该消息一经流传，就在各大短视频平台迅速蔓延，然后大部分市民都加入了蔬菜等日常物资的抢购潮。但是没过多久，随着疫情新闻发布会的结束，并没有如大家所传需要进入静默期，而是严惩假消息传播的始作俑者。2.最近在刷微信小视频或者抖音的时候，总可以发现北大韦神的日常，有他授课的、日常生活被别人拍摄的。其中有一条是几个教授花数月解不开的难题，被韦神几分钟解决。这个视频经过几天发酵后，最后韦东奕亲自澄清，并没有如此事情，只是流言。我们每个人都处在信息爆炸的今天，说不定你某个无意的举动，经过蝴蝶效应的发酵，就会把你推上风口浪尖。甚至在你受到别人关注的时候，即使很多和你并不相关的事情，都会被杜撰出来。我们暂且不讨论那些杜撰此类消息的人们，为了博人眼球，无所不用其极。我们能做的是，在流言中如何让自己尽可能成为一名“智者”。现在的我们，每天都会接受来自外界的很多信息，其传播之广、传播之快，远超常人想象，在这些未经证实的“消息”被推送到我们面前，我们需要做的是：1.不做任何消息都照单全收的“接收者”，需要加上自己的判断。2.流言止于智者。于2022年05月14日23:29:33家中","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]},{"title":"数据分割介绍","slug":"ML/数据分割介绍","date":"2021-06-12T01:14:21.000Z","updated":"2022-06-04T03:47:45.895Z","comments":true,"path":"机器学习/ML/数据分割介绍/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"在机器学习中，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景:老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？答案是否定的，可能有的同学只会做这10道题却能得高分。回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。可是，我们只有一个包含m个样例的数据集既要训练，又要测试，怎样才能做到呢？答案是:通过对D进行适当的处理，从中产生出训练集S和测试集T。（这个也是我们前面一直在做的事情）。下面我们一起总结一下几种常见的做法：留出法交叉验证法自助法 1 留出法“留出法”(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。大家在使用的过程中，需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。如果从采样( sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为**“分层采样”( stratified sampling)。**例如通过对D进行分层样而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。另一个需注意的问题是，即便在给定训练测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中，这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。因此，单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性( fidelity)。这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。使用Python实现留出法：1234from sklearn.model_selection import train_test_split#使用train_test_split划分训练集和测试集train_X , test_X, train_Y ,test_Y = train_test_split( X, Y, test_size=0.2,random_state=0)在留出法中，有一个特例，叫：留一法( Leave-One-Out，简称LOO），即每次抽取一个样本做为测试集。显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集一每个子集包含个样本；使用Python实现留一法：123456789101112from sklearn.model_selection import LeaveOneOutdata = [1, 2, 3, 4]loo = LeaveOneOut()for train, test in loo.split(data): print(\"%s %s\" % (train, test))'''结果[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3]'''留一法优缺点：优点：留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，留一法的评估结果往往被认为比较准确。缺点：留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，则需训练1百万个模型，而这还是在未考虑算法调参的情况下。 2 交叉验证法“交叉验证法”( cross validation)先将数据集D划分为k个大小相似的互斥子集，即。每个子集$$D_i$$都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”(k- fold cross validation)。k最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。下图给出了10折交叉验证的示意图。**与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。**为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有“10次10折交叉验证”。交叉验证实现方法，除了咱们前面讲的GridSearchCV之外，还有KFold, StratifiedKFold KFold和StratifiedKFold1from sklearn.model_selection import KFold,StratifiedKFold用法：将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。注意点：对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本参数说明：n_splits：表示划分几等份shuffle：在每次划分时，是否进行洗牌①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的属性：①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器1234567891011121314151617181920212223242526import numpy as npfrom sklearn.model_selection import KFold,StratifiedKFoldX = np.array([ [1,2,3,4], [11,12,13,14], [21,22,23,24], [31,32,33,34], [41,42,43,44], [51,52,53,54], [61,62,63,64], [71,72,73,74]])y = np.array([1,1,0,0,1,1,0,0])folder = KFold(n_splits = 4, random_state=0, shuffle = False)sfolder = StratifiedKFold(n_splits = 4, random_state = 0, shuffle = False) for train, test in folder.split(X, y): print('train:%s | test:%s' %(train, test)) print(\"\") for train, test in sfolder.split(X, y): print('train:%s | test:%s'%(train, test)) print(\"\")结果：1234567891011121314151617# 第一个for，输出结果为：train:[2 3 4 5 6 7] | test:[0 1]train:[0 1 4 5 6 7] | test:[2 3]train:[0 1 2 3 6 7] | test:[4 5]train:[0 1 2 3 4 5] | test:[6 7]# 第二个for，输出结果为：train:[1 3 4 5 6 7] | test:[0 2]train:[0 2 4 5 6 7] | test:[1 3]train:[0 1 2 3 5 7] | test:[4 6]train:[0 1 2 3 4 6] | test:[5 7]可以看出，sfold进行4折计算时候，是平衡了测试集中，样本正负的分布的；但是fold却没有。 3 自助法我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此**实际评估的模型所使用的训练集比D小，**这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？“自助法”( bootstrapping)是一个比较好的解决方案，它直接以自助采样法( bootstrap sampling)为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D:每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率是$$(1-\\frac{1}{m})^m$$，取极限得到即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。于是我们可将D′用作训练集，D\\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称**“包外估计”(out- of-bagestimate）**自助法优缺点：优点：自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。 4 总结综上所述：当我们数据量足够时，选择留出法简单省时，在牺牲很小的准确度的情况下，换取计算的简便；当我们的数据量较小时，我们应该选择交叉验证法，因为此时划分样本集将会使训练数据过少；当我们的数据量特别少的时候，我们可以考虑留一法。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"分类中解决类别不平衡问题","slug":"ML/分类中解决类别不平衡问题","date":"2019-06-10T02:24:21.000Z","updated":"2022-06-04T03:47:40.240Z","comments":true,"path":"机器学习/ML/分类中解决类别不平衡问题/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%88%86%E7%B1%BB%E4%B8%AD%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/","excerpt":"","text":"分类中解决类别不平衡问题其实，在现实环境中，采集的数据（建模样本）往往是比例失衡的。比如网贷数据，逾期人数的比例是极低的（千分之几的比例）；奢侈品消费人群鉴定等。 1 类别不平衡数据集基本介绍当遇到数据类别不平衡的时候，我们该如何处理。在Python中，有Imblearn包，它就是为处理数据比例失衡而生的。安装Imblearn包1pip3 install imbalanced-learn第三方包链接：https://pypi.org/project/imbalanced-learn/创造数据集12345678910111213from sklearn.datasets import make_classificationimport matplotlib.pyplot as plt#使用make_classification生成样本数据X, y = make_classification(n_samples=5000, n_features=2, # 特征个数= n_informative（） + n_redundant + n_repeated n_informative=2, # 多信息特征的个数 n_redundant=0, # 冗余信息，informative特征的随机线性组合 n_repeated=0, # 重复信息，随机提取n_informative和n_redundant 特征 n_classes=3, # 分类类别 n_clusters_per_class=1, # 某一个类别是由几个cluster构成的 weights=[0.01, 0.05, 0.94], # 列表类型，权重比 random_state=0)查看各个标签的样本12345#查看各个标签的样本量from collections import CounterCounter(y)# Counter(&#123;2: 4674, 1: 262, 0: 64&#125;)数据集可视化123# 数据集可视化plt.scatter(X[:, 0], X[:, 1], c=y)plt.show()可以看出样本的三个标签中，1，2的样本量极少，样本失衡。下面使用imblearn进行过采样。接下来，我们就要基于以上数据，进行相应的处理。关于类别不平衡的问题，主要有两种处理方式：过采样方法增加数量较少那一类样本的数量，使得正负样本比例均衡。欠采样方法减少数量较多那一类样本的数量，使得正负样本比例均衡。 2 解决类别不平衡数据方法介绍 2.1 过采样方法 2.1.1 什么是过采样方法对训练集里的少数类进行“过采样”（oversampling），即增加一些少数类样本使得正、反例数目接近，然后再进行学习。 2.1.2 随机过采样方法随机过采样是在少数类 中随机选择一些样本，然后**通过复制所选择的样本生成样本集 ，**将它们添加到 中来扩大原始数据集从而得到新的少数类集合 。新的数据集 。通过代码实现随机过采样方法：12345678910111213# 使用imblearn进行随机过采样from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)#查看结果Counter(y_resampled) #过采样后样本结果# Counter(&#123;2: 4674, 1: 4674, 0: 4674&#125;)# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：对于随机过采样，由于需要对少数类样本进行复制来扩大数据集，造成模型训练复杂度加大。另一方面也容易造成模型的过拟合问题，因为随机过采样是简单的对初始样本进行复制采样，这就使得学习器学得的规则过于具体化，不利于学习器的泛化性能，造成过拟合问题。为了解决随机过采样中造成模型过拟合问题，又能保证实现数据集均衡的目的，出现了过采样法代表性的算法SMOTE算法。 2.1.3 过采样代表性算法-SMOTESMOTE全称是Synthetic Minority Oversampling即合成少数类过采样技术。SMOTE算法是对随机过采样方法的一个改进算法，由于随机过采样方法是直接对少数类进行重采用，会使训练集中有很多重复的样本，容易造成产生的模型过拟合问题。而SMOTE算法的基本思想是对每个少数类样本 ，从它的最近邻中随机选择一个样本 （ 是少数类中的一个样本），然后在 和 之间的连线上随机选择一点作为新合成的少数类样本。SMOTE算法合成新少数类样本的算法描述如下：对于少数类中的每一个样本 ，以欧氏距离为标准计算它到少数类样本集 中所有样本的距离，得到其k近邻。根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本 ，从其k近邻中随机选择若干个样本，假设选择的是 。对于每一个随机选出来的近邻 ，分别与 按照如下公式构建新的样本。我们用图文表达的方式，再来描述一下SMOTE算法。先随机选定一个少数类样本 。找出这个少数类样本 的K个近邻（假设K=5），5个近邻已经被圈出。随机从这K个近邻中选出一个样本 （用绿色圈出来了）。4)在少数类样本 和被选中的这个近邻样本 之间的连线上，随机找一点。这个点就是人工合成的新的样本点（绿色正号标出）。SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。代码实现：1234567891011# SMOTE过采样from imblearn.over_sampling import SMOTEX_resampled, y_resampled = SMOTE().fit_resample(X, y)Counter(y_resampled)# 采样后样本结果# [(0, 4674), (1, 4674), (2, 4674)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show() 2.2 欠采样方法 2.2.1 什么是欠采样方法直接对训练集中多数类样本进行“欠采样”（undersampling），即去除一些多数类中的样本使得正例、反例数目接近，然后再进行学习。 2.2.2 随机欠采样方法随机欠采样顾名思义即从多数类 中随机选择一些样样本组成样本集 。然后将样本集 从 中移除。新的数据集 。代码实现：123456789101112# 随机欠采样from imblearn.under_sampling import RandomUnderSamplerrus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)Counter(y_resampled)# 采样后结果[(0, 64), (1, 64), (2, 64)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但是这也存在一些问题。对于随机欠采样，由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。官网链接：https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"独立同分布","slug":"ML/独立同分布","date":"2019-06-08T02:24:21.000Z","updated":"2022-06-04T03:47:35.264Z","comments":true,"path":"机器学习/ML/独立同分布/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83/","excerpt":"","text":"独立同分布IID(independent and identically distributed) 1 独立同分布(i.i.d.)在概率统计理论中，如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。在西瓜书中解释是：输入空间中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。 2 简单解释 — 独立、同分布、独立同分布（1）独立：每次抽样之间没有关系，不会相互影响举例：给一个骰子，每次抛骰子抛到几就是几，这是独立；如果我要抛骰子两次之和大于8，那么第一次和第二次抛就不独立，因为第二次抛的结果和第一次相关。（2）同分布：每次抽样，样本服从同一个分布举例：给一个骰子，每次抛骰子得到任意点数的概率都是六分之一，这个就是同分布（3）独立同分布：i.i.d.，每次抽样之间独立而且同分布 3 机器学习领域的重要假设IID独立同分布即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。 4 目前发展机器学习并不总要求独立同分布，在不少问题中要求样本数据采样自同一个分布是因为希望用训练数据集得到的模型可以合理的用于测试数据集，使用独立同分布假设能够解释得通。目前一些机器学习内容已经不再囿于独立同分布假设下，一些问题会假设样本没有同分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"2019-春节假期生活杂感","slug":"程序人生/2019年春节假期生活杂感","date":"2019-02-05T05:32:15.000Z","updated":"2022-05-14T16:04:47.503Z","comments":true,"path":"程序人生/程序人生/2019年春节假期生活杂感/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/2019%E5%B9%B4%E6%98%A5%E8%8A%82%E5%81%87%E6%9C%9F%E7%94%9F%E6%B4%BB%E6%9D%82%E6%84%9F/","excerpt":"","text":"又是和往年一样的春节放假，依旧是常规的操作，放假回来，在家懒着，然后腊月底和爸一起去上坟，最后几天贴春联，挂灯笼等操作。【三十而立，面包杂想】现在是腊三十，不管从阳历还是阴历，自己总算要又长一岁了，虚岁要步入三十大关。古代三十而立，在此插入，非常有必要解释一下，到底什么是三十而立12345孔子所说的“三十而立”，是指他在这个时候做事合于礼，言行都很得当。 [1] 言：谦卑 中 传递祥和！行：举止 彬彬有礼！现常用来指人开始有所成就。三十而立：三十岁的时候就可以自立于世。2019-02-05(正月初一)接着上次的内容继续扯东扯西，三十而立，就自己理解，立的内容是两方面吧，一个是精神层面的面包，一个是真实世界的面包。关于精神层面，具体而言，自从从事计算机相关的工作，这个并没有什么大的进步，关于真实世界的面包，可能限于环境的限制，更多的考虑的是这方面的内容吧，有房吗，有车吗。。。有时候醒悟，面包不仅仅一直局限于真实世界的单一口味，还需要充实自己的精神世界。最好的目标彼岸，真实环境，精神层面都有面包收获，这应该是自己期望达到的目标吧。【2019-沉淀再想】这个假期，说实在的，过得不是那么心安理得，因为有很多事情需要自己去完成，但是都没有去行动，比如一直说要去完成的那篇论文，比如说要准备机器学习，再比如说对自己人工智能的充电。就那么没心没肺的过着，但是当回想的时候，还是蛮有愧疚感的。可能春节回家继续办公、学习，这件事情本身就比较反人类，哈哈。既然回家之前把目标确定了，那么就在接下来的这几天努力尽量完成目标，别加深自己的罪恶感。关于假期目标，其实是希望达到高度的折射，也从另一方面说明了2019必须干的事情。12如何沉淀自己人工智能相关领域的知识？需要自己计划好，然后去坚持完成关于2019的基调，需要主要精力去做的事情，都需要自己定位好，然后去严格的执行，只有一步步的去执行，才能在最后获取到相应的高度，加油！！【关于潮流变化】假期回家，被父母每天观看的快手充斥着，自己也为了体验一下究竟什么样产品可以勾住几乎所有人，下载浏览，快手给我带来更大的吃惊之处是，我妈竟然在昨天从快手中提出20元，这个是她这段时间直播的收入，瞬间觉得，这个社会发展的和自己之前的理解有些偏差。这个社会在追求快速发展的同时也失去了一些东西。之前自己是一直不下载观看快手之类的app的，因为觉得里面的内容，大部分都没有营养，看的过程中，也就是当时博君一笑，尔尔。但是当这个东西还可以赚钱，在利益的驱动下，是否会有更多没有营养的内容被鼓励产生。产品的制作，宣传已经没有了底线可言，怎么让产品更符合大众，更能吊起大众的胃口，怎么设计。如此产品，怎么经得起深究思考呢？或者，产本设计者自己就没有设计一个底线，只是在追求利益。","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]}],"categories":[{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"},{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"},{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"},{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]}