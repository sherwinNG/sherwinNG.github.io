{"meta":{"title":"sherwinNG's blog","subtitle":"遇见更好的自己","description":"The way of the future!","author":"sherwin","url":"http://sherwinzhang.com","root":"/"},"pages":[{"title":"404","date":"2020-02-19T01:33:16.829Z","updated":"2020-02-19T01:33:16.829Z","comments":true,"path":"404.html","permalink":"http://sherwinzhang.com/404.html","excerpt":"","text":"这是网页标题"},{"title":"","date":"2022-06-03T07:16:01.465Z","updated":"2022-06-03T07:15:12.823Z","comments":true,"path":"google13fc42b677fff64a.html","permalink":"http://sherwinzhang.com/google13fc42b677fff64a.html","excerpt":"","text":"google-site-verification: google13fc42b677fff64a.html"},{"title":"分类","date":"2022-06-03T16:22:28.226Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"categories/index.html","permalink":"http://sherwinzhang.com/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2022-06-03T16:22:32.664Z","updated":"2022-05-13T09:28:31.824Z","comments":false,"path":"about/index.html","permalink":"http://sherwinzhang.com/about/index.html","excerpt":"","text":"个人详细介绍12&#123;你好&#125;"},{"title":"书单","date":"2022-06-03T16:22:30.631Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"books/index.html","permalink":"http://sherwinzhang.com/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-06-03T16:22:20.686Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"repository/index.html","permalink":"http://sherwinzhang.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-06-03T16:22:23.877Z","updated":"2022-03-21T07:18:46.000Z","comments":true,"path":"links/index.html","permalink":"http://sherwinzhang.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-06-03T16:22:17.542Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"tags/index.html","permalink":"http://sherwinzhang.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"BERT之后，NLP主要预训练模型演变梳理","slug":"NLP/BERT之后，NLP主要预训练模型演变梳理","date":"2022-06-20T13:20:21.000Z","updated":"2022-08-10T14:36:48.168Z","comments":true,"path":"自然语言处理/NLP/BERT之后，NLP主要预训练模型演变梳理/","link":"","permalink":"http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/","excerpt":"","text":"BERT之后，NLP主要预训练模型演变梳理 1.背景BERT [1]（Bidirectional Encoder Representation from Transformers）是由 Google AI 于 2018 年 10 月提出的一种基于深度学习的语言表示模型。BERT 发布时，在 11 种不同的自然语言处理（NLP）测试任务中取得最佳效果。截至2022年6月，很多研究者又基于BERT提出了很多新的模型，本文旨在梳理基于BERT模型优化后部分预训练模型，以便读者能够更快掌握BERT相关内容，为后期工作中使用BERT相关模型提供便捷性。 2.BERT基本介绍BERT 主要的模型结构是 Transformer 的编码器部分。Transformer[2] 是由 Ashish 等于 2017年提出的，用于Google机器翻译，包含编码器（Encoder）和解码器（Decoder） 两部分。其中 BERT-base 与 BERT-large 模型分别采用了 12 层与 24 层的 Transformer 编码器作为模型网络层。相比于传统用于 NLP 任务的循环神经网络 （RNN）及长短时记忆网络（LSTM）等，Transformer 拥有更强大的文本编码能力，也能更高效地利用 GPU 等高性能设备完成大规模训练工作。基于 BERT 模型的自然语言处理任务通过两个过程来实现：在预训练的过程中，首先利用大规模没有标注过的文本语料，例如百科知识、网页新闻等，通过充分的自监督训练，有效学习文本的语言特征，得到深层次的文本向量表示，形成相应文本的预训练模型。在微调过程中，直接将预训练过程中完成收敛的网络参数（即嵌入层和网络层）作为起始模型，根据具体的下游任务（如分类、序列标注等），输入人工标注好的数据集，完成模型的进一步拟合与收敛。从而得到一个可用的深度学习模型来实现特定的自然语言处理任务，例如文本分类、序列标注等。自 BERT 发布以来，基于“预训练-微调”的两阶段方法逐渐成为自然语言处理研究的主流。图片来源：参考文献[1] 3.ERNIEERNIE[3]（Enhanced Representation through Knowledge Integration）是百度（清华几乎在同一时间[2019]也发布了ERNIE版本，不过现在社会上谈起ERNIE，大多指百度版ERNIE）在2019年4月基于BERT模型做的进一步优化，在中文的NLP任务上得到了state-of-the-art的结果。其主要是通过对知识进行整合，达到增强表达的目的。受BERT的掩码策略启发，ERNIE旨在学习由知识掩码策略增强的语言表征，其中包括实体级掩码和短语级掩码。实体级策略通常由多个单词组成的实体。短语级策略将由多个单词组成的整个短语作为一个概念单元进行屏蔽。ERNIE和BERT的区别：图片来源：参考文献[3]在使用先验知识来增强预训练语言模型时ERNIE并没有直接添加知识嵌入，而是使用了一种多阶段知识掩码策略，将短语和实体集成到语言表示中。句子中不同的掩码级别如下图：图片来源：参考文献[3]基本级别的掩码第一个学习阶段是使用基本的掩码，它将一个句子视为一系列基本的语言单元，对于英语，基本的语言单元是单词，对于汉语，基本的语言单元是汉字。在训练过程中，我们随机屏蔽15%的基本语言单元，并使用句子中的其他基本单元作为输入，并训练一个转换器来预测被屏蔽的单元。基于基本层掩码，我们可以得到基本的单词表示。因为它是在基本语义单元随机掩码的基础上训练的，所以高层语义的知识表示很难完全建模。短语级别的掩码第二阶段是使用短语级掩码。短语是作为概念单位的一小群单词或字符。对于英语，我们使用词汇分析和组块工具来获取句子中短语的边界，并使用一些依赖于语言的切分工具来获取其他语言（如汉语）中的单词/短语信息。在短语级掩码阶段，我们还使用基本语言单元作为训练输入，不像随机基本单元掩码那样，这次我们在句子中多选几个短语，掩码并预测同一短语中的所有基本单元。在这个阶段，短语信息被编码到单词嵌入中。实体级别的掩码第三阶段是实体级掩码。我们把一些专有名词，如地点、人名、组织、产品等抽象为实体进行屏蔽。实体通常包含句子中的重要信息。在短语屏蔽阶段，我们首先分析一个句子中的命名实体，然后屏蔽和预测实体中所有的空缺。经过这三个阶段的学习，我们可以获得语义信息丰富的表达。经过实验结果表明，在五个自然语言处理任务（包括自然语言推理，语义相似性，命名实体识别，情感分析和检索问答）上，ERNIE优于当时其他基准方法。此外ERNIE在完形填空测试中具有更强大的知识推理能力。 4.RoBERTaRoBERTa[4]是Facebook和华盛顿大学于2019年7月在论文《RoBERTa: A Robustly Optimized BERT Pretraining Approach》中提出的。文章在 BERT模型的基础上提出了 BERT 模型的改进版 RoBERTa，使其获得了更好的自然语言任务处理效果，并在 GLUE，SQuAD，RACE 三个榜上取得最好的SOTA。RoBERTa主要在三方面对之前提出的BERT做了改进：其一是模型的具体细节层面，改进了优化函数；其二是训练策略层面，改用了动态掩码的方式训练模型，证明了NSP（Next Sentence Prediction）训练策略的不足，采用了更大的batch size；其三是数据层面，一方面使用了更大的数据集，另一方面是使用字节级别的BPE（Bytes-level BEP ）来处理文本数据。 5.DeBERTaDeBERTa[5]（Decoding-enhanced BERT with Disentangled Attention）是微软发表于ICLR2021上的预训练语言模型。2021年1月DeBERTa在SuperGLUE这项自然语言理解基准任务上**「超越人类」**，以90.3分夺冠。DeBERTa从两方面改进了BERT预训练的方法：自注意力**「解耦」**机制用2个向量分别表示content 和 position，即word本身的文本内容和位置。word之间的注意力权重则使用word内容之间和位置之间的解耦矩阵。这是因为word之间的注意力不仅取决于其文本内容，还依赖于两者的相对位置。图片来源：参考文献[6]用Enhanced Mask Decoder(「EMD」)强化预训练输出层原始的BERT存在预训练和微调不一致问题。预训练阶段，隐层最终的输出输入到softmax预测被mask掉的token，而微调阶段则是将隐层最终输出输入到特定任务的decoder。这个decoder根据具体任务不同可能是一个或多个特定的decoder，如果输出是概率，那么还需要加上一个softmax层。为消除这种不一致性，DeBERTa将MLM与其他下游任务同等对待，并将原始BERT中输出层的softmax替换为**「增强后的mask decoder(EMD)」**，EMD包含一个或多个Transformer层和一个softmax输出层。至此，结合了BERT和EMD的DeBERTa成为了一个encoder-decoder模型。使用这两种技术，新的预训练语言模型DeBERTa在许多下游NLP任务上的表现都优于RoBERTa和BERT。DeBERTa这项工作展示了探索自注意的词表征解耦以及使用任务特定解码器改进预训练语言模型的潜力。 6.综述本文针对BERT系列部分典型模型进行梳理，希望为大家梳理出在BERT提出后，整体的优化脉络。同时基于BERT的优化方向可以总结为如下：首先，大量的研究者通过对 BERT 的两个预训练目标进行改进提升模型对文本特征的学习能力，如：ERNIE、RoBERTa、DeBERTa等。对于预训练目标的优化改进是最常见同时也是效果最好的改造方式，所以本文在前面介绍中，也主要梳理了该方向的主要模型。其次，针对特定领域的显性知识，研究者提出在预训练模型中融合外部知识的方法，进一步丰富了模型所学习的文本特征，如用于专利文本的 PatentBERT：。这两种路线提升了模型的特征学习能力，但是并没有对预训练模型内部结构进行实质性的改进。部分研究者从 Transformer 神经网络出发，对其内部结构进行了改进，从而扩展了模型的应用场景，如：BART。最后，针对 BERT 模型参数量过大导致普通的硬件设备无法有效训练和加载的问题，大量的研究者提出模型压缩的方法，进而提升了 BERT 模型的易用性，如：ALBERT。 7.参考资料bert [1] https://arxiv.org/pdf/1810.04805.pdftransformer[2] https://arxiv.org/pdf/1706.03762.pdfernie [3] https://arxiv.org/pdf/1904.09223.pdfroberta [4] https://arxiv.org/pdf/1907.11692.pdfdeberta[5] https://arxiv.org/pdf/2006.03654.pdfdeberta[6] https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_ca 附件BERT模型优化改进路线总结：","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"}]},{"title":"AI模型部署-fastapi[3-3]","slug":"AI综合/AI模型部署-fastapi[3-3]","date":"2022-06-07T13:34:29.000Z","updated":"2022-06-07T13:38:49.530Z","comments":true,"path":"AI综合/AI综合/AI模型部署-fastapi[3-3]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-fastapi[3-3]/","excerpt":"","text":"一、使用方法1.fastapi服务代码123456789101112131415161718# A机器中某路径下，app.py# fastapi模块导入from fastapi import FastAPIfrom pydantic import BaseModel# 通过item类定义数据类型class Item(BaseModel): data: dictapp = FaseAPI()@app.post(\"/v1/\")async def creat_item(item: Item): return item.data[\"text\"]2.fastapi服务部署12345678# A机器中终端执行# 服务调试uvicorn app:app --reload --host 0.0.0.0 --port 8001# 服务部署nohup uvicorn app:app --host 0.0.0.0 --port 8001 &gt; ./ret.log 2&gt;&amp;1 &amp;# nohup command &amp; -- 在后台运行3.部署好的服务请求12345678910111213141516171819202122# B机器中某路径下，test.py# 也可在同一台机器中测试# coding=utf-8import requests import jsonimport timeurl = \"http://0.0.0.0:8001/v1/\"data = &#123; \"id\": 1234, \"data\": \"这是一条测试数据！\"&#125;start = time.time()res = requests.post(url, json=data, timeout=200)end = time.time()print(\"耗费时间：\", end - start)print(\"返回的结果：\", res.json()) 二、总结如果使用fastapi框架部署深度学习模型1.在A机器中书写fastapi服务代码2.在A机器终端部署写好的服务3.在B机器调用部署好的服务","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"AI模型部署-flask[3-2]","slug":"AI综合/AI模型部署-flask[3-2]","date":"2022-06-07T13:04:29.000Z","updated":"2022-06-07T13:38:42.937Z","comments":true,"path":"AI综合/AI综合/AI模型部署-flask[3-2]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask[3-2]/","excerpt":"","text":"一、背景深度学习在训练好模型中，我们需要进行部署，有一些框架中已经自带部署框架，如TensorFlow中的 TensorFlow serving，paddlepaddle中的 paddle serving等。但是有些框架自带部署能力并不友好，或者如pytorch类框架，直接官方就建议使用python的flask框架。如果你在训练好模型后，想自己基于flask框架部署模型，下文就可以解决该类需求。 二、使用方法1.flask服务代码1234567891011121314151617181920212223242526# A机器中某路径下，app.py# 导入了 Flask 类。该类的实例将会成为我们的 WSGI 应用。from flask import Flaskfrom flask import request# 创建一个flask类的实例。app = Flask(__name__)# 第一个参数是应用模块或者包的名称。 __name__ 是一个适用于大多数情况的快捷方式。# 有了这个参数， Flask才能知道在哪里可以找到模板和静态文件等东西。# 定义服务请求路径和方式, 使用POST请求# 使用 route() 装饰器来告诉 Flask 触发函数的URL 。@app.route(\"/v1/\", methods=[\"POST\"])# \"/v1/\" 表示请求服务路径# \"POST\" 不同的请求方法， 常见的有 get,postdef example(): \"\"\"flask模型预测\"\"\" text = request.get_json() # 测试请求数据内容 # print(text) \"\"\" 逻辑书写 \"\"\" return text2.flask服务部署1234567891011# A机器中终端执行# 服务调试gunicorn -w 1 -b 0.0.0.0:8001 app:app# -w 代表开启的进程数, 我们只开启一个进程# -b 服务的IP地址和端口# app:app 是指执行的主要对象位置, 在app.py中的app对象# 服务部署nohup gunicorn -w 1 -b 0.0.0.0:8001 app:app &gt; ./ret.log 2&gt;&amp;1 &amp;# nohup command &amp; -- 在后台运行3.部署好的服务请求1234567891011121314151617181920212223# B机器中某路径下，test.py# 也可在同一台机器中测试# coding=utf-8import requests import jsonimport timeurl = \"http://0.0.0.0:8001/v1/\"data = &#123; \"id\": 1234, \"data\": \"这是一条测试数据！\"&#125;start = time.time()res = requests.post(url, json=data, timeout=200)end = time.time()print(\"耗费时间：\", end - start)print(\"返回的结果：\", res.json()) 三、总结如果使用flask框架部署深度学习模型1.在A机器中书写flask服务代码2.在A机器终端部署写好的服务3.在B机器调用部署好的服务","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"AI模型部署-flask和fastapi对比[3-1]","slug":"AI综合/AI模型部署-flask 和fastapi 对比[3-1]","date":"2022-06-07T12:44:29.000Z","updated":"2022-06-07T13:38:37.020Z","comments":true,"path":"AI综合/AI综合/AI模型部署-flask 和fastapi 对比[3-1]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask%20%E5%92%8Cfastapi%20%E5%AF%B9%E6%AF%94[3-1]/","excerpt":"","text":"1.基本介绍flask、fastapi都是python语言在Web领域的框架，使用它们可以帮助我们提升效率，节省时间，避免我们在组装”汽车“的时候，还要从”轮子“、”螺丝钉“做起。python语言中，web领域主流框架有：django, flask, tornado, fastapi等本文主要从AI中训练好的模型如何更便捷、高效的部署，所以着重介绍flask和fastapi. 1.1 flask是什么Flask 是一个用 Python 编写的轻量级 Web 应用框架。其被称为“微框架”，因为它使用简单的核心，通过扩展增加其他功能。如文件上传等。官方网站 1.2 fastapi是什么FastAPI是一个高性能Web框架，用于构建API。FastAPI主要特性：快速：非常高的性能，与NodeJS和Go相当快速编码：将功能开发速度提高约200％至300％更少的错误：减少约40％的人为错误直观：强大的编辑器支持，自动补全无处不在，调试时间更少简易：旨在易于使用和学习，减少阅读文档的时间。简短：减少代码重复。稳健：获取可用于生产环境的代码，具有自动交互式文档基于标准：基于并完全兼容 API 的开放标准OpenAPI和JSON SchemaFastAPI要求：Python 3.6+官方网站 2.两者对比 2.1 flask优缺点优点与其他 Web 应用程序框架不同，flask 让你可以完全控制 Web 开发，从而完全控制应用程序和 Web 开发。Flask 允许进行单元测试，并且由于其内置的开发服务器，集成的支持等，因此可以通过对一些扩展进行调整来过渡到 Web 框架。Flask 简单易用，非常适合初学者使用，为开发人员提供了更好地学习和理解它的空间。它还使开发人员可以毫不费力地快速创建应用程序。缺点Flask 的很多模块由第三方开发，容易引起安全漏洞。Flask 具有一个单一的来源，表示它将依次处理每个请求，因此，无论有多少个请求，它仍然会轮流处理它们，这会耗费更多时间。可以将 Flask 用于商业项目。它可以帮助你快速入门，**但是网站高负荷情况下效果不佳。**你可以快速实施 Flask 项目，例如：电子商务系统。Facebook / Twitter机器人。在线社交网络。静态网站。如果你要做一些小型个人项目，比如聊天机器人，或者想实现产品的快速原型，或者喜欢自由的编写代码控制程序的流程，那么可以选择 Flask。 2.2 FastApi优缺点优点**自动类型检查。**这意味着更少的 Bug，即使在深度嵌套的 JSON 请求中，Fast API 也会验证开发人员的数据类型。集众所长，站在巨人的肩膀上。FastAPI 建立在 JSON Schema（用于验证JSON数据结构的工具），OAuth 2.0（用于授权的行业标准协议）和**OpenAPI（这是可公开获得的应用程序编程接口）**之类的标准之上。现代化。FastAPI 使使用称为 graphene-python 的 Python 库轻松构建 GraphQL API 。**快速、高性能。**可以和 NodeJS 和 Go 相提并论。缺点由于 FastAPI 相对较新，因此与其他框架相比，社区较小，第三方的教程相对较少。FastAPI 适用于构建高性能的 API，本身支持异步，如果要构建异步 API，可以优先选择 FastAPI。作者写开发fastapi的缘由：FastApi诞生的缘由 2.3 比较Stack Overflow 2021 开发人员调查最受欢迎的 Web 框架：来源：https://insights.stackoverflow.com/survey/2021/#section-most-loved-dreaded-and-wanted-web-frameworks从社区，性能，灵活性，学习成本，稳定性进行比较。社区活跃程度。Flask框架因为搭建较早，且开发者使用较多，所以社区非常活跃；FastAPI 的社区目前还比较小，因为它相对较新。性能。在性能方面，FastAPI 是领跑者，因为它是面向速度的。灵活性。灵活性是开发人员非常重视的东西，FastAPI 在代码方面是非常灵活的，其不限制代码布局。学习成本。FastAPI &lt; Flask稳定性Flask &gt; api","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"目前使用不错软件整理","slug":"apple生态/目前使用不错软件整理","date":"2022-06-03T08:44:34.000Z","updated":"2022-06-03T16:22:54.587Z","comments":true,"path":"apple生态/apple生态/目前使用不错软件整理/","link":"","permalink":"http://sherwinzhang.com/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/","excerpt":"","text":"办公日事清：计划管理screenbrush:屏幕画笔、标记pdf阅读器：pdf expert做笔记功能，生态能力（ipad做笔记，可以在其他端查看） 图文笔记sublime（替代mac文本文档）typora（markdown书写）搭配ipic使用（截图上传到云端）onenote(学习类笔记)有道云（写随笔的地方）xmind(脑图制作)agenda(带分类窗口和时间线的笔记记录平台) 图片截图软件：snipaste, 截图效果一般，但是贴图功能非常强大 影音视频录制：obs视频剪辑:screenflow 系统bartender:mac bar管理工具，让你的bar没有那么杂乱的软件magnet:屏幕窗口管理：尤其适合多屏用户（分享自己的快捷键）MonitorControl一键调节外置显示器亮度及音量的软件 网络浏览器Safari(为了苹果生态使用)chrome(当Safari无法发挥作用时使用) 学习bob:非常好用的翻译软件，支持截图翻译、输入翻译、复制翻译123苹果生态 iCloud云盘(存储必要文档) 阅读书籍（苹果生态阅读+笔记同步）","categories":[{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"}],"tags":[{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"}]},{"title":"流言猛如虎","slug":"程序人生/流言猛如虎","date":"2022-05-14T15:41:15.000Z","updated":"2022-06-04T03:48:10.861Z","comments":true,"path":"程序人生/程序人生/流言猛如虎/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E6%B5%81%E8%A8%80%E7%8C%9B%E5%A6%82%E8%99%8E/","excerpt":"","text":"最近发生的几件事情，作为一个旁观者，让我深刻体会到在信息爆炸的今天，流言的可怕性。暂且在此记录，同时告诫自己以后类似情况应对之法。1.本周四下午，突然传出北京市因为疫情原因，新闻发布会将会宣布北京进入静默管理3天。该消息一经流传，就在各大短视频平台迅速蔓延，然后大部分市民都加入了蔬菜等日常物资的抢购潮。但是没过多久，随着疫情新闻发布会的结束，并没有如大家所传需要进入静默期，而是严惩假消息传播的始作俑者。2.最近在刷微信小视频或者抖音的时候，总可以发现北大韦神的日常，有他授课的、日常生活被别人拍摄的。其中有一条是几个教授花数月解不开的难题，被韦神几分钟解决。这个视频经过几天发酵后，最后韦东奕亲自澄清，并没有如此事情，只是流言。我们每个人都处在信息爆炸的今天，说不定你某个无意的举动，经过蝴蝶效应的发酵，就会把你推上风口浪尖。甚至在你受到别人关注的时候，即使很多和你并不相关的事情，都会被杜撰出来。我们暂且不讨论那些杜撰此类消息的人们，为了博人眼球，无所不用其极。我们能做的是，在流言中如何让自己尽可能成为一名“智者”。现在的我们，每天都会接受来自外界的很多信息，其传播之广、传播之快，远超常人想象，在这些未经证实的“消息”被推送到我们面前，我们需要做的是：1.不做任何消息都照单全收的“接收者”，需要加上自己的判断。2.流言止于智者。于2022年05月14日23:29:33家中","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]},{"title":"linux环境下安装Anaconda","slug":"计算机相关/linux环境下安装Anaconda","date":"2021-10-10T00:55:39.000Z","updated":"2022-06-16T00:56:03.776Z","comments":true,"path":"计算机相关/计算机相关/linux环境下安装Anaconda/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Anaconda/","excerpt":"","text":"linux环境下安装Anaconda 1.下载：获取anaconda在清华镜像站的网址，然后在服务器端wget 网址就行了。清华镜像站中anaconda的所有版本的网址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/找到自己想要的那个版本，然后右键 -&gt; 复制链接地址。接下来在服务器端找一个好的目录，wget + 复制好的地址，运行就好。如：1wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh 2.安装下载好之后，在下载目录目录中，出现一个Anaconda3-5.3.1-Linux-x86_64.sh这样子的文件，运行它就好,切换到该文件目录运行1bash Anaconda3-5.3.1-Linux-x86_64.sh根据指示操作：步骤一：按下ENTER继续。步骤二：在按下enter键之后，安装又会停止，并且出现一个“more”的字样，按空格就好。步骤三：接收协议，在这里输入“yes”表示接收协议。步骤四：是否在环境中配置anaconda的环境， 输入yes，不然你自己配环境很麻烦。步骤五：是否安装vscode？这个看个人爱好了，一般我选择否，输入no之后，安装完成。注意：需要退出终端后，再次进入才能输入conda命令。或者不想退出的话，刷新一下环境变量，之后输入conda就有反应了12刷新环境变量命令：source .bash_profile步骤六：输入conda验证是否安装成功，如果出现如下截图内容，说明安装成功接下来就可以愉快的在anaconda中玩耍了。","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"tmux命令总结","slug":"计算机相关/tmux命令总结","date":"2021-10-08T15:21:18.000Z","updated":"2022-06-08T15:21:39.627Z","comments":true,"path":"计算机相关/计算机相关/tmux命令总结/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/tmux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/","excerpt":"","text":"tmux命令总结 1. 是什么tmux即terminal multiplexer（终端复用器），它可以启动一系列终端会话。它解绑了会话和终端窗口。关闭终端窗口再打开，会话并不终止，而是继续运行再执行。将会话与终端窗后彻底分离。 2. 怎么用 2.1 安装12安装：yum install tmux 2.2 入门运行tmux：# tmux新建会话： # tmux new -s SESSION-NAME查看已创建的会话：# tmux ls进入一个已知会话： # tmux a -t SESSION-NAME 或 # tmux attach -t SESSION-NAME暂时离开当前会话：# tmux detach该命令会从当前会话中退出去, 因此才会有稍后重新接入会话这么一说关闭会话：# tmux kill-session -t SESSION-NAME在会话内部或外部执行均可 2.3 进阶 2.3.1 分屏操作很多情况下, 需要在一个会话中运行多个命令，执行多个任务,我们可以在一个会话的多个窗口里组织他们。分屏：分为水平分屏和垂直分屏水平分屏 – 快捷键：先按 ctrl+b, 放开后再按%如何区分水平分屏和垂直分屏，看%和&quot;在键盘中的大概位置，就可以理解垂直分屏 – 快捷键：先按 ctrl+b, 放开后再按 &quot;分屏后的窗口中光标互相切换 – 快捷键：先按ctrl+b, 放开后再按下o (或者直接方向键) 使用快捷键左右分屏 ：Ctrl + b, % (分割当前窗口)上下分屏 ：Ctrl + b, &quot; (分割当前窗口)关闭分屏 ：Ctrl + b, x (关闭所在分屏窗口)显示分屏编号 ：Ctrl + b, q (显示分屏编号)分屏切换 ：Ctrl + b, 方向键 (基本可以自由切换) 2.3.2 切换tmux会话终端快捷键：先按ctrl+b, 放开后再按s 2.3.3 屏中内容上下滚动模式进入tmux翻屏模式：先按 ctrl ＋ｂ，松开，然后再按 [上下翻页：直接通过触摸板控制退出：q 2.3.4 当前窗格全屏显示先按ctrl+b, 放开后再按z：当前窗格全屏显示，再使用一次会变回原来大小 2.3.5 列出当前所有窗口先按ctrl+b, 放开后再按w; 3.其他单独运行tmux命令，即开启一个tmux会话。不能在tmux会话里面再新建会话，会报错：“sessions should be nested with care, unset $TMUX to force”。终端内显示时间：快捷键：先按ctrl+b, 放开后再按t ；退出时间界面：按q键参考资料：https://blog.51cto.com/13683137989/1961188https://zhuanlan.zhihu.com/p/98384704","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"数据分割介绍","slug":"ML/数据分割介绍","date":"2021-06-12T01:14:21.000Z","updated":"2022-06-04T03:47:45.895Z","comments":true,"path":"机器学习/ML/数据分割介绍/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"在机器学习中，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景:老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？答案是否定的，可能有的同学只会做这10道题却能得高分。回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。可是，我们只有一个包含m个样例的数据集既要训练，又要测试，怎样才能做到呢？答案是:通过对D进行适当的处理，从中产生出训练集S和测试集T。（这个也是我们前面一直在做的事情）。下面我们一起总结一下几种常见的做法：留出法交叉验证法自助法 1 留出法“留出法”(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。大家在使用的过程中，需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。如果从采样( sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为**“分层采样”( stratified sampling)。**例如通过对D进行分层样而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。另一个需注意的问题是，即便在给定训练测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中，这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。因此，单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性( fidelity)。这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。使用Python实现留出法：1234from sklearn.model_selection import train_test_split#使用train_test_split划分训练集和测试集train_X , test_X, train_Y ,test_Y = train_test_split( X, Y, test_size=0.2,random_state=0)在留出法中，有一个特例，叫：留一法( Leave-One-Out，简称LOO），即每次抽取一个样本做为测试集。显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集一每个子集包含个样本；使用Python实现留一法：123456789101112from sklearn.model_selection import LeaveOneOutdata = [1, 2, 3, 4]loo = LeaveOneOut()for train, test in loo.split(data): print(\"%s %s\" % (train, test))'''结果[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3]'''留一法优缺点：优点：留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，留一法的评估结果往往被认为比较准确。缺点：留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，则需训练1百万个模型，而这还是在未考虑算法调参的情况下。 2 交叉验证法“交叉验证法”( cross validation)先将数据集D划分为k个大小相似的互斥子集，即。每个子集$$D_i$$都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”(k- fold cross validation)。k最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。下图给出了10折交叉验证的示意图。**与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。**为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有“10次10折交叉验证”。交叉验证实现方法，除了咱们前面讲的GridSearchCV之外，还有KFold, StratifiedKFold KFold和StratifiedKFold1from sklearn.model_selection import KFold,StratifiedKFold用法：将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。注意点：对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本参数说明：n_splits：表示划分几等份shuffle：在每次划分时，是否进行洗牌①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的属性：①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器1234567891011121314151617181920212223242526import numpy as npfrom sklearn.model_selection import KFold,StratifiedKFoldX = np.array([ [1,2,3,4], [11,12,13,14], [21,22,23,24], [31,32,33,34], [41,42,43,44], [51,52,53,54], [61,62,63,64], [71,72,73,74]])y = np.array([1,1,0,0,1,1,0,0])folder = KFold(n_splits = 4, random_state=0, shuffle = False)sfolder = StratifiedKFold(n_splits = 4, random_state = 0, shuffle = False) for train, test in folder.split(X, y): print('train:%s | test:%s' %(train, test)) print(\"\") for train, test in sfolder.split(X, y): print('train:%s | test:%s'%(train, test)) print(\"\")结果：1234567891011121314151617# 第一个for，输出结果为：train:[2 3 4 5 6 7] | test:[0 1]train:[0 1 4 5 6 7] | test:[2 3]train:[0 1 2 3 6 7] | test:[4 5]train:[0 1 2 3 4 5] | test:[6 7]# 第二个for，输出结果为：train:[1 3 4 5 6 7] | test:[0 2]train:[0 2 4 5 6 7] | test:[1 3]train:[0 1 2 3 5 7] | test:[4 6]train:[0 1 2 3 4 6] | test:[5 7]可以看出，sfold进行4折计算时候，是平衡了测试集中，样本正负的分布的；但是fold却没有。 3 自助法我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此**实际评估的模型所使用的训练集比D小，**这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？“自助法”( bootstrapping)是一个比较好的解决方案，它直接以自助采样法( bootstrap sampling)为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D:每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率是$$(1-\\frac{1}{m})^m$$，取极限得到即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。于是我们可将D′用作训练集，D\\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称**“包外估计”(out- of-bagestimate）**自助法优缺点：优点：自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。 4 总结综上所述：当我们数据量足够时，选择留出法简单省时，在牺牲很小的准确度的情况下，换取计算的简便；当我们的数据量较小时，我们应该选择交叉验证法，因为此时划分样本集将会使训练数据过少；当我们的数据量特别少的时候，我们可以考虑留一法。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"DeepFM","slug":"ML/DeepFM","date":"2021-04-09T02:24:21.000Z","updated":"2022-08-10T08:49:50.000Z","comments":true,"path":"机器学习/ML/DeepFM/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/DeepFM/","excerpt":"","text":"DeepFM在前面一篇文章中提到，目前遇到特征组合的问题，主流做法主要会分成两类：FM系列、DNN系列。关于DNN相关内容，是深度学习基础知识，本处不展开介绍，直接使用。本文主要介绍FM+DNN的结合体：DeepFM相关内容。文章依旧主要从三方面展开对FM算法介绍When – 什么时候需要考虑DeepFM算法What – 究竟什么是DeepFM算法How – DeepFM怎么使用 1. When什么时候需要考虑DeepFM基于CTR预估的推荐系统，究其根本，其实是学习到用户点击行为背后隐含的特征组合。在各种各样的推荐场景中，低阶特征组合或者高阶的特征组合都会对最终用户的行为产生影响。之前介绍的FM通过对每一维特征的隐变量内积提取特征组合，最终结果还算不错。虽然理论上FM可以对高阶特征组合进行建模，可是因为计算复杂度的原因一般到二阶的特征组合就结束。那么高阶特征该怎么办呢，此时你应该会和很多大牛的想法一样，通过多层的神经网络去解决。 1.1 DNN的局限性以下图片内容参考自张俊林教授在AI大会上的分享我们知道，对于离散特征的处理，一般都是把特征转换为one-hot编码形式，不过把one-hot编码类型的特征输入到DNN中，会导致网络参数过多：就如上图，从输入层到隐藏层，将会产生50亿的参数。解决上面这个问题的方法是：把特征分为不同的field，从one-hot变成dense vector:然后再添加两层全连接层，让其和dense vector进行组合，此时高阶特征的组合就搞定了不过上面方式把低阶和高阶特征组合隐含的体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。又该怎么做呢？ 1.2 模型融合方式此时，我们能想到的就是把DNN与FM进行一个合理的融合：二者的融合总的来说有两种形式，一是串行结构，二是并行结构。融合方式一：并行结构融合方式二：串行结构而我们今天要讲到的DeepFM，就是并行结构中的一种典型代表。 2. What究竟什么是DeepFM算法 2.1 简单介绍DeepFM是2017年华为诺亚方舟实验室发表的一篇论文。论文链接：https://arxiv.org/pdf/1703.04247.pdfDeepFM整体结构：根据上图，我们把图像分成左半部分和右半部分，其实**这也就是DeepFM包含的两部分：DNN部分和FM部分，其中DNN部分负责高阶特征的提取，FM部分负责低阶特征的提取。**这两部分共享同样的输入。 2.2 模型细节接下来，我们从下往上，分别看一下组成DeepFM的各个部分。 a) 架构间参数的传递架构间参数的传递，有几处需要注意，尤其是上面标记红色圈中部分。其中1中，是针对不同特征做的embedding【FM的二阶两两交互计算部分和 deep部分是共享这个embedding结果的】2是FM的一阶计算部分【使用权重直接对原始特征做的一阶计算】3是对应FM的二阶计算阶段，对经过权重embedding的结果做二阶交叉计算4是deep部分的全连接计算部分，使用神经网络进行计算 b) FM计算的过程FM的计算公式，我们在之前文章中讲过，此处拿来直接使用：y^(x)=w0+∑i=1nwixi+∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\hat{y}(x) = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}y^​(x)=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​上公式中，第一项和第二项公式对应上面标圈部分1的内容，即：w0+∑i=1nwixiw_0+\\sum^{n}_{i=1}{w_ix_i}w0​+i=1∑n​wi​xi​公式中第三项公式对应上面标圈部分2的内容，即：∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​ c) DNN部分DNN是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是极其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个特性：1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2）在FM里得到的隐变量VikV_{ik}Vik​现在作为了嵌入层网络的权重。 d) DeepFM预测结果输出最后，上DeepFM中FM和DNN预测结果的输出方式：y^=sigmoid(yFM+yDNN)\\hat{y}=sigmoid(y_{FM}+y_{DNN})y^​=sigmoid(yFM​+yDNN​) 3.HowDeepFM怎么使用推荐百度官方基于paddlepaddle实现框架：链接：https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/deepfm_dygraphPs：参考readme运行一遍就搞定。 参考资料：资料一：https://arxiv.org/pdf/1703.04247.pdf资料二：https://zhuanlan.zhihu.com/p/67795161资料三：http://wiki.baidu.com/pages/viewpage.action?pageId=765563246资料四：https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/deepfm_dygraph","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"因子分解机（Factorization Machines）","slug":"ML/因子分解机（Factorization Machines）","date":"2021-04-08T02:24:21.000Z","updated":"2022-08-10T08:49:13.000Z","comments":true,"path":"机器学习/ML/因子分解机（Factorization Machines）/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88Factorization%20Machines%EF%BC%89/","excerpt":"","text":"因子分解机（Factorization Machines）在推荐系统中，CTR（click-through rate）预估是非常重要的环节，其主要是用于判断一个商品是否被用于推荐。谈到CTR预估，有一个算法不得不提一下，LR（logistic regression）逻辑回归。在推荐系统发展的历史长河中，LR绝对有浓墨重彩的一笔。比如在2020年和微博做算法的同学交流，对方称他们依旧在推荐中使用LR，当然这离不开其非常容易实现大规模实时并行处理的优势。我们知道LR模型其实是一种线性的学习模型，所以它并不可以获取一些高阶特征（非线性）的信息。但是我们在处理多特征的推荐模型时，除了单特征外，有时候还想对一些特征进行组合。关于遇到特征组合的问题，目前主流做法主要分为两类：FM系列、DNN系列。本文将着重介绍FM算法。文章主要从三方面展开对FM算法介绍When – 什么时候需要考虑FM算法What – 究竟什么是FM算法How – FM怎么使用 1. When什么时候需要考虑FM算法需要考虑的情况：情况一：在建模过程中，除了考虑单个特征，还需要考虑特征与特征之间的关联信息时。比如，企业产品个性化定价中，我们除了想知道收入水平、教育水平对用户购买会员的影响，还想知道这两者组合起来对购买会员的影响。情况二：当特征下包含分类比较多的时候，如果通过one-hot处理，就会形成高维的稀疏矩阵，此时直接计算会导致计算量太大，特征权重更新较慢。比如，依旧是企业中个性化定价项目中，特征职业类别会包含很多分类，one-hot编码后，特征空间一下子就会暴增，导致计算量太大，甚至会形成维灾难。FM算法的优势就是对上面两种情况的处理。第一，进行特征组合，通过两两特征组合，引入了交叉项得分；其次，针对维灾难问题，通过引入隐向量，同时对参数矩阵进行矩阵分解，完成对特征的参数估计。 2. What究竟什么是FM算法 2.1 简单介绍因子分解机（Factorization Machines，简称为FM）是2010年由Steffen Rendle提出，是一种基于矩阵分解的机器学习算法。主要用于解决数据稀疏的业务场景下（如推荐业务），特征怎样组合的问题，可以用于求解分类、回归和排序问题。原文：Factorization Machines 2.2 公式推导在引出FM算法前，先看一下线性回归表达式：y=w0+∑i=1nwixiy = w_0+\\sum^{n}_{i=1}{w_ix_i}y=w0​+i=1∑n​wi​xi​其中w0w_0w0​为偏置，wiw_iwi​为每个特征xix_ixi​对应的权重值。我们在前面讨论过线性回归模型最大的缺点就是只能解决单个特征或者需要人工进行特征组合，那么是否可以把特征组合的能力体现在模型的层面呢，显然OK，如下公式：y=w0+∑i=1nwixi+∑i=1n∑j=i+1nwijxixjy = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{w_{ij}x_ix_j}}y=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​wij​xi​xj​上面公式，是把两两组合的特征引入模型了，但是又出现了另一个问题，这个组合的特征泛化能力太弱了，因为如果在训练数据中，xixj=0x_ix_j=0xi​xj​=0,那么wi,j=0w_{i,j}=0wi,j​=0。结果就是wi,jw_{i,j}wi,j​无法通过训练得出。为了求解wi,jw_{i,j}wi,j​，我们对每个特征分量xix_ixi​引入辅助向量vi=(vi1,vi2,...,vik)v_i=(v_{i1},v_{i2},...,v_{ik})vi​=(vi1​,vi2​,...,vik​)。然后，利用vivjTv_iv_j^Tvi​vjT​对wijw_{ij}wij​进行求解（根据矩阵分解思路：对于正定矩阵W，存在矩阵V，使得W=VVTW=VV^TW=VVT）；V=[v11v12⋯v1kv21v22⋯v2k⋮⋮⋱⋮vn1vn2⋯vnk]=[V1V2⋮Vn]V= \\left[ \\begin{matrix} v_{11} &amp; v_{12} &amp; \\cdots &amp; v_{1k} \\\\ v_{21} &amp; v_{22} &amp; \\cdots &amp; v_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ v_{n1} &amp; v_{n2} &amp; \\cdots &amp; v_{nk} \\\\ \\end{matrix} \\right] = \\left[ \\begin{matrix} V_{1} \\\\ V_{2} \\\\ \\vdots \\\\ V_{n} \\\\ \\end{matrix} \\right]V=⎣⎢⎢⎢⎡​v11​v21​⋮vn1​​v12​v22​⋮vn2​​⋯⋯⋱⋯​v1k​v2k​⋮vnk​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎢⎡​V1​V2​⋮Vn​​⎦⎥⎥⎥⎤​那么wijw_{ij}wij​组成的矩阵可以表示成：W^=VVT=[V1V2⋮Vn][V1TV2T⋯VnT]\\hat{W}=VV^T= \\left[ \\begin{matrix} V_{1} \\\\ V_{2} \\\\ \\vdots \\\\ V_{n} \\\\ \\end{matrix} \\right] \\left[ \\begin{matrix} V_{1}^{T} V_{2}^{T} &amp; \\cdots &amp; V_{n}^{T} \\\\ \\end{matrix} \\right]W^=VVT=⎣⎢⎢⎢⎡​V1​V2​⋮Vn​​⎦⎥⎥⎥⎤​[V1T​V2T​​⋯​VnT​​]此时，我们就可以得到FM的表达式：y^(x)=w0+∑i=1nwixi+∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\hat{y}(x) = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}y^​(x)=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​&lt;vi,vj&gt;=∑f=1kvifvjf&lt;v_i,v_j&gt; =\\sum^{k}_{f=1}{v_{if}v_{jf}}&lt;vi​,vj​&gt;=f=1∑k​vif​vjf​其中，n是特征数量， viv_{i}vi​是第iii维特征的隐向量， &lt;,&gt;代表向量点积。隐向量的长度为k&lt;&lt;nk&lt;&lt;nk&lt;&lt;n，包含 kkk个描述特征的因子。同时，直观判断上面表达式的复杂度是O(kn2)O(kn^2)O(kn2)FM现在被广泛应用的其中一个优点是可以通过数学公式化解，把原来表面上看起来O(kn2)O(kn^2)O(kn2)的复杂度降低为O(kn)O(kn)O(kn)。具体的化简过程如下：此时FM的公式就完全搞定。接下来一起看一下如何更新权重值。 2.3 权值求解可以采用随机梯度下降法SGD求解参数(当然其他类似SGD的方法都是可以的)：\\frac{\\partial }{\\partial \\theta}\\hat{y}(X)= \\begin{cases} 1, &amp;if\\ \\theta = w_0\\\\\\ x_i, &amp;if\\ \\theta = w_i\\\\\\ x_i\\sum_{j=1}^n{v_j, fx_j-v_i, fx^2_i}, &amp;if\\ \\theta =v_{i,f}\\\\ \\end{cases}其中，∑j=1nVjfxj\\sum^{n}_{j=1}{V_{jf}x_j}∑j=1n​Vjf​xj​和i无关，可以事先求出。计算过程中，每个梯度都可以在O(1)时间内求出，整体的参数更新时间为O(kn)。经过迭代之后就可以求出结果。 3. HowFM怎么使用使用模式一：通过xlearn搭建模型xlearn官方文档：https://xlearn-doc-cn.readthedocs.io/en/latest/index.html具体举例此处不再赘述，官方文档讲解非常直观。写在最后，产出本篇文档其实主要是为了后面DeepFM文章做铺垫，所以本文讲解重原理轻应用。 参考资料资料一：Factorization Machines资料二：https://zhuanlan.zhihu.com/p/58160982资料三：http://wiki.baidu.com/pages/viewpage.action?pageId=181930125","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"分类中解决类别不平衡问题","slug":"ML/分类中解决类别不平衡问题","date":"2019-08-11T02:28:21.000Z","updated":"2022-08-10T14:58:55.006Z","comments":true,"path":"机器学习/ML/分类中解决类别不平衡问题/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%88%86%E7%B1%BB%E4%B8%AD%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/","excerpt":"","text":"分类中解决类别不平衡问题在现实环境中，采集的数据（建模样本）往往是比例失衡的。比如网贷数据，逾期人数的比例是极低的（千分之几的比例）；奢侈品消费人群鉴定等。 1.类别不平衡数据集基本介绍在这一节中，我们一起看一下，当遇到数据类别不平衡的时候，我们该如何处理。在Python中，有Imblearn包，它就是为处理数据比例失衡而生的。安装Imblearn包1pip3 install imbalanced-learn第三方包链接：https://pypi.org/project/imbalanced-learn/创造数据集12345678910111213from sklearn.datasets import make_classificationimport matplotlib.pyplot as plt#使用make_classification生成样本数据X, y = make_classification(n_samples=5000, n_features=2, # 特征个数= n_informative（） + n_redundant + n_repeated n_informative=2, # 多信息特征的个数 n_redundant=0, # 冗余信息，informative特征的随机线性组合 n_repeated=0, # 重复信息，随机提取n_informative和n_redundant 特征 n_classes=3, # 分类类别 n_clusters_per_class=1, # 某一个类别是由几个cluster构成的 weights=[0.01, 0.05, 0.94], # 列表类型，权重比 random_state=0)查看各个标签的样本12345#查看各个标签的样本量from collections import CounterCounter(y)# Counter(&#123;2: 4674, 1: 262, 0: 64&#125;)数据集可视化123# 数据集可视化plt.scatter(X[:, 0], X[:, 1], c=y)plt.show()可以看出样本的三个标签中，1，2的样本量极少，样本失衡。下面使用imblearn进行过采样。接下来，我们就要基于以上数据，进行相应的处理。关于类别不平衡的问题，主要有两种处理方式：过采样方法增加数量较少那一类样本的数量，使得正负样本比例均衡。欠采样方法减少数量较多那一类样本的数量，使得正负样本比例均衡。 2.解决类别不平衡数据方法介绍 2.1 过采样方法 2.1.1 什么是过采样方法对训练集里的少数类进行“过采样”（oversampling），即增加一些少数类样本使得正、反例数目接近，然后再进行学习。 2.1.2 随机过采样方法随机过采样是在少数类 中随机选择一些样本，然后**通过复制所选择的样本生成样本集 ，**将它们添加到 中来扩大原始数据集从而得到新的少数类集合 。新的数据集 。通过代码实现随机过采样方法：12345678910111213# 使用imblearn进行随机过采样from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)#查看结果Counter(y_resampled) #过采样后样本结果# Counter(&#123;2: 4674, 1: 4674, 0: 4674&#125;)# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：对于随机过采样，由于需要对少数类样本进行复制来扩大数据集，造成模型训练复杂度加大。另一方面也容易造成模型的过拟合问题，因为随机过采样是简单的对初始样本进行复制采样，这就使得学习器学得的规则过于具体化，不利于学习器的泛化性能，造成过拟合问题。为了解决随机过采样中造成模型过拟合问题，又能保证实现数据集均衡的目的，出现了过采样法代表性的算法SMOTE算法。 2.1.3 过采样代表性算法-SMOTESMOTE全称是Synthetic Minority Oversampling即合成少数类过采样技术。SMOTE算法是对随机过采样方法的一个改进算法，由于随机过采样方法是直接对少数类进行重采用，会使训练集中有很多重复的样本，容易造成产生的模型过拟合问题。而SMOTE算法的基本思想:对每个少数类样本 ，从它的最近邻中随机选择一个样本 （ 是少数类中的一个样本），然后在 和 之间的连线上随机选择一点作为新合成的少数类样本。SMOTE算法合成新少数类样本的算法描述如下：对于少数类中的每一个样本 ，以欧氏距离为标准计算它到少数类样本集 中所有样本的距离，得到其k近邻。根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本 ，从其k近邻中随机选择若干个样本，假设选择的是 。对于每一个随机选出来的近邻 ，分别与 按照如下公式构建新的样本。我们用图文表达的方式，再来描述一下SMOTE算法。先随机选定一个少数类样本 。找出这个少数类样本 的K个近邻（假设K=5），5个近邻已经被圈出。随机从这K个近邻中选出一个样本 （用绿色圈出来了）。4)在少数类样本 和被选中的这个近邻样本 之间的连线上，随机找一点。这个点就是人工合成的新的样本点（绿色正号标出）。SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。代码实现：1234567891011# SMOTE过采样from imblearn.over_sampling import SMOTEX_resampled, y_resampled = SMOTE().fit_resample(X, y)Counter(y_resampled)# 采样后样本结果# [(0, 4674), (1, 4674), (2, 4674)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show() 2.2 欠采样方法 2.2.1 什么是欠采样方法直接对训练集中多数类样本进行“欠采样”（undersampling），即去除一些多数类中的样本使得正例、反例数目接近，然后再进行学习。 2.2.2 随机欠采样方法随机欠采样顾名思义即从多数类 中随机选择一些样样本组成样本集 。然后将样本集 从 中移除。新的数据集 。代码实现：123456789101112# 随机欠采样from imblearn.under_sampling import RandomUnderSamplerrus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)Counter(y_resampled)# 采样后结果[(0, 64), (1, 64), (2, 64)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但是这也存在一些问题。对于随机欠采样，由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。官网链接：https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"独立同分布","slug":"ML/独立同分布","date":"2019-06-08T02:24:21.000Z","updated":"2022-06-04T03:47:35.264Z","comments":true,"path":"机器学习/ML/独立同分布/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83/","excerpt":"","text":"独立同分布IID(independent and identically distributed) 1 独立同分布(i.i.d.)在概率统计理论中，如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。在西瓜书中解释是：输入空间中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。 2 简单解释 — 独立、同分布、独立同分布（1）独立：每次抽样之间没有关系，不会相互影响举例：给一个骰子，每次抛骰子抛到几就是几，这是独立；如果我要抛骰子两次之和大于8，那么第一次和第二次抛就不独立，因为第二次抛的结果和第一次相关。（2）同分布：每次抽样，样本服从同一个分布举例：给一个骰子，每次抛骰子得到任意点数的概率都是六分之一，这个就是同分布（3）独立同分布：i.i.d.，每次抽样之间独立而且同分布 3 机器学习领域的重要假设IID独立同分布即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。 4 目前发展机器学习并不总要求独立同分布，在不少问题中要求样本数据采样自同一个分布是因为希望用训练数据集得到的模型可以合理的用于测试数据集，使用独立同分布假设能够解释得通。目前一些机器学习内容已经不再囿于独立同分布假设下，一些问题会假设样本没有同分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"如何理解无偏估计？","slug":"ML/如何理解无偏估计","date":"2019-05-20T02:28:21.000Z","updated":"2022-08-10T14:58:37.847Z","comments":true,"path":"机器学习/ML/如何理解无偏估计/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/","excerpt":"","text":"如何理解无偏估计？ 1.如何理解无偏估计无偏估计：就是我认为所有样本出现的概率一样。假如有N种样本我们认为所有样本出现概率都是1/N。然后根据这个来计算数学期望。此时的数学期望就是我们平常讲的平均值。数学期望本质就是平均值 2.无偏估计为何叫做“无偏”？它要“估计”什么？首先回答第一个问题：它要“估计”什么？它要估计的是整体的数学期望（平均值）。第二个问题：那为何叫做无偏？有偏是什么？假设这个是一些样本的集合:$$X=x_1, x_2, x_3,…,x_N$$ 我们根据样本估计整体的数学期望（平均值）。因为正常求期望是加权和，什么叫加权和这个就叫加权和。每个样本出现概率不一样，概率大的乘起来就大，这个就产生偏重了（有偏估计）。但是，但是我们不知道某个样本出现的概率啊。比如你从别人口袋里面随机拿了3张钞票。两张是十块钱，一张100元，然后你想估计下他口袋里的剩下的钱平均下来每张多少钱（估计平均值）。然后呢？无偏估计计算数学期望就是认为所有样本出现概率一样大，没有看不起哪个样本。回到求钱的平均值的问题。无偏估计我们认为每张钞票出现概率都是1/2（因为只出现了10和100这两种情况，所以是1/2。如果是出现1 10 100三种情况，每种情况概率则是1/3。哪怕拿到了两张十块钱，我还是认为十块钱出现的概率和100元的概率一样。不偏心。所以无偏估计，所估计的别人口袋每张钱的数学期望（平均值）=10 * 1/2+100 * 1/2。有偏估计那就是偏重那些出现次数多的样本。认为样本的概率是不一样的。我出现了两次十块钱，那么我认为十块钱的概率是2/3，100块钱概率只有1/3. 有偏所估计的别人口袋每张钱的数学期望（平均值）=10 * 2/3+100 * 1/3。 3.为何要用无偏估计？因为现实生活中我不知道某个样本出现的概率啊，就像骰子，我不知道他是不是加过水银。所以我们暂时按照每种情况出现概率一样来算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"模型的保存和加载","slug":"ML/模型的保存和加载","date":"2019-05-09T02:28:21.000Z","updated":"2022-08-10T14:58:45.728Z","comments":true,"path":"机器学习/ML/模型的保存和加载/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/","excerpt":"","text":"模型的保存和加载 1.sklearn模型的保存和加载APIfrom sklearn.externals import joblib保存：joblib.dump(estimator, ‘test.pkl’)加载：estimator = joblib.load(‘test.pkl’) 2.线性回归的模型保存加载案例1234567891011121314151617181920212223242526272829303132333435363738def load_dump_demo(): \"\"\" 模型保存和加载 :return: \"\"\" # 1.获取数据 data = load_boston() # 2.数据集划分 x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=22) # 3.特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-线性回归(岭回归) # # 4.1 模型训练 # estimator = Ridge(alpha=1) # estimator.fit(x_train, y_train) # # # 4.2 模型保存 # joblib.dump(estimator, \"./data/test.pkl\") # 4.3 模型加载 estimator = joblib.load(\"./data/test.pkl\") # 5.模型评估 # 5.1 获取系数等值 y_predict = estimator.predict(x_test) print(\"预测值为:\\n\", y_predict) print(\"模型中的系数为:\\n\", estimator.coef_) print(\"模型中的偏置为:\\n\", estimator.intercept_) # 5.2 评价 # 均方误差 error = mean_squared_error(y_test, y_predict) print(\"误差为:\\n\", error) 3.tips如果你在学习过程中，发现使用上面方法报如下错误：1ImportError: cannot import name 'joblib' from 'sklearn.externals' (/Library/Python/3.7/site-packages/sklearn/externals/__init__.py)这是因为scikit-learn版本在0.21之后，无法使用from sklearn.externals import joblib进行导入，你安装的scikit-learn版本有可能是最新版本。如果需要保存模块，可以使用：12345# 安装pip install joblib# 导入import joblib安装joblib,然后使用joblib.load进行加载；使用joblib.dump进行保存参考：https://scikit-learn.org/stable/modules/model_persistence.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"ROC曲线的绘制","slug":"ML/ROC曲线的绘制","date":"2019-03-11T02:28:21.000Z","updated":"2022-08-10T14:59:09.912Z","comments":true,"path":"机器学习/ML/ROC曲线的绘制/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/ROC%E6%9B%B2%E7%BA%BF%E7%9A%84%E7%BB%98%E5%88%B6/","excerpt":"","text":"ROC曲线的绘制关于ROC曲线的绘制过程，通过以下举例进行说明假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。然后在这6次展示的时候都通过model算出了点击的概率序列。下面看三种情况。 1.曲线绘制 1.1 序列一曲线绘制如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）1100000.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 1.2 序列二曲线绘制如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）与原来的序列一起，得到序列（从概率从高到低排）1010000.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；3）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；4）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 1.3 序列三曲线绘制如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）与原来的序列一起，得到序列（从概率从高到低排）0000110.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（6:0.9,5:0.8,4:0.7,2:0.6,3:0.5,1:0.4）；2）从概率最大开始取一个点作为正类，取到点6，计算得到TPR=0.0，FPR=0.25；3）从概率最大开始，再取一个点作为正类，取到点5，计算得到TPR=0.0，FPR=0.5；4）再从最大开始取一个点作为正类，取到点4，计算得到TPR=0.0，FPR=0.75;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0.25,0.0),(0.5,0.0),(0.75,0.0),(1.0,0.0),(1.0,0.5),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 2.意义解释如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"2019-春节假期生活杂感","slug":"程序人生/2019年春节假期生活杂感","date":"2019-02-05T05:32:15.000Z","updated":"2022-05-14T16:04:47.503Z","comments":true,"path":"程序人生/程序人生/2019年春节假期生活杂感/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/2019%E5%B9%B4%E6%98%A5%E8%8A%82%E5%81%87%E6%9C%9F%E7%94%9F%E6%B4%BB%E6%9D%82%E6%84%9F/","excerpt":"","text":"又是和往年一样的春节放假，依旧是常规的操作，放假回来，在家懒着，然后腊月底和爸一起去上坟，最后几天贴春联，挂灯笼等操作。【三十而立，面包杂想】现在是腊三十，不管从阳历还是阴历，自己总算要又长一岁了，虚岁要步入三十大关。古代三十而立，在此插入，非常有必要解释一下，到底什么是三十而立12345孔子所说的“三十而立”，是指他在这个时候做事合于礼，言行都很得当。 [1] 言：谦卑 中 传递祥和！行：举止 彬彬有礼！现常用来指人开始有所成就。三十而立：三十岁的时候就可以自立于世。2019-02-05(正月初一)接着上次的内容继续扯东扯西，三十而立，就自己理解，立的内容是两方面吧，一个是精神层面的面包，一个是真实世界的面包。关于精神层面，具体而言，自从从事计算机相关的工作，这个并没有什么大的进步，关于真实世界的面包，可能限于环境的限制，更多的考虑的是这方面的内容吧，有房吗，有车吗。。。有时候醒悟，面包不仅仅一直局限于真实世界的单一口味，还需要充实自己的精神世界。最好的目标彼岸，真实环境，精神层面都有面包收获，这应该是自己期望达到的目标吧。【2019-沉淀再想】这个假期，说实在的，过得不是那么心安理得，因为有很多事情需要自己去完成，但是都没有去行动，比如一直说要去完成的那篇论文，比如说要准备机器学习，再比如说对自己人工智能的充电。就那么没心没肺的过着，但是当回想的时候，还是蛮有愧疚感的。可能春节回家继续办公、学习，这件事情本身就比较反人类，哈哈。既然回家之前把目标确定了，那么就在接下来的这几天努力尽量完成目标，别加深自己的罪恶感。关于假期目标，其实是希望达到高度的折射，也从另一方面说明了2019必须干的事情。12如何沉淀自己人工智能相关领域的知识？需要自己计划好，然后去坚持完成关于2019的基调，需要主要精力去做的事情，都需要自己定位好，然后去严格的执行，只有一步步的去执行，才能在最后获取到相应的高度，加油！！【关于潮流变化】假期回家，被父母每天观看的快手充斥着，自己也为了体验一下究竟什么样产品可以勾住几乎所有人，下载浏览，快手给我带来更大的吃惊之处是，我妈竟然在昨天从快手中提出20元，这个是她这段时间直播的收入，瞬间觉得，这个社会发展的和自己之前的理解有些偏差。这个社会在追求快速发展的同时也失去了一些东西。之前自己是一直不下载观看快手之类的app的，因为觉得里面的内容，大部分都没有营养，看的过程中，也就是当时博君一笑，尔尔。但是当这个东西还可以赚钱，在利益的驱动下，是否会有更多没有营养的内容被鼓励产生。产品的制作，宣传已经没有了底线可言，怎么让产品更符合大众，更能吊起大众的胃口，怎么设计。如此产品，怎么经得起深究思考呢？或者，产本设计者自己就没有设计一个底线，只是在追求利益。","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]}],"categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"},{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"},{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"},{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"},{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"},{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"},{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"},{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"},{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]}