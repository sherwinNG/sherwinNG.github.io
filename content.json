{"meta":{"title":"sherwinNG's blog","subtitle":"遇见更好的自己","description":"The way of the future!","author":"sherwin","url":"http://sherwinzhang.com","root":"/"},"pages":[{"title":"404","date":"2022-09-17T08:30:30.358Z","updated":"2020-02-19T01:33:16.829Z","comments":false,"path":"404.html","permalink":"http://sherwinzhang.com/404.html","excerpt":"","text":"这是网页标题"},{"title":"","date":"2022-09-17T08:30:30.382Z","updated":"2022-06-03T07:15:12.823Z","comments":false,"path":"google13fc42b677fff64a.html","permalink":"http://sherwinzhang.com/google13fc42b677fff64a.html","excerpt":"","text":"google-site-verification: google13fc42b677fff64a.html"},{"title":"关于","date":"2023-04-08T07:57:56.030Z","updated":"2023-04-08T07:57:56.020Z","comments":false,"path":"about/index.html","permalink":"http://sherwinzhang.com/about/index.html","excerpt":"","text":"你是谁Sherwin, 已过而立之年，正在追求“立”之支柱。职业：深度学习、机器学习算法研究、优化（调参侠转为面向prompt编程）。写作该博客的目的：希望在成长的过程中，留下点滴痕迹。 你的技术背景深度学习:深度学习框架 (Pytorch, Tensorflow, PaddlePaddle),深度学习算法(RNN系列、CNN系列),自然语言处理大模型 (Transformer, Bert系列, Ernie系列, GPT系列等)机器学习:机器学习框架 (scikit-learn),机器学习算法 (LR, SVM, GBDT, XGBOOST, LightGBM等)数据分析和处理:Python (Numpy, Pandas, Seaborn), MATLAB, R; Mysql, Redis 你的博客主题：技术文章分享随想杂谈 你的写作风格理科男的固有朴实 你的兴趣爱好骑行、羽毛球（水平：2.5 - 3.0）、篮球 一些你的tagPythoner、机器学习、深度学习、自然语言处理、智能客服、对话系统、AI布道师以终为始、人生有限公司更新于 2023.1"},{"title":"书单","date":"2022-09-17T08:30:30.380Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"books/index.html","permalink":"http://sherwinzhang.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-09-17T08:30:30.381Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"categories/index.html","permalink":"http://sherwinzhang.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-09-17T08:30:30.383Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"links/index.html","permalink":"http://sherwinzhang.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-09-17T08:30:30.383Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"repository/index.html","permalink":"http://sherwinzhang.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-09-17T08:30:30.384Z","updated":"2022-03-21T07:18:46.000Z","comments":false,"path":"tags/index.html","permalink":"http://sherwinzhang.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"GPT-4 幻觉问题（hallucinations）","slug":"chatGPT/GPT-4 幻觉问题（hallucinations）","date":"2023-03-21T02:24:21.000Z","updated":"2023-04-18T23:49:09.789Z","comments":true,"path":"chatGPT/chatGPT/GPT-4 幻觉问题（hallucinations）/","link":"","permalink":"http://sherwinzhang.com/chatGPT/chatGPT/GPT-4%20%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98%EF%BC%88hallucinations%EF%BC%89/","excerpt":"","text":"GPT-4 幻觉问题（hallucinations） 1. 什么是 GPT-4的幻觉问题GPT-4是OpenAI最先进的多模态模型，能够在各种基准测试中表现出人类水平 。但它也有幻觉问题（hallucinations），即生成一些错误或无意义的内容 。这可能会误导用户，损害GPT-4的信誉和可靠性，甚至造成风险和危害。这种现象被称为幻觉，因为它们类似于人类在感知事物时，看到或听到一些不存在或与现实相悖的东西。OpenAI 自己进行的答案真实性测试中， gpt-4 准确率在70%-80%之间，chatgpt-v4准确率在50%-60%之间， 因为测试集是特定构造的，具有相对意义，OpenAI结论是:“GPT-4 scores 40% higher than our latest GPT-3.5 on our internal adversarial factuality evaluations” 2. 为什么会有 GPT-4的幻觉问题GPT-4的幻觉问题主要有三个原因：数据噪声：GPT-4从互联网上收集的数据中学习，但这些数据并不都是准确或高质量的。模型局限：GPT-4在某些领域或任务上还没有达到人类水平，甚至低于人类水平。用户指令：用户提供的指令或请求可能不清晰、合理或可行，导致GPT-4无法正确地回应。 3. GPT-4的幻觉问题有哪些影响GPT-4的幻觉问题可能会对用户、开发者和社会产生不利影响：用户体验：GPT-4的幻觉问题可能会降低用户对GPT-4的信任和满意度，影响用户体验。开发者责任：GPT-4的幻觉问题可能会增加开发者的责任和风险，要求开发者更加谨慎和负责地使用和部署GPT-4 。社会影响：GPT-4的幻觉问题可能会对社会造成负面影响，挑战社会的价值和规范 。 4. GPT-4 幻觉问题的模型层面原因分析GPT 系列模型本质能力是预测下一个词是什么，这样就使其全局规划的能力。例子1第一个问题回答错误但是如果我们让其列出所有数字，然后计数，它可以给出正确答案。例子2GPT-4 直接回答如下这种简单算术题只有58%的正确率，然而，如果GPT-4“慢慢来”回答这个问题，那么准确率很容易就会上升。例如，如果我们要求模型使用以下提示写下中间步骤：以下表达式的值是多少？116 * 114 + 178 * 157 = ? 让我们一步一步地思考如何求解表达式，记下所有中间步骤，然后得出最终解。则当数字在区间1−40内时，准确度会达到100%，而在区间1–200内，准确度则会达到90%。例子3这些例子说明了下一个词预测范式的一些局限性，这些局限性表现为模型缺乏规划、工作记忆、回溯和推理能力。模型依赖于一个局部和贪婪的过程来生成下一个词，而没有对任务或输出的全局或深入的理解。因此，模型擅长生成流畅和连贯的文本，但在解决复杂或创造性的问题方面有局限性，这些问题不能以顺序的方式来处理，因而我们可以对任务进行如下分类：任务类型定义例子渐进式任务可以通过逐渐添加有助于解决问题的词或句子来完成的任务，不需要太多的创造力或洞察力，而只需要运用已有的知识和技能。写一篇文本的摘要，回答事实性问题，根据给定的押韵方案创作一首诗，或者解决一个遵循标准程序的数学问题。非连续性任务不能通过逐渐添加词或句子来完成的任务，而需要一个突破性的想法来解决问题，需要发现或创造一种新的视角或框架。解决一个需要对公式进行新颖或创造性应用的数学问题，写一个笑话或一个谜语，提出一个科学假设或一个哲学论证，或者创造一种新的写作体裁或风格。当前GPT对渐进式任务能处理的很好， 而对非连续性任务则比较困难。 模型在执行需要提前规划或需要一个“灵光一闪”的创意的任务时，表现出困难，换句话说，模型在需要人类天才那种形式的概念跳跃的任务上，表现不佳。 4. 如何应对 GPT-4的幻觉问题针对GPT-4的幻觉问题，我们可以从以下四个方面采取应对措施： 通用方法数据处理：通过清洗和扩充数据，提高数据的质量和覆盖度，为模型提供更好的学习材料。模型优化：通过改进、评估和对齐模型，增加模型的深度、宽度、复杂度和对齐度，提升模型的能力、常识、问题解决和安全性，并使用对抗测试或人工评估检测和纠正模型的错误输出。用户交互：通过引导和监督用户，提供清晰、合理、可行和安全的指令或请求，减少沟通障碍，提高用户体验和满意度。多模态输入：通过结合图像和文本信息，提高模型的语义理解和表达能力。 我们重点关注的改善模型校准：让模型能够评估自己的信心水平，当它不确定答案时，要么不回答，要么给出一个信心区间或概率。插入外部信息：让模型能够利用其他的信息源，如搜索引擎、知识库、数据库等，来补充自己缺乏的知识，避免编造事实。后期检查：让模型在生成内容后，对其进行一些逻辑或事实的检验，发现并纠正可能的错误或矛盾。优化用户体验：让模型在与用户交互时，明确表达自己的能力和局限，提醒用户注意可能的幻觉，引导用户进行合理的使用和判断。 5. 后续方向当前模型中存在的幻觉问题，无疑是后续研究的重点方向， 后续可继续关注此方面的进展。《Reflexion: an autonomous agent with dynamic memory and self-reflection》3 是一篇在arXiv上发表的论文，提出了一种赋予智能体动态记忆和自我反思能力的方法，以增强其现有的推理追踪和任务特定的动作选择能力。论文基于最近的研究，提出了Reflexion这种方法，使智能体能够通过一个简单而有效的启发式来发现幻觉实例，避免动作序列中的重复，并在某些环境中构建一个内部记忆地图。论文评估了智能体在AlfWorld环境中完成决策任务和在HotPotQA环境中完成知识密集型、搜索型问答任务的能力，并观察到了97%和51%的成功率，并对自我反思这一涌现性质进行了讨论。 6. 参考资料OpenAI GPT-4 官方技术报告《Sparks of Artificial General Intelligence: Early experiments with GPT-4》","categories":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/categories/chatGPT/"}],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/tags/chatGPT/"}]},{"title":"chatGPT相关大佬的解读","slug":"chatGPT/chatGPT相关大佬的解读","date":"2023-03-03T02:24:21.000Z","updated":"2023-04-18T23:42:43.263Z","comments":true,"path":"chatGPT/chatGPT/chatGPT相关大佬的解读/","link":"","permalink":"http://sherwinzhang.com/chatGPT/chatGPT/chatGPT%E7%9B%B8%E5%85%B3%E5%A4%A7%E4%BD%AC%E7%9A%84%E8%A7%A3%E8%AF%BB/","excerpt":"","text":"chatGPT相关大佬的解读随时更新中 李宏毅【自然语言处理】台大李宏毅 - 来自暗黑大陆的模型 GPT-3_哔哩哔哩_bilibiliChatGPT (可能)是怎么炼成的 - GPT社会化的过程 (李宏毅)_哔哩哔哩_bilibili算法领域的“大力出奇迹”：ChatGPT！李宏毅教授重磅解读ChatGPT的底层逻辑！感兴趣的同学赶紧收藏学习了！_哔哩哔哩_bilibiliChatGPT原理剖析 李宏毅_哔哩哔哩_bilibili【生成式AI】【李宏毅】GPT-4 來了! GPT-4 這次有什麼神奇的能力呢？_哔哩哔哩_bilibili 李沐GPT，GPT-2，GPT-3 论文精读【论文精读】_哔哩哔哩_bilibiliInstructGPT 论文精读【论文精读·48】_哔哩哔哩_bilibiliGPT-4论文精读【论文精读·53】_哔哩哔哩_bilibili 车万翔车万翔CHATGPT调研报告 知乎GPT / GPT-2 / GPT-3 / InstructGPT 进化之路预训练语言模型之GPT-1，GPT-2和GPT-3ChatGPT/InstructGPT详解通向AGI之路：大型语言模型（LLM）技术精要","categories":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/categories/chatGPT/"}],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/tags/chatGPT/"}]},{"title":"如何监测部署服务是否正常运行，同时挂掉后自动重启","slug":"计算机相关/如何监测部署服务是否正常运行，同时挂掉后自动重启","date":"2023-02-20T11:12:42.000Z","updated":"2023-02-20T11:12:58.000Z","comments":true,"path":"计算机相关/计算机相关/如何监测部署服务是否正常运行，同时挂掉后自动重启/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E5%A6%82%E4%BD%95%E7%9B%91%E6%B5%8B%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%EF%BC%8C%E5%90%8C%E6%97%B6%E6%8C%82%E6%8E%89%E5%90%8E%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF/","excerpt":"","text":"如何监测部署服务是否正常运行，同时挂掉后自动重启 1.需求在服务器中启动服务，偶尔服务会挂断。通过Python实现脚本进行端口监测，失败后自动重启，同时发送信息到终端接收。 2.实现思路：1.通过subprocess 实现端口重启2.给接收终端发送信息3.通过socket实现端口检测4.通过crontab部署定时监测 3.具体实现前三步骤实现方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import socketimport subprocessimport requestsimport jsonimport logging# 日志记录logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)logger = logging.getLogger(__name__)# 配置host = '*.*.*.*' # 主机名compre_port = **** # 端口号compre_start_serv_path = './start.sh' # 启动脚本路径# 信息发送def send_msg(text): # 通过邮件等方式发送 pass logging.info(\"信息发送成功！\")# 端口重启def serv_restart(port, start_serv_path): subprocess.run(['sh', start_serv_path]) logging.info(\"重新开启端口：&#123;&#125;\".format(port))# 端口检测def port_check(text, port, start_serve_path): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex((host, port)) # 端口正常连接 if result == 0: logging.info(\"Port &#123;&#125; is open on &#123;&#125;\".format(port, host)) # 端口出现异常,1-发送信息，2-重启端口 else: logging.info(\"Port &#123;&#125; is close on &#123;&#125;\".format(port, host)) # 信息发送 send_msg(text) # 重启端口 serv_restart(port, start_serve_path) sock.close()if __name__ == \"__main__\": # 端口检测 text = '综合' port_check(text, compre_port, compre_start_serv_path)通过crontab 部署定时任务1* * * * * Command具体可以参考：https://www.runoob.com/w3cnote/linux-crontab-tasks.html","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"chatGPT API使用以及参数说明","slug":"chatGPT/chatGPT-api","date":"2023-02-08T02:24:21.000Z","updated":"2023-03-31T00:53:45.738Z","comments":true,"path":"chatGPT/chatGPT/chatGPT-api/","link":"","permalink":"http://sherwinzhang.com/chatGPT/chatGPT/chatGPT-api/","excerpt":"","text":"OpenAI 官方发布chatGPT(GPT3.5) API 使用指南该篇文章为 NLPer 和 chatGPT 合作完成。3月2日，OpenAI放出了真正的ChatGPT API，本次提供的API是真正的模型，就像在说明文档中第一句话：ChatGPT is powered by gpt-3.5-turbo, OpenAI’s most advanced language model.ChatGPT由OpenAI最先进的语言模型gpt-3.5-turbo提供支持。接下来，通过如下几步骤，一起看看如何使用chatGPT API：获取API密钥安装依赖项创建API调用以及处理响应 1.获取API密钥要使用GPT-3.5 Turbo，需要先获取API密钥。请访问OpenAI的官方网站并注册一个帐户。注册后，可以通过如下截图页面获得API密钥。OpenAI官方网站：https://openai.com/如何注册openAI账户，网上很多资料，在此不再赘述。步骤一：步骤二：创建完成后，secret key 一定不要泄露，要不自己账户的钱可能会被别人使用。 2.安装依赖项要使用GPT-3.5 Turbo，需要安装适当的依赖项。如果使用Python，可以通过pip安装openai包。1pip3 install openai 3.创建API调用以及处理响应12345678910111213141516171819202122232425262728# 导入openai库import openai def openai_reply(content, apikey): # 设置API密钥 openai.api_key = apikey # 构造请求 -- 参数具体说明后面解释 response = openai.ChatCompletion.create( model=\"gpt-3.5-turbo-0301\", # gpt-3.5-turbo-0301 messages=[ &#123;\"role\": \"user\", \"content\": content&#125; ], temperature=0.5, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0, ) return response.choices[0].message.content if __name__ == '__main__': content = '为一个&#123;年龄&#125;岁的人写一个800字的童话故事，这个故事要有一定的寓意。' ans = openai_reply(content, '你的secret key') print(ans)通过以上方式，就可以实现chatGPT API 的调用体验。 4.其他说明 4.1 调用参数解释没有注明是 [必须] 的，为可选参数model[必须]：要使用的模型ID。目前仅支持gpt-3.5-turbo和 gpt-3.5-turbo-0301；gpt-3.5-turbo-0301模型有效期到6月1日，而gpt-3.5-turbo会持续更新。messages[必须]：以聊天格式生成聊天完成的消息。messages字段组成部分包括角色role和content问题两个部分组成，如下所示： messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}, {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"}, {\"role\": \"user\", \"content\": \"Where was it played?\"} ] 在gpt-3.5-turbo模型中，角色role包含system系统、assistant助手和用户user三种类型。System角色相当于告诉ChatGPT具体以何种角色回答问题，需要在content中指明具体的角色和问题内容。而gpt-3.5-turbo-0301主要区别在于更加关注问题内容，而不会特别关注具体的角色部分。assistant助手和用户user则相当于已经指明了角色，content直接写入关注的问题即可。temperature：使用什么采样温度，介于 0 和 2 之间。较高的值（如 0.8）将使输出更加随机，而较低的值（如 0.2）将使输出更加集中和确定。top_p：一种替代 temperature 采样的方法，称为核采样(nucleus sampling)，模型考虑具有 top_p 概率质量标记的结果。所以 0.1 意味着只考虑构成前 10% 概率质量的标记。官方建议改变这个或temperature参数，不需要两者同时改变。n：为每个输入消息生成多少个聊天完成选项。比如n=3,就会生成3个聊天应答结果。stream：是否使用控制流的方式输出，默认为false如果stream取值为False，返回全部文字结果，可通过response[“choices”][0][“text”]进行读取。但是，字数越多，等待返回时间越长。如果steam取值为True时，那么返回结果是一个Python generator，需要通过迭代获取结果，大约每秒钟4个字。stop：达到4个序列时，API 停止生成更多的 token.max_tokens：最大生成token数生成的答案允许的最大 token 数。默认情况下，模型可以返回的标记数为（4096 - prompt token）。presence_penalty：-2.0 和 2.0 之间的数字。正值会根据到目前为止是否出现在文本中来惩罚新 token，从而增加模型谈论新主题的可能性。frequency_penalty：-2.0 和 2.0 之间的数字。正值会根据新标记在文本中的现有频率对其进行惩罚，从而降低模型逐字重复同一行的可能性。logit_bias：偏差设置。user：用户的唯一标识符，可以帮助 OpenAI 监控和检测滥用行为。参考官方文档：https://platform.openai.com/docs/api-reference/chat/create 4.2 费用说明调用费用为0.002美元/1000tokens。注意：1.这个字数包括问题和返回结果字数。2.token解释：在英语中，token 可以短至一个字符，也可以长至一个单词（例如，a或apple），在某些语言中，token 甚至可以短于一个字符，甚至长于一个单词。例如，字符串“ChatGPT is great!”被编码为六个标记：[“Chat”, “G”, “PT”, “ is”, “ great”, “!”]. 4.3 是否可以进行微调gpt-3.5-turbo？不可以。自 2023 年 3 月 1 日起，您只能微调基础 GPT-3 模型。","categories":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/categories/chatGPT/"}],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/tags/chatGPT/"}]},{"title":"GPT系列论文&报告","slug":"chatGPT/GPT系列文章","date":"2023-01-21T02:24:21.000Z","updated":"2023-04-18T23:39:35.116Z","comments":true,"path":"chatGPT/chatGPT/GPT系列文章/","link":"","permalink":"http://sherwinzhang.com/chatGPT/chatGPT/GPT%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/","excerpt":"","text":"GPT系列论文&amp;报告随时更新中英文全称：Generative Pre-trained Transformer演进历程：GPT-1(2018-06)：Improving Language Understanding by Generative Pre-trainingGPT-2(2019-02)：Language Models are unsupervised multitask learnersGPT-3(2020-05)：Language models are few shot learnersGPT主要是基于 Google 2017-06发出的《Attention is All Your Need》文章中 Transformer 的 decoder 架构，2018-06 提出了第一版GPT模型，2018-10 Google 提出了Bert 模型。GPT可以实现功能：可以实现文本补全、代码补全、翻译、自然语言到计算机语言的转换、聊天等功能非常多，具体可以参考官网：https://platform.openai.com/examples 原论文&amp;报告 GPT-1论文链接：GPT-1在GPT-1之前（和ELMo同一年），传统的NLP模型往往使用大量的数据对有监督的模型进行任务相关的模型训练，但是这种有监督学习的任务存在两个缺点：需要大量的标注数据，而高质量的标注数据往往很难获得，一方面需要很多事后需要专业的技术人员做标注，另一方面数据的获取非常昂贵。根据一个任务训练的模型很难泛化到其它任务中，这个模型只能叫做“领域专家”而不是真正的理解了NLP。GPT-1的思想是先通过在无标签的数据上学习一个生成式的语言模型，然后再根据特定热任务进行微调，处理的有监督任务包括：自然语言推理问答和常识推理语义相似度分类总结来说，GPT-1就是无监督训练+有监督微调。 GPT-2论文链接：GPT-2GPT-2的目标旨在训练一个泛化能力更强的词向量模型，它并没有对GPT-1的网络进行过多的结构的创新与设计，只是使用了更多的网络参数和更大的数据集。GPT-2的学习目标是使用无监督的预训练模型做有监督的任务。当一个语言模型足够大时，它就足矣覆盖所有的有监督任务，即所有的有监督学习都是无监督语言模型的一个子集。GPT-2的最大贡献是验证了通过海量数据和大量参数训练出来的词向量模型有迁移到其它类别任务中而不需要额外的训练。但是很多实验也表明，GPT-2的无监督学习的能力还有很大的提升空间。 GPT-3论文链接：GPT-3GPT3的参数量达到了海量级别，1750亿的参数量，45TB的训练数据使得它成为2020年最强大的语言模型。In-context learning是这篇论文中介绍的一个重要概念，它在GPT3中类似于内循环，而整体的针对不同任务的整体优化则是外循环。在大量的语言模型数据集中，GPT-3超过了绝大多数的zero-shot或者few-shot的state-of-the-art方法。另外GPT-3在很多复杂的NLP任务中也超过了fine-tune之后的state-of-the-art方法，例如闭卷问答，模式解析，机器翻译等。除了这些传统的NLP任务，GPT-3在一些其他的领域也取得了非常震惊的效果，例如进行数学加法，文章生成，编写代码等。 GPT-3.5论文链接：GPT-3.5GPT3.5，也就是InstructGPT/ChatGPT，他们采用了GPT-3的网络结构，通过指示学习构建训练样本来训练一个反应预测内容效果的奖励模型（RM），最后通过这个奖励模型的打分来指导强化学习模型的训练。这个训练过程主要分为三步：根据采集的SFT数据集对GPT-3进行有监督的微调（Supervised FineTune，SFT）；收集人工标注的对比数据，训练奖励模型（Reword Model，RM）；使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型。GPT3.5引入了人工标注之后，让模型的“价值观”和的正确程度和人类行为模式的“真实性”上都大幅的提升。 GPT-4技术报告：GPT-4结论：GPT-4朝着AGI迈出了坚实一步，但离AGI仍然遥远。GPT-4的智能模式与人类智能具有非常大的差异。GPT-4无法创造新知识GPT-4能够从纯文本中产生视觉概念GPT-4在代码理解上的能力达到前所未有的高度在数学计算能力上远远落后于其他能力GPT-4仍然缺乏常识GPT-4能够使用工具在外部世界进行交互，但仍然存在一些典型问题GPT-4在理解意图、情绪方面具有显著进步由于GPT-4采用的自回归结构，导致它在规划能力上、working memory上明显不足。GPT-4导致的社会问题主要包括错误答案、被错误使用、偏见和进一步加剧不平等。","categories":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/categories/chatGPT/"}],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/tags/chatGPT/"}]},{"title":"推荐系统通俗介绍","slug":"推荐系统/推荐系统通俗介绍","date":"2022-11-18T15:06:11.000Z","updated":"2022-11-19T06:59:43.499Z","comments":true,"path":"推荐系统/推荐系统/推荐系统通俗介绍/","link":"","permalink":"http://sherwinzhang.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%80%9A%E4%BF%97%E4%BB%8B%E7%BB%8D/","excerpt":"本文主要对推荐系统做了基本介绍，从推荐系统为什么会出现，然后讲解了推荐系统基本概念，以及推荐和搜索的区别；梳理了推荐系统的发展历史，推荐系统主要架构，部分推荐系统案例；最后自己设计了一个视频推荐系统整体流程。","text":"推荐系统通俗介绍资料整理，来源于北大刘宏志教授讲座内容。在介绍推荐系统前，我们先想一个问题，为什么要有推荐系统？在推荐系统没有出现前，难道我们就不能更好的生活？其实这方面主要归因于，互联网技术的迅猛发展，带来信息爆炸，进而我们接触到的信息都是超载的。我们将转变接受信息的理念：”多即是少、少即是多“。试想，我们去逛超市，面对琳琅满目的同类商品，是不是也会犯”选择困难症“。 1.推荐系统概念关于推荐系统概念，我们可以认为：推荐系统是一种主动的信息过滤系统；即将信息过滤的过程由”用户主动搜索“转变为”系统主动推送“。什么样的系统需要用户主动搜索呢？我们常见的就是不同的搜索引擎，如：百度、Google等。用户主动搜索建立在两个前提：用户知道自己要什么用户知道自己该如何描述而推荐是挖掘并且满足用户的潜在需求，如：今日头条、Amazon等。同时，推荐系统还是一种双边匹配系统，把恰当的商品（信息）推荐给人帮助用户发现其所喜好的或需要的小众、非主流商品；帮助商户将其商品展现在对它们感兴趣的用户面前。这一切正如《长尾》的作者 Chris Anderson 所言：We are leaving the age of information and entering the age of recommendation. 2.推荐系统发展历史如果我们想要遇见将来，适当的途径是研究这门学科的历史和现状。信息过载、推荐系统，这些词语并不是最近才被人提及。推荐系统的整个发展历程又是怎么变化的呢，请看下面梳理内容。参考资料可以点击下原文查看参考自：https://cloud.tencent.com/developer/article/1652169 3.个性化推荐系统框架基于公式的描述：映射函数 f:U×I→Rf:U×I→Rf:U×I→R输入：用户画像（U）：评分、偏好、人口统计学资料、上下文等项目画像（I）：项目描述（属性）、内容等计算：兴趣度或相关度（R），用于排序输出：针对每个用户，给出项目排序列表用户画像，即对用户的特点和兴趣进行建模从用户相关的各种数据中挖掘或抽取出用户在不同属性上的标签例如：年龄、性别、职业、婚姻状态、兴趣、未来可能行为等项目画像，即对项目的特点进行建模从项目相关的各种数据中挖掘和抽取出项目在不同属性上的标签实现对项目（例如商品、服务等）的精准的定位 4.部分案例分 4.1 搜狐视频个性化推荐架构 4.2 今日头条推荐系统架构 4.3 Netflix推荐系统架构图 5. 视频推荐系统流程设计构建用户画像：输入数据：用户注册数据、行为日志、系统展示日志 等事实标签：性别、年龄、地域、人群（学生、上班族等）等模型标签：主题偏好、兴趣标签（明星、导演、风格等）等预测标签：用户活跃度、用户价值 等视频（项目）画像：输入数据：视频描述、视频内容、相关用户信息 等事实标签：主演、导演、出品人、主题 等模型与预测标签：评分、热度、关键词 、适合人群等最后目标： 总结本文主要对推荐系统做了基本介绍，从推荐系统为什么会出现，然后讲解了推荐系统基本概念，以及推荐和搜索的区别；梳理了推荐系统的发展历史，推荐系统主要架构，部分推荐系统案例；最后自己设计了一个视频推荐系统整体流程。","categories":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://sherwinzhang.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"},{"name":"RS","slug":"RS","permalink":"http://sherwinzhang.com/tags/RS/"}]},{"title":"《人世间》小说已阅","slug":"程序人生/《人世间》小说已阅","date":"2022-10-09T11:41:15.000Z","updated":"2022-10-17T15:02:19.231Z","comments":true,"path":"程序人生/程序人生/《人世间》小说已阅/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E3%80%8A%E4%BA%BA%E4%B8%96%E9%97%B4%E3%80%8B%E5%B0%8F%E8%AF%B4%E5%B7%B2%E9%98%85/","excerpt":"","text":"国庆期间，因为疫情，同时北京马上二十大召开，出京不便，于是在放假前几天买了这部小说。不辱使命，几天时间把上、中、下三部都翻完。我发现，对于这种描写近现代几代人生活的小说，我有一种莫名的的痴迷，比如之前看过的《平凡的世界》、《白鹿原》、《大江东去》。看完之后，不得不感叹作者描写人物的功力，那么多人物，每个都刻画的惟妙惟肖，从周志刚、周秉义、周蓉、周秉昆等等，都会在读者的脑中建立起很好的模型。周家三代人，以及围绕三代人展开的亲戚朋友，一个很大的网，网上每个节点的人物都会在合理的位置出现，塑造着自己的形象。整本书从文化大革命写起，首先给人描写了文化大革命下，远离北京的东北地区是一种什么样的状态，受到文化大革命影响的人们，又是如何应对；然后是文化大革命结束，恢复高考，周家三兄妹，分成两条支线，分别发展；等到他们毕业分成三条支线，再谱写着自己的故事：仕途线、高校线、底层线。梁老在写做中，一定更多的偏向了底层线，围绕周秉昆又做了展开，写了在酱油厂的朋友、写了光字片的邻居。除了这种主要一代人物随着时间的划分，我理解应该还有一种对比，周家三代人的对比，第一代：周志刚；第二代：周秉昆（他是全书的主角）；第三代：周聪。这三代人，应该走出了一个抛物线，从第一代建筑工人到第二代多个领域开花，再到第三代的发展。在此处一定要说为什么第二代可以走向最辉煌的局面，虽然周志刚夫妇都是农民出身，但是他们的身上有凛然正气，在教育子女中没有只要面子，不要“里子”；同时他们的教育没有人云亦云，书中描写印象比较深刻的地方，文化大革命的时候，她妈给他们放风，他们在里屋一起讨论当时不允许看的国外名著，借此，造就了周秉义和周蓉更完美的人格，也是这个环境，最后他的妻子、她的丈夫就是一起讨论的伙伴。周秉昆没有发展的非常好，与在家中是最小的孩子，没有去思考更多、更深层次的内容有一定关系吧。不过他的一大特点是爱结交更多朋友，比如在酱油厂的时候，结交的那些朋友，在书中也是一条线，通过每年正月他们是否聚在一起，来说明当时所处的背景，或者彼此之间的友谊。总体而言，第二代人发展的相当棒，至于周家第三代，我理解，如果没有这么突出的第二代，他们断然不会发展的这么好，这应该与他们没有更加强调子女教育有关系吧 ，或者说，在德方面的强调更少一些。书中的一些内容，可以看到对当今社会的批判，对一些不公现象的揭露，不过大方向依旧是：好人好报。当然这是所有社会的价值观。但是当你看到书中描写人物去世的时候，还是会感到唏嘘，比如周志刚的因病去世、比如周楠的见义勇为去世，等等都还是让人唏嘘，最初看到周楠去世时，觉得作者的写法有些奇怪，不像之前看的小说，比如：《平凡的世界》田晓霞去世，写完后，立即写带给少平的打击，然后把一点描述的特别深入。但是《人世间》不是，他说完这个结果后，立即就转到另一个事清的描述，后来才会写郑娟是多么多么悲伤、周秉昆是多么多么悲伤，两种不同的写法吧。今天感想先到此处，其他留作以后再去闲谈。已阅，是我喜欢的菜。于 2022-10-09 19:40:25","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]},{"title":"DeBERTa系列模型介绍","slug":"NLP/DeBERTa模型介绍","date":"2022-08-20T03:20:21.000Z","updated":"2023-04-18T23:12:04.106Z","comments":true,"path":"自然语言处理/NLP/DeBERTa模型介绍/","link":"","permalink":"http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/DeBERTa%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"DeBERTa系列模型介绍本文是BERT相关系列模型第四篇内容，前三篇见该分类下内容。 1.基本介绍DeBERTa（Decoding-enhanced BERT with disentangled attention）模型是微软在2021年提出的，到现在其实已经迭代了三个版本，第一版发布的时候在SuperGLUE[1]排行榜上就已经获得了超越人类的水平。目前，一些比较有挑战的NLP任务，甚至是NLG任务都会用DeBERTa模型当成预训练模型，进一步微调。图1：DeBERTA V1 模型结构来源：https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_caDeBERTa 模型使用了两种新技术改进了 BERT 和 RoBERTa 模型，同时还引入了一种新的微调方法以提高模型的泛化能力。两种新技术的改进：注意力解耦机制：图1右侧黄色部分**增强的掩码解码器 ：**图1左侧Enhanced Mask Decoder部分（V3版本中，用其他方法替换）新的微调方法：虚拟对抗训练方法。结果表明，这些技术显著提高了模型预训练的效率以及自然语言理解（NLU）和自然语言生成（NLG）下游任务的性能。与 Large-RoBERTa 相比，基于一半训练数据训练的 DeBERTa 模型在很多 NLP 任务中始终表现得更好，MNLI 提高了+0.9%（90.2%–&gt;91.1%），SQuAD v2.0提高了+2.3%（88.4%–&gt;90.7%），RACE提高了+3.6%（83.2%–&gt;86.8%）。同时，通过训练由48个Transformer层和15亿参数组成的Large-DeBERTa模型。其性能得到显著提升，单个 DeBERTa 模型在平均得分方面首次超过了 SuperGLUE 基准测试上的表现，同时集成的 DeBERTa 模型目前位居榜首。 截至 2021 年 1 月 6 日，SuperGLUE 排行榜已经超过了人类基线（90.3 VS 89.8）。 2.DeBERTa架构 2.1 DeBERTa V1 2.1.1 结构图图1：DeBERTa V1模型结构 2.1.1 优化点解释 a) 注意力解耦（Disentangled attention）在BERT 中，输入层的每个单词都使用一个向量来表示，该向量是其单词（内容）嵌入和位置嵌入的总和。图2：BERT输入表示. 输入嵌入是token embeddings, segmentation embeddings 和position embeddings 之和。而 DeBERTa 中的每个单词使用两个对其内容和位置分别进行编码的向量来表示，并且注意力单词之间的权重分别使用基于它们的内容和相对位置的解码矩阵来计算。这么做的原因是，经观察发现，单词对的注意力权重不仅取决于它们的内容，还取决于它们的相对位置。 例如，“deep”和“learning”这两个词相邻出现时，它们之间的依赖性比它们出现在不同句子中时要强得多。对于序列中位置 i 处的 token，微软使用了两个向量， {Hi} 和{ Pi|j }表示它，它们分别表示其内容和与位置 j 处的token的相对位置。 token i 和 j 之间的交叉注意力得分的计算可以分解为四个部分:也就是说，一个单词对的注意力权重可以使用其内容和位置的解耦矩阵计算为四个注意力（内容到内容，内容到位置，位置到内容和位置到位置）的得分总和。现有的相对位置编码方法在计算注意力权重时使用单独的嵌入矩阵来计算相对位置偏差。 这等效于仅使用上等式中的“内容到内容”和“内容到位置”来计算注意力权重。微软认为位置到内容也很重要，因为单词对的注意力权重不仅取决于它们的内容，还会和相对位置有关。根据它们的相对位置，只能使用内容到位置和位置到内容进行完全建模。 由于微软使用相对位置嵌入，因此位置到位置项不会提供太多附加信息，因此在实现中将其从上等式中删除。 b) 增强的掩码解码器DeBERTa和BERT模型一样，也是使用MLM进行预训练的，在该模型中，模型被训练为使用 mask token 周围的单词来预测mask词应该是什么。 DeBERTa将上下文的内容和位置信息用于MLM。 解耦注意力机制已经考虑了上下文词的内容和相对位置，但没有考虑这些词的绝对位置，这在很多情况下对于预测至关重要。如：给定一个句子“a new store opened beside the new mall”，并用“store”和“mall”两个词 mask 以进行预测。 仅使用局部上下文（即相对位置和周围的单词）不足以使模型在此句子中区分store和mall，因为两者都以相同的相对位置在new单词之后。 为了解决这个限制，模型需要考虑绝对位置，作为相对位置的补充信息。 例如，句子的主题是“store”而不是“mall”。 这些语法上的细微差别在很大程度上取决于单词在句子中的绝对位置。有两种合并绝对位置的方法。 BERT模型在输入层中合并了绝对位置。 在DeBERTa中，微软在所有Transformer层之后将它们合并，然后在softmax层之前进行 mask token 预测，如图3 所示。这样，DeBERTa捕获了所有Transformer层中的相对位置，同时解码被mask的单词时将绝对位置用作补充信息 。 此即为 DeBERTa 增强型掩码解码器(EMD)。图3： 解码器比较来源：https://arxiv.org/pdf/2006.03654.pdfEMD的结构如图2b 所示。 EMD有两个输入(即 I,HI,HI,H)。 HHH 表示来自先前的transformer层的隐藏状态，并且 III 可以是用于解码的任何必要信息，例如，绝对位置嵌入或从先前的EMD层输出。 nnn 表示 nnn 个EMD堆叠层，其中每个EMD层的输出将是下一个EMD层的输入III，最后一个EMD层的输出将直接输出到语言模型头。 nnn 层可以共享相同的权重。 在实验中，微软设定n=2n=2n=2 ，即2层共享相同的权重，以减少参数的数量，并使用绝对位置嵌入作为第一个EMD层的III。 当 I=HI=HI=H 和 n=1n=1n=1 时，EMD与BERT解码器层相同。 不过，EMD更通用、更灵活，因为它可以使用各种类型的输入信息进行解码。 c) 虚拟对抗训练方法对抗训练是NLPer经常使用的技术，在做比赛或者公司业务的时候一般都会使用对抗训练来提升模型的性能。DeBERTa预训练里面引入的对抗训练叫SiFT，它攻击的对象不是word embedding，而是embedding之后的layer norm。规模不变微调(Scale-invariant-Fine-Tuning SiFT)算法一种新的虚拟对抗训练算法， 主要用于模型的微调。虚拟对抗训练是一种改进模型泛化的正则化方法。 它通过对抗性样本提高模型的鲁棒性，对抗性样本是通过对输入进行细微扰动而创建的。 对模型进行正则化，以便在给出特定于任务的样本时，该模型产生的输出分布与该样本的对抗性扰动所产生的输出分布相同。对于之前的NLP任务，一般会把扰动应用于单词嵌入，而不是原始单词序列。 但是嵌入向量值的范围在不同的单词和模型之间有所不同。 对于具有数十亿个参数的较大模型，方差会变大，从而导致对抗训练有些不稳定。受层归一化的启发，微软提出了SiFT算法，该算法通过应用扰动的归一化词嵌入来提高训练稳定性。 即在实验中将DeBERTa微调到下游NLP任务时，SiFT首先将单词嵌入向量归一化为随机向量，然后将扰动应用于归一化的嵌入向量。 实验表明，归一化大大改善了微调模型的性能。 2.2 DeBERTa V22021年2月微软放出的 V2 版本在 V1 版本的基础上又做了一些改进：**1.词表：**在 v2 中，tokenizer扩的更大，从V1中的50K，变为 128K 的新词汇表。2.nGiE(nGram Induced Input Encoding) v2 模型在第一个转换器层之外使用了一个额外的卷积层，以更好地学习输入标记的依赖性。**3.共享位置和内容的变换矩阵：**通过实验表明，这种方法可以在不影响性能的情况下保存参数。**4.应用桶方法对相对位置进行编码：**v2 模型使用对数桶对相对位置进行编码。图3： DeBERTa V2优化结果对比来源：https://arxiv.org/pdf/2006.03654.pdf优化结果：2.0版几个变更对模型的影响，增大词典效果最显著。 2.3 DeBERTa V32021年11月微软又放出了 V3 版本。这次的版本在模型层面并没有修改，而是将预训练任务由掩码语言模型（MLM）换成了ELECTRA一样类似GAN的RTD (Replaced token detect) 任务。我们知道BERT模型只使用了编码器层和MLM进行训练。而ELECTRA 使用 GAN 的思想，利用生成对抗网络构造两个编码器层进行对抗训练。其中一个是基于MLM训练的生成模型，另一个是基于二分类训练的判别模型。生成模型用于生成不确定的结果同时替换输入序列中的掩码标记，然后将修改后的输入序列送到判别模型。判别模型需要判断对应的 token 是原始 token 还是被生成器替换的 token。不同的训练方法实验尝试：1、[ES]生成模型和判别模型共享embedding层训练；2、[NES]生成模型和判别模型不共享embedding层，也不共享参数层，交替训练；3、[GDES]生成模型和判别模型部分共享。通过实验表明：图3： DeBERTa V3优化结果对比来源：https://arxiv.org/pdf/2111.09543.pdfDeBERTa V3 在某些任务中相比之前模型有不小的涨幅，其中GDES模式优化效果最好。 3.相关codeDeBERTa官方文档，可以直接调库实现：DeBERTa code_1: https://deberta.readthedocs.io/en/latest/index.htmlhaggingface实现DeBERTa，可以直接调库实现：DeBERTa code_2: https://huggingface.co/docs/transformers/model_doc/deberta官方源码：DeBERTa code_3: https://github.com/microsoft/DeBERTa 4.总结1.DeBERTa V1 相比 BERT 和 RoBERTa 模型的改进：两种技术改进：注意力解耦机制增强的掩码解码器新的微调方法：虚拟对抗训练方法(SiFT)。2.DeBERTa V2 改进tokenizer扩的更大（优化效果最明显）transformer外使用了额外的卷积层共享位置和内容的变换矩阵应用桶方法对相对位置进行编码3.DeBERTa V3 改进在模型层面并没有修改，将预训练任务MLM换成了ELECTRA一样类似GAN的RTD任务 5.参考文献[1] SuperGLUE: https://super.gluebenchmark.com/leaderboard[2] DeBERTa: https://arxiv.org/pdf/2006.03654.pdf[3] DeBERTa 2.0: https://huggingface.co/docs/transformers/model_doc/deberta-v2[4] DeBERTa 3.0: https://arxiv.org/pdf/2111.09543.pdf[5] 微软 deDERTa 官网: https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_ca","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"}]},{"title":"RoBERTa：一种稳健优化BERT的预训练方法","slug":"NLP/RoBERTa：一种稳健优化BERT的预训练方法","date":"2022-08-15T03:20:21.000Z","updated":"2022-08-28T14:21:41.212Z","comments":true,"path":"自然语言处理/NLP/RoBERTa：一种稳健优化BERT的预训练方法/","link":"","permalink":"http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/RoBERTa%EF%BC%9A%E4%B8%80%E7%A7%8D%E7%A8%B3%E5%81%A5%E4%BC%98%E5%8C%96BERT%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95/","excerpt":"","text":"摘要语言模型的预训练带来了显著的性能提高，但比较不同的方法具有一定的挑战性。因为其训练的计算成本很高，同时不同的模型通常又是在不同规模的私有数据集上进行的，而且超参数选择也会对最终结果有重大影响。我们提出了BERT预训练的研究(Devlin等人，2019年)，测量了许多关键超参数和训练数据大小的影响。在实验中，发现了BERT模型的一些问题，同时提出一种新的模型，这种模型可以匹配或超过BERT后发布的每一个模型的性能。我们最好的模型在GLUE, RACE和SQuAD上取得了最先进的结果。这些结果突出了以前忽略的设计选择的重要性。基于此，我们发布了其模型和代码。 1 文章介绍自训练方法，如 ELMo (Peters et al., 2018)、GPT (Radford et al., 2018)、BERT (Devlin et al., 2019)、XLM (Lample and Conneau, 2019) 和 XLNet (Yang et al., 2019) al., 2019) 带来了显着的性能提升，但要确定这些方法的哪些方面贡献最大可能具有挑战性。 因为训练的计算成本很高，限制了我们对模型的微调，还有不同模型通常使用未公开的私人训练数据完成，进一步限制了我们衡量模型的效果。我们提出了 BERT 预训练的进一步研究（Devlin 等人，2019 年），其中包括评估超参数调整和训练集大小的影响。 我们发现 了BERT 训练中的不足，并提出了一种改进的 BERT 模型训练方法，我们称之为 RoBERTa，它可以匹配或超过所有后 BERT 方法的性能。 我们的修改很简单，它们包括：（1）训练模型的时间更长，批量更大，数据更多； (2) 去除下一句预测目标； (3) 较长序列的训练； (4) 动态改变应用于训练数据的遮掩模式。 我们还收集了一个与其他私人使用的数据集相当大小的大型新数据集（CC-NEWS），以更好地控制训练集大小的影响。在控制训练数据时，我们改进的训练方法突破了 GLUE 和 SQuAD 上已发布的 BERT 结果。 当对额外数据进行更长时间的训练时，我们的模型在公共 GLUE数据集排行榜上的得分为 88.5，与 Yang 等人报告的 88.4 相匹配（2019）。 我们的模型在 4/9 的 GLUE 任务：MNLI、QNLI、RTE 和 STS-B 上的评分达到了新高度。 我们在 SQuAD 和 RACE 上也获得了最高的得分。 总体而言，我们重新确定 BERT 的掩码语言模型与其他最近提出的训练方法（例如扰动自回归语言建模（Yang 等人，2019））具有相同的竞争力（Yang 等人，2019）。综上所述，本文的贡献在于（1）我们提出了一组重要的 BERT 设计选择和训练策略，并引入了能够提高下游任务性能的替代方案； (2) 我们使用新的数据集 CC-NEWS，并确认使用更多数据进行预训练可以进一步提高下游任务的性能； (3) 我们的训练改进表明，在正确的设计选择下，掩码类语言模型预训练与所有其他最近发布的方法相比依旧具有很强竞争力。 我们利用PyTorch 实现了该模型、以及预训练和微调代码。 2 背景在本节中，我们简要概述了 BERT (Devlin et al., 2019) 预训练方法以及我们将在下一节中通过实验检查的一些调优方法。 2.1 初始设置BERT 将两部分x1,...,xNx_1,... ,x_Nx1​,...,xN​ 和 y1,...,yMy_1,..., y_My1​,...,yM​ 的串联作为输入。 这两个片段作为单个输入序列呈现给 BERT，并用特殊标记分隔它们：[CLS],x1,...,xN,[SEP],y1,...,yM,[EOS][CLS],x_1,...,x_N,[SEP],y_1,...,y_M,[EOS][CLS],x1​,...,xN​,[SEP],y1​,...,yM​,[EOS]。 对M 和 N 约束是 M + N &lt; T ，其中 T 是控制训练期间最大序列长度的参数。该模型首先在大型未标记文本语料库上进行预训练，然后使用最终任务标记数据进行微调。 2.2 结构说明BERT 使用了 Transformer 架构（Vaswani 等人，2017 年），我们不会对其进行详细回顾。 我们使用具有 L 层的Transformer结构。 每个块使用的自注意头为A个，隐藏维度为 H。 2.3 训练目标BERT的预训练模型会考虑两个任务：带掩码的语言模型和下一句预测任务。带掩码的语言模型 (Masked Language Model, MLM) 在输入序列中随机样本替换为特殊标记 [MASK]。 MLM是预测掩码标记的交叉熵损失。 BERT 首先统一选择 15% 的输入标记。 在选定的标记中，80% 被 [MASK] 替换，10% 保持不变，10% 被随机选择的词汇标记替换。在最初的实现中，随机掩码和替换在开始时执行一次，然后在训练期间保存。同时在整个训练过程中，虽然数据是一样的，但是每个训练句子的掩码并不会保持一致（参见第 4.1 节）。下一句预测（Next Sentence Prediction NSP）是一种二元分类损失，用于预测原始文本中两段是否相连。通过从文本语料库中提取连续句子来创建正样本。负样本是通过将不同文档中的片段配对来创建的。以相等的概率采样正、负样本。NSP目标旨在提高下游任务的性能，如自然语言推理（Bowman等人，2015），该任务需要对句子之间的关系进行推理。 2.4 优化方法通过Adam（Kingma和Ba，2015）使用以下参数对BERT进行优化：β1β_1β1​=0.9，β2β_2β2​=0.999，ϵ=1e−6\\epsilon=1e-6ϵ=1e−6，L2的衰减权重为0.01。在前10000个步骤中，学习速率被设定到1e-4的峰值，然后线性衰减。BERT训练时，所有层和注意力权重上的 dropout 均为0.1，并增加了GELU激活函数（Hendrycks和Gimpel，2016）。模型预训练更新参数：S=1000000，小批量设定参数：B=256，最大长度token：T=512。 2.5 数据介绍BERT使用了BOOKCORPUS（Zhu等人，2015）和英语维基百科的组合共16GB的未压缩文本进行训练。 3 实验设置在本节中，我们描述了BERT复现研究的实验设置。 3.1 初始启动我们在FAIRSEQ（Ott等人，2019年）中重新实现了BERT。我们主要使用了第2节中给出的原始BERT优化超参数，但峰值学习率和步骤数除外，这两个参数会针对每个设置进行调整。此外，我们还发现训练对Adam非常敏感，在某些情况下，我们在微调后获得了更好的性能。同样，我们发现设置β2=0.98β_2=0.98β2​=0.98可以提高大批量训练时的稳定性。我们使用token最大值为T=512进行预训练。与Devlin等人（2019年）不同，我们不会随机增加短序列，也不会在前90%的更新中使用缩短的序列长度进行训练，而是使用全长序列进行训练。我们在DGX-1机器上使用混合精度浮点算法进行训练，每台机器配备为：8×32GB Nvidia V100 GPU。 3.2 数据介绍bert类模型的预训练非常依赖于大量的文本。Baevski等人(2019)指出，增加数据量可以改善最终任务的性能。之前一些研究已经在比原始BERT更大、更多样化的数据集上进行了训练(Radford等人，2019年;Yang等人，2019;Zellers等人，2019年)。不幸的是，并不是所有额外的数据集都被公开。对于我们的研究，我们专注于收集尽可能多的数据进行实验，使我们能够匹配数据的总体质量和数量，进而适应每次的结果比较。我们考虑了五个不同领域的英语语料库，总共超过160GB的未压缩文本。主要使用了以下文本语料库:BOOKCORPUS (Zhu et al.， 2015)加上英文维基百科。这是用来训练BERT的原始数据。(16GB)。CC-NEWS，从CommonCrawl新闻数据集的英语部分收集(Nagel, 2016)到的数据。该数据包含2016年9月至2019年2月期间的6300万篇英语新闻文章。(过滤后包含76GB)OPENWEBTEXT (Gokaslan和Cohen, 2019)， Radford等人(2019)描述的WebText语料库的开源数据。这部分数据是从Reddit上分享的url中提取的web内容。(38GB)STORIES，一个在Trinh和Le(2018)中引入的数据集，包含CommonCrawl数据的子集，用来匹配Winograd模式的故事风格。(31 GB)。 3.3 评估方法在之前的研究中，我们使用以下三个基准来评估我们在下游任务中预训练的模型。通用语言理解评估(The General Language Understanding Evaluation GLUE)方法(Wang et al.， 2019b)由9个数据集组成，用于评估自然语言理解系统。任务分为单句分类任务和句子对分类任务。GLUE官方会提供分割好的数据，以及一个提交服务器和允许参与者根据私人测试数据评估和比较他们系统的排行榜。在第4节的研究中，我们介绍了在相应的单任务训练数据上微调预训练模型后的结果。我们的微调过程和最初的BERT论文保持一致(Devlin et al.， 2019)。在第5节中，我们另外介绍了从公共排行榜中获得的测试集结果。这些结果依赖于一些特定任务的修改。SQuAD斯坦福问答数据集(The Stanford Question Answering Dataset SQuAD)提供了一段上下文和一个问题。其任务是从上下文中提取相关答案来回答这个问题。我们评估了《SQuAD》的两个版本:V1.1和V2.0 (Rajpurkar et al.， 2016年和2018年)。在V1.1中，上下文中总是包含一个答案，而在V2.0中，有些问题在提供的上下文中没有得到回答，这使得任务更具挑战性。对于SQuAD V1.1，我们采用与BERT (Devlin et al.， 2019)相同的预测方法。对于SQuAD V2.0，我们添加了一个额外的二元分类器来预测问题是否可回答，我们通过对分类项和损失项求和来联合训练该模型。在评估过程中，我们只预测分类为可回答的范围。RACE （The ReAding Comprehension from Examinations ）(Lai et al.， 2017)任务是一个大型阅读理解数据集，包含超过2.8万篇文章和近10万个问题。该数据集来自中国的英语考试，专为初高中学生设计。在RACE中，每一篇文章都有多个问题。每个问题的任务是从四个选项中选择一个正确答案。RACE比其他流行的阅读理解数据集有更长的上下文，并且需要推理的问题的比例更大。 4 训练过程分析本节将探索哪些选择对成功预训练BERT模型比较重要。我们保持模型体系结构不变。具体来说，我们在训练BERT模型时使用与BERTBASEBERT_{BASE}BERTBASE​相同的配置(L = 12, H = 768, A = 12, 110M参数)。 4.1 动、静态掩码对比正如第2节所讨论的，BERT依赖于随机掩码和预测token。原始的BERT实现在数据预处理过程中只进行一次掩码，产生单一的静态掩码。为了避免在每个epoch中对训练实例使用相同的掩码，训练数据被复制10份，以便每个序列在40个训练epoch中以10种不同的方式进行掩码。所以，每个训练序列在训练过程中使用相同的掩码训练4次。我们将此策略与动态掩码进行比较。**在动态掩码中，每次向模型提供序列时都会生成掩码。**当需要更多的步骤或更大的数据集进行预训练时，这一点的影响将非常大。表1: BERTBASEBERT_{BASE}BERTBASE​静态屏蔽和动态屏蔽的比较。我们统计了SQuAD的F1值，以及MNLI-m和SST-2的准确性。结果是5个随机初始化(种子)的中值。参考结果来自Yang等人(2019)。表1比较了Devlin等人(2019)发布的BERTBASEBERT_{BASE}BERTBASE​结果与我们重新实现静态或动态掩码的结果。我们发现，采用静态掩码的重新实现与原始BERT模型的性能相似，而动态掩码略好于静态掩码。考虑到这些结果和动态掩码的额外效益，我们在接下来的实验中使用动态掩码。 4.2 模型输入规范化和下一句预测在原始BERT预训练过程中，模型会观察两个连接的句子内容，它们要么从同一文档(p = 0.5)连续采样，要么从不同的文档采样。除了构建带掩码的语言模型(MLM)外，该模型还通过衡量下一句预测(NSP)损失来预测观察到的文档片段是否来自相同的文档。假设下一句预测的损失是训练原始BERT模型的一个重要因素。Devlin等人(2019)观察到，去除NSP会损害性能，将会使QNLI、MNLI和SQuAD 1.1的性能显著下降。然而，最近的一些研究对NSP损失的必要性提出了质疑(Lample和Conneau, 2019;Yang等人，2019;Joshi等人，2019年)。为了更好地理解这种差异，我们比较了几种不同的训练形式:SEGMENT-PAIR+NSP: 这遵循BERT (Devlin等人，2019)中使用的原始输入格式，带有NSP损失。每个输入都有一对片段，每个片段可以包含多个自然句子，但组合的总长度必须小于512个token。此处我们保留了NSP的损失。SENTENCE-PAIR+NSP: 每个输入包含一对自然句子，或者从一个文档的连续部分采样，或者从单独的文档采样。由于这些输入明显少于512个token，因此我们增加了批处理大小，使token的总数保持类似于SEGMENT-PAIR+NSP。此处我们保留了NSP的损失。FULL-SENTENCES: 每个输入都包含从一个或多个文档连续采样的完整句子，因此总长度最多为512个token。输入可以跨越文档边界。当到达一个文档末尾时，我们开始对下一个文档中的句子进行采样，并在文档之间添加额外的分隔符。此处我们移除了NSP损失评估。DOC-SENTENCES: 输入的构造类似于完整句子，只是它们不能跨越文档边界。在文档末尾采样的输入可能小于512个token，因此我们在这些情况下动态地增加批处理大小，以达到与FULL- sentence相似的标记总数。我们移除了NSP损失评估。表2: 在BOOKCORPUS和WIKIPEDIA上预训练的基本模型结果。所有模型训练1M 步，批大小256。我们计算了SQuAD的F1值，MNLI-m、SST-2和RACE的准确性。结果是五个随机初始化(种子)的中值。BERTBASEBERT_{BASE}BERTBASE​和XLNetBASEXLNet_{BASE}XLNetBASE​的结果来自Yang等人(2019)。表2显示了四种不同设置的实验结果。我们首先比较了Devlin等人(2019)的原始SEGMENT-PAIR输入格式和SENTENCE-PAIR格式；两种格式都保留了NSP损失，但后者使用单个句子。我们发现，使用单独的句子会损害后续任务的表现，我们认为这是因为模型无法学习长期依赖。接下来，我们比较没有NSP损失的训练和使用单个文档中的文本块(DOC-SENTENCES)的训练。我们发现，与Devlin等人(2019)相比，该设置优于最初公布的BERTBASEBERT_{BASE}BERTBASE​结果，删除NSP损失略微改善了下游任务的性能。原始的BERT实现可能只删除了损失项，而仍然保留SEGMENT-PAIR输入格式。最后，我们发现限制来自单个文档的序列(DOC-SENTENCES)比来自多个文档的序列(FULL-SENTENCES)性能略好。然而，由于DOC-SENTENCES格式导致的批处理大小不同，我们在其余实验中仍然使用FULL-SENTENCES，以便与相关工作进行更容易的比较。 4.3 大批量训练之前的神经机器翻译工作表明，当学习率适当提高时，使用非常大的小批量训练可以提高优化速度和结束任务的性能(Ott et al.， 2018)。最近的研究也表明了BERT同样适用于大规模批量训练(You等人，2019年)。Devlin等人(2019)最初对BERTBASEBERT_{BASE}BERTBASE​进行了1M 步、批大小为256的训练。通过梯度积累，这在计算成本上等价于批量大小为2K、125K步或批量大小为8K、31K步的训练。表3: 在不同批处理大小(bsz)的BOOKCORPUS和WIKIPEDIA上训练的基本模型上对训练数据(ppl)的困惑度和准确性。我们调整了每个设置的学习率(lr)；模型传递数据的次数(epoch)是相同的，同时计算资源也是相同的。在表3中，我们比较了随着批处理规模的增加，BERTBASEBERT_{BASE}BERTBASE​的复杂度和结束任务性能，并控制了通过训练数据的次数。我们观察到，大批量训练提高了掩码语言建模目标的困惑度和结束任务的准确性。通过分布式数据并行训练，大批量的数据也更容易并行化，在后面的实验中，我们使用批量大小值为8K进行训练。值得注意的是，You等人(2019)使用更大的批大小训练BERT模型，最多达到32K的批量。该部分工作我们在本文中不做讨论，留给后来的学者进一步研究。 4.4 文本编码字节对编码(Byte-Pair Encoding BPE) (Sennrich等人，2016)是字符级和单词级表示的混合，它可以处理自然语言语料库中常见的大型词汇。BPE依赖于子词单元，通过对训练语料库进行统计分析来提取子词单元，而不是全部词汇。BPE词汇量通常在10K-100K子词单位之间。然而，在建模大型和多样化的语料库(如本工作中考虑的语料库)时，unicode字符可以占这个词汇表的相当大的一部分。Radford等人(2019)引入了一种巧妙的BPE实现，它使用字节而不是 unicode 字符作为基本子字单元。使用字节可以学习中等大小(50K单位)的子单词词汇表，它仍然可以在不引入任何未知标记的情况下对任何输入文本进行编码。原始BERT实现(Devlin等人，2019年)使用了一个大小为30K的字符级BPE词汇表，该词汇表是在使用启发式标记规则下对输入进行预处理后学习的。在Radford等人(2019)之后，我们考虑使用包含50K子词单元的更大字节的BPE词汇表来训练BERT，而不需要对输入进行任何额外的预处理或标记。这分别为BERTBASEBERT_{BASE}BERTBASE​和BERTLARGEBERT_{LARGE}BERTLARGE​增加了大约15M和20M的额外参数。早期的实验显示，这些编码之间只有轻微的差异，Radford等人(2019)的BPE在某些结束任务表现略差。尽管如此，我们相信通用编码方案的优势大于性能上的轻微下降，并在我们的其余实验中使用这种编码。这些编码更详细的比较留给以后的工作。 5 RoBERTa在前一节中，我们讨论了对BERT预训练过程进行修改，以提高结束任务的性能。现在，我们将汇总这些改进并评估它们的综合影响。我们将这种配置称为RoBERTa，用于健壮优化BERT方法。具体来说，RoBERTa使用动态屏蔽(章节4.1)、没有NSP损失的完整句子(章节4.2)、大的小批量(章节4.3)和更大的字节级BPE(章节4.4)进行训练。此外，我们研究了其他两个在之前的工作中被忽视的重要因素:(1) 用于预训练的数据(2) 训练通过数据的次数例如，最近提出的XLNet架构(Yang等人，2019年)是使用比原始BERT多近10倍的数据进行预训练的(Devlin等人，2019年)。它的批大小是BERT的8倍，优化步骤是BERT的一半。为了帮助从其他建模选择中分离出这些因素的重要性(例如，训练前目标)，我们开始按照BERTLARGEBERT_{LARGE}BERTLARGE​架构(L = 24, H = 1024, A = 16,355M参数)训练RoBERTa。我们在Devlin等人(2019)使用的可比较的BOOK-CORPUS + WIKIPEDIA数据集上预训练100K步，使用1024 V100 GPU对模型进行了大约一天的预训练。表4:当我们预训练更多的数据(16GB→160GB的文本)和更长的预训练(100K→300K→500K步骤)时，RoBERTa的结果展示。RoBERTa与BERTLARGEBERT_{LARGE}BERTLARGE​的架构和训练目标相匹配。BERTLARGEBERT_{LARGE}BERTLARGE​和XLNetLARGEXLNet_{LARGE}XLNetLARGE​的结果分别来自Devlin等人(2019)和Yang等人(2019)。所有GLUE任务的完成结果可以在附录中找到。在表4中展示了训练的结果。当控制训练数据时，我们观察到RoBERTa比最初报告的BERTLARGEBERT_{LARGE}BERTLARGE​结果有很大的改进，这再一步证明了我们在第4节中探讨的设计选择的重要性。接下来，我们将这些数据与3.2节中描述的另外三个数据集结合起来。我们使用与之前相同数量的训练步骤(100K)在组合数据上训练RoBERTa。我们总共预训练了超过160GB的文本。我们观察到所有下游任务的性能进一步提高，验证了数据大小和预训练多样性的重要性。最后，我们对RoBERTa进行了更长的预训练，将预训练步数从100K增加到300K，再进一步增加到500K。我们再次观察到下游任务性能的显著提高，300K和500K步模式在大多数任务中优于XLNetLARGEXLNet_{LARGE}XLNetLARGE​。我们注意到，即使我们训练时间最长的模型也不会过度拟合我们的数据，会从额外的训练中受益。在本文的剩余部分，我们在三个不同的基准上评估我们最好的RoBERTa模型:GLUE, SQuaD和RACE。 5.1 在GLUE上的结果对于GLUE，我们考虑两种微调设置。在第一个设置（single-task, dev）中，我们对GLUE任务微调了RoBERTa，只使用对应任务的训练数据。对于每个任务，我们考虑一个有限的超参数集，其批大小∈{16,32}，学习率∈{1e−5,2e−5,3e−5}，在前6%的步骤中进行线性递增，然后线性衰减到0。我们调整了10个epoch，并基于开发集上的每个任务评估指标执行early stopping。其余超参数与训练前保持一致。在这个设置中，我们观察了在五个随机初始化中每个任务的中值结果，并没有模型集成。在第二个设置(ensembles, test)中，我们通过GLUE排行榜将RoBERTa与测试集上的其他方法进行比较。GLUE排行榜的许多提交都依赖于多任务微调，而我们的提交只依赖于单任务微调。对于RTE、STS和MRPC，我们发现从MNLI单任务模型开始进行微调是有帮助的，而不是从基线预训练RoBERTa开始。我们探索了一个稍微宽一些的超参数空间，具体内容见附录。GLUE的两个任务需要特定任务的微调方法，以获得具有竞争力的排行榜结果。QNLI: GLUE排行榜上最近提交的结果采用了QNLI任务的两两排序公式(Liu et al.， 2019b,a;Yang等人，2019)。这种公式大大简化了评估任务，但不能直接与BERT相比(Devlin et al.， 2019)。根据最近的工作，我们的测试提交采用了排序方法，但是为了与BERT直接比较，我们也产出了基于纯分类方法的结果。WNLI: 我们发现提供的NLI-format的WNLI数据处理起来很有挑战性。所以我们使用来自Super- GLUE (Wang et al.， 2019a)重新格式化的WNLI数据，它用于表示查询代词和指示物的跨度。我们使用Kocijan等人(2019)的利润率排名损失对RoBERTa进行微调。对于给定的输入句子，我们使用space (Honnibal and Montani, 2017)从句子中提取额外的候选名词短语，并对我们的模型进行微调。表5:GLUE的结果。所有结果都基于24层架构。BERTLARGEBERT_{LARGE}BERTLARGE​和XLNetLARGEXLNet_{LARGE}XLNetLARGE​的结果分别来自Devlin等人(2019)和Yang等人(2019)。开发集上的RoBERTa结果是五次运行的中位数。测试集上的RoBERTa结果是单任务模型的集合。对于RTE、STS和MRPC，我们从MNLI模型而不是基线预训练模型开始进行微调。平均数据来自GLUE排行榜。在表5中展示了我们的结果。在第一种设置(single-task, dev)中，RoBERTa在所有9个GLUE任务开发集上都取得了最先进的结果。同时，RoBERTa使用与BERTLARGEBERT_{LARGE}BERTLARGE​相同的掩码语言建模预训练目标和架构，但始终优于BERTLARGEBERT_{LARGE}BERTLARGE​和XLNetLARGEXLNet_{LARGE}XLNetLARGE​。在第二种设置中(ensembles, test)，我们将RoBERTa提交给GLUE排行榜，并在9个任务中的4个取得了最先进的结果，并获得了迄今为止最高的平均分。这是特别令人兴奋的，因为RoBERTa不依赖于多任务微调，不像大多数其他结果的提交。我们期望未来的工作可以通过加入更复杂的多任务微调程序来进一步改进这些结果。 5.2 在 SQuAD上的结果在SQuAD上，我们采用了比以往更简单的方法。尽管BERT (Devlin等人，2019年)和XLNet (Yang等人，2019年)都使用额外的QA数据集来扩充他们的训练数据，但我们只使用提供的SQuAD训练数据对RoBERTa进行微调。Yang等人(2019)也采用了自定义的分层学习率微调XLNet，而我们对所有层使用相同的学习率。对于SQuAD 1.1，我们遵循与Devlin等人(2019)相同的微调程序。对于SQuAD v2.0，我们对给定的问题是否可回答进行了分类;我们通过对分类项和跨度损失项求和的方法，将该分类器与跨度预测器联合训练。表6:SQuAD的结果。†表明结果依赖于额外的外部训练数据。RoBERTa在开发和测试设置中只使用提供的SQuAD数据。BERTLARGEBERT_{LARGE}BERTLARGE​和XLNetLARGEXLNet_{LARGE}XLNetLARGE​的结果分别来自Devlin等人(2019)和Yang等人(2019)。表6展示了我们的结果。在SQuAD v1.1中，RoBERTa与XLNet最先进的开发集相匹配。在SQuAD 2.0版本的开发套件中，RoBERTa设置了全新方法，比XLNet提高了0.4分(EM)和0.6分(F1)。我们还将RoBERTa提交到公开的SQuAD 2.0排行榜，并评估它与其他系统的性能。大多数顶级系统都建立在BERT (Devlin等人，2019年)或XLNet (Yang等人，2019年)的基础上，这些系统都依赖于额外的外部训练数据。相比之下，我们的提交没有使用任何额外的数据。除了一个模型，RoBERTa模型比其他提交的模型都要好。同时，RoBERTa是那些不依赖于数据增强的系统中得分最高的系统。 5.3 在RACE上的结果在RACE中，系统提供了一段文本、一个相关的问题和四个候选答案。系统需要对四个候选答案中哪个是正确的进行分类。为了完成这个任务，我们修改RoBERTa，将每个候选答案与相应的问题和文章连接起来。然后，我们对这四个序列进行编码，并通过一个全连接层传递得到的[CLS]表示，该层用于预测正确答案。我们截断长于128个token的问题-答案对，如果需要还会截断通道，这样总长度最多不超过512个token。表7:RACE测试集的结果。BERTLARGEBERT_{LARGE}BERTLARGE​和XLNetLARGEXLNet_{LARGE}XLNetLARGE​的结果来自Yang等人(2019)。RACE测试集的结果如表7所示。RoBERTa在中等和高级设置中都是最高得分。 6 相关工作预训练方法设计有不同的训练目标，包括语言建模（Dai 和 Le，2015；Peters 等，2018；Howard 和 Ruder，2018）、机器翻译（McCann 等，2017）和掩码语言建模（Devlin et al., 2019;Lample and Conneau, 2019）。最近的许多论文都对每个最终任务使用了微调模型的基本方法（Howard and Ruder, 2018;Radford et al., 2018），以及使用一些变体的预训练掩码语言模型。然而，较新的方法通过多任务微调（Dong 等人，2019 年）、结合实体嵌入（Sun 等人，2019 年）、跨度预测（Joshi 等人，2019 年）、和自回归预训练的多种变体（Song 等人，2019 年；Chan 等人，2019 年；Yang 等人，2019 年）。通过训练更大的模型和更多的数据，性能通常也会得到提高（Devlin 等人，2019 年；Baevski et al., 2019; Yang et al., 2019;Radford et al., 2019)。我们的目标是复制、简化和更好地调整BERT 的训练，作为更好地了解所有这些方法的相对性能的参考点。 7 结论在预训练BERT模型时，我们评估了许多设计决策。结果发现通过对模型进行更长的训练，在更多的数据上使用更大的批处理，可以大大提高性能，同时去掉了下一句预测目标，使用长序列训练，增加动态改变掩码模式。我们改进的预训练模型，称之为RoBERTa，在GLUE、RACE和SQuAD上实现了非常不错的结果，并且不需要对GLUE进行多任务优化，不需要对SQuAD进行额外的数据训练。这些结果说明了以前被忽视了的设计决策的重要性，并表明BERT的预训练目标与最近提出的替代方案相比仍然具有竞争力。此外，我们还使用了一个新的数据集CC-NEWS，并在下地址发布了用于预训练和微调的模型和代码：https://github.com/pytorch/fairseq.","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"}]},{"title":"BERT之后，NLP主要预训练模型演变梳理","slug":"NLP/BERT之后，NLP主要预训练模型演变梳理","date":"2022-06-20T13:20:21.000Z","updated":"2022-08-28T14:23:33.630Z","comments":true,"path":"自然语言处理/NLP/BERT之后，NLP主要预训练模型演变梳理/","link":"","permalink":"http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/","excerpt":"","text":"1.背景BERT [1]（Bidirectional Encoder Representation from Transformers）是由 Google AI 于 2018 年 10 月提出的一种基于深度学习的语言表示模型。BERT 发布时，在 11 种不同的自然语言处理（NLP）测试任务中取得最佳效果。截至2022年6月，很多研究者又基于BERT提出了很多新的模型，本文旨在梳理基于BERT模型优化后部分预训练模型，以便读者能够更快掌握BERT相关内容，为后期工作中使用BERT相关模型提供便捷性。 2.BERT基本介绍BERT 主要的模型结构是 Transformer 的编码器部分。Transformer[2] 是由 Ashish 等于 2017年提出的，用于Google机器翻译，包含编码器（Encoder）和解码器（Decoder） 两部分。其中 BERT-base 与 BERT-large 模型分别采用了 12 层与 24 层的 Transformer 编码器作为模型网络层。相比于传统用于 NLP 任务的循环神经网络 （RNN）及长短时记忆网络（LSTM）等，Transformer 拥有更强大的文本编码能力，也能更高效地利用 GPU 等高性能设备完成大规模训练工作。基于 BERT 模型的自然语言处理任务通过两个过程来实现：在预训练的过程中，首先利用大规模没有标注过的文本语料，例如百科知识、网页新闻等，通过充分的自监督训练，有效学习文本的语言特征，得到深层次的文本向量表示，形成相应文本的预训练模型。在微调过程中，直接将预训练过程中完成收敛的网络参数（即嵌入层和网络层）作为起始模型，根据具体的下游任务（如分类、序列标注等），输入人工标注好的数据集，完成模型的进一步拟合与收敛。从而得到一个可用的深度学习模型来实现特定的自然语言处理任务，例如文本分类、序列标注等。自 BERT 发布以来，基于“预训练-微调”的两阶段方法逐渐成为自然语言处理研究的主流。图片来源：参考文献[1] 3.ERNIEERNIE[3]（Enhanced Representation through Knowledge Integration）是百度（清华几乎在同一时间[2019]也发布了ERNIE版本，不过现在社会上谈起ERNIE，大多指百度版ERNIE）在2019年4月基于BERT模型做的进一步优化，在中文的NLP任务上得到了state-of-the-art的结果。其主要是通过对知识进行整合，达到增强表达的目的。受BERT的掩码策略启发，ERNIE旨在学习由知识掩码策略增强的语言表征，其中包括实体级掩码和短语级掩码。实体级策略通常由多个单词组成的实体。短语级策略将由多个单词组成的整个短语作为一个概念单元进行屏蔽。ERNIE和BERT的区别：图片来源：参考文献[3]在使用先验知识来增强预训练语言模型时ERNIE并没有直接添加知识嵌入，而是使用了一种多阶段知识掩码策略，将短语和实体集成到语言表示中。句子中不同的掩码级别如下图：图片来源：参考文献[3]基本级别的掩码第一个学习阶段是使用基本的掩码，它将一个句子视为一系列基本的语言单元，对于英语，基本的语言单元是单词，对于汉语，基本的语言单元是汉字。在训练过程中，我们随机屏蔽15%的基本语言单元，并使用句子中的其他基本单元作为输入，并训练一个转换器来预测被屏蔽的单元。基于基本层掩码，我们可以得到基本的单词表示。因为它是在基本语义单元随机掩码的基础上训练的，所以高层语义的知识表示很难完全建模。短语级别的掩码第二阶段是使用短语级掩码。短语是作为概念单位的一小群单词或字符。对于英语，我们使用词汇分析和组块工具来获取句子中短语的边界，并使用一些依赖于语言的切分工具来获取其他语言（如汉语）中的单词/短语信息。在短语级掩码阶段，我们还使用基本语言单元作为训练输入，不像随机基本单元掩码那样，这次我们在句子中多选几个短语，掩码并预测同一短语中的所有基本单元。在这个阶段，短语信息被编码到单词嵌入中。实体级别的掩码第三阶段是实体级掩码。我们把一些专有名词，如地点、人名、组织、产品等抽象为实体进行屏蔽。实体通常包含句子中的重要信息。在短语屏蔽阶段，我们首先分析一个句子中的命名实体，然后屏蔽和预测实体中所有的空缺。经过这三个阶段的学习，我们可以获得语义信息丰富的表达。经过实验结果表明，在五个自然语言处理任务（包括自然语言推理，语义相似性，命名实体识别，情感分析和检索问答）上，ERNIE优于当时其他基准方法。此外ERNIE在完形填空测试中具有更强大的知识推理能力。 4.RoBERTaRoBERTa[4]是Facebook和华盛顿大学于2019年7月在论文《RoBERTa: A Robustly Optimized BERT Pretraining Approach》中提出的。文章在 BERT模型的基础上提出了 BERT 模型的改进版 RoBERTa，使其获得了更好的自然语言任务处理效果，并在 GLUE，SQuAD，RACE 三个榜上取得最好的SOTA。RoBERTa主要在三方面对之前提出的BERT做了改进：其一是模型的具体细节层面，改进了优化函数；其二是训练策略层面，改用了动态掩码的方式训练模型，证明了NSP（Next Sentence Prediction）训练策略的不足，采用了更大的batch size；其三是数据层面，一方面使用了更大的数据集，另一方面是使用字节级别的BPE（Bytes-level BEP ）来处理文本数据。 5.DeBERTaDeBERTa[5]（Decoding-enhanced BERT with Disentangled Attention）是微软发表于ICLR2021上的预训练语言模型。2021年1月DeBERTa在SuperGLUE这项自然语言理解基准任务上**「超越人类」**，以90.3分夺冠。DeBERTa从两方面改进了BERT预训练的方法：自注意力**「解耦」**机制用2个向量分别表示content 和 position，即word本身的文本内容和位置。word之间的注意力权重则使用word内容之间和位置之间的解耦矩阵。这是因为word之间的注意力不仅取决于其文本内容，还依赖于两者的相对位置。图片来源：参考文献[6]用Enhanced Mask Decoder(「EMD」)强化预训练输出层原始的BERT存在预训练和微调不一致问题。预训练阶段，隐层最终的输出输入到softmax预测被mask掉的token，而微调阶段则是将隐层最终输出输入到特定任务的decoder。这个decoder根据具体任务不同可能是一个或多个特定的decoder，如果输出是概率，那么还需要加上一个softmax层。为消除这种不一致性，DeBERTa将MLM与其他下游任务同等对待，并将原始BERT中输出层的softmax替换为**「增强后的mask decoder(EMD)」**，EMD包含一个或多个Transformer层和一个softmax输出层。至此，结合了BERT和EMD的DeBERTa成为了一个encoder-decoder模型。使用这两种技术，新的预训练语言模型DeBERTa在许多下游NLP任务上的表现都优于RoBERTa和BERT。DeBERTa这项工作展示了探索自注意的词表征解耦以及使用任务特定解码器改进预训练语言模型的潜力。 6.综述本文针对BERT系列部分典型模型进行梳理，希望为大家梳理出在BERT提出后，整体的优化脉络。同时基于BERT的优化方向可以总结为如下：首先，大量的研究者通过对 BERT 的两个预训练目标进行改进提升模型对文本特征的学习能力，如：ERNIE、RoBERTa、DeBERTa等。对于预训练目标的优化改进是最常见同时也是效果最好的改造方式，所以本文在前面介绍中，也主要梳理了该方向的主要模型。其次，针对特定领域的显性知识，研究者提出在预训练模型中融合外部知识的方法，进一步丰富了模型所学习的文本特征，如用于专利文本的 PatentBERT：。这两种路线提升了模型的特征学习能力，但是并没有对预训练模型内部结构进行实质性的改进。部分研究者从 Transformer 神经网络出发，对其内部结构进行了改进，从而扩展了模型的应用场景，如：BART。最后，针对 BERT 模型参数量过大导致普通的硬件设备无法有效训练和加载的问题，大量的研究者提出模型压缩的方法，进而提升了 BERT 模型的易用性，如：ALBERT。 7.参考资料bert [1] https://arxiv.org/pdf/1810.04805.pdftransformer[2] https://arxiv.org/pdf/1706.03762.pdfernie [3] https://arxiv.org/pdf/1904.09223.pdfroberta [4] https://arxiv.org/pdf/1907.11692.pdfdeberta[5] https://arxiv.org/pdf/2006.03654.pdfdeberta[6] https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_ca 附件BERT模型优化改进路线总结：","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"}]},{"title":"Flask中 jsonify有什么作用？如何使用？","slug":"python/Flask中 jsonify有什么作用？如何使用？","date":"2022-06-08T02:24:02.000Z","updated":"2023-04-18T23:09:42.940Z","comments":true,"path":"python/python/Flask中 jsonify有什么作用？如何使用？/","link":"","permalink":"http://sherwinzhang.com/python/python/Flask%E4%B8%AD%20jsonify%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%EF%BC%9F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%EF%BC%9F/","excerpt":"","text":"Flask中 jsonify有什么作用？如何使用？jsonify是Flask框架提供的一个函数，用于将Python对象转换为JSON格式的响应。它将Python对象转换为JSON格式的字符串，并将其作为HTTP响应的主体返回给客户端，同时设置正确的Content-Type响应头，表明响应主体是JSON格式的数据。jsonify函数的主要作用是简化Flask应用程序中返回JSON响应的过程。在Flask应用程序中，返回JSON响应通常需要将Python对象手动序列化为JSON格式的字符串，然后将其封装到Flask的Response对象中。而使用jsonify函数可以简化这个过程，使得代码更加简洁、易读。以下是一个示例代码：12345678910111213141516pythonCopy codefrom flask import Flask, jsonifyapp = Flask(__name__)@app.route('/users')def get_users(): users = [ &#123;\"name\": \"Alice\", \"age\": 30&#125;, &#123;\"name\": \"Bob\", \"age\": 25&#125;, &#123;\"name\": \"Charlie\", \"age\": 35&#125; ] return jsonify(users) # 将Python对象转换为JSON格式的响应if __name__ == '__main__': app.run()在上面的代码中，当客户端访问/users时，get_users函数将返回一个Python列表users，其中包含三个字典对象。使用jsonify函数可以将这个列表转换为JSON格式的响应，并将其作为HTTP响应返回给客户端。客户端收到响应后，可以使用JSON解析器将响应主体解析为JSON对象，从而获取每个用户的详细信息。需要注意的是，使用jsonify函数需要先安装Flask框架。另外，jsonify函数使用的是Python内置的json模块进行序列化操作，因此需要确保序列化的对象是支持JSON序列化的类型。","categories":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/tags/python/"}]},{"title":"AI模型部署-fastapi[3-3]","slug":"AI综合/AI模型部署-fastapi[3-3]","date":"2022-06-07T13:34:29.000Z","updated":"2022-06-07T13:38:49.530Z","comments":true,"path":"AI综合/AI综合/AI模型部署-fastapi[3-3]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-fastapi[3-3]/","excerpt":"","text":"一、使用方法1.fastapi服务代码123456789101112131415161718# A机器中某路径下，app.py# fastapi模块导入from fastapi import FastAPIfrom pydantic import BaseModel# 通过item类定义数据类型class Item(BaseModel): data: dictapp = FaseAPI()@app.post(\"/v1/\")async def creat_item(item: Item): return item.data[\"text\"]2.fastapi服务部署12345678# A机器中终端执行# 服务调试uvicorn app:app --reload --host 0.0.0.0 --port 8001# 服务部署nohup uvicorn app:app --host 0.0.0.0 --port 8001 &gt; ./ret.log 2&gt;&amp;1 &amp;# nohup command &amp; -- 在后台运行3.部署好的服务请求12345678910111213141516171819202122# B机器中某路径下，test.py# 也可在同一台机器中测试# coding=utf-8import requests import jsonimport timeurl = \"http://0.0.0.0:8001/v1/\"data = &#123; \"id\": 1234, \"data\": \"这是一条测试数据！\"&#125;start = time.time()res = requests.post(url, json=data, timeout=200)end = time.time()print(\"耗费时间：\", end - start)print(\"返回的结果：\", res.json()) 二、总结如果使用fastapi框架部署深度学习模型1.在A机器中书写fastapi服务代码2.在A机器终端部署写好的服务3.在B机器调用部署好的服务","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"AI模型部署-flask[3-2]","slug":"AI综合/AI模型部署-flask[3-2]","date":"2022-06-07T13:04:29.000Z","updated":"2022-06-07T13:38:42.937Z","comments":true,"path":"AI综合/AI综合/AI模型部署-flask[3-2]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask[3-2]/","excerpt":"","text":"一、背景深度学习在训练好模型中，我们需要进行部署，有一些框架中已经自带部署框架，如TensorFlow中的 TensorFlow serving，paddlepaddle中的 paddle serving等。但是有些框架自带部署能力并不友好，或者如pytorch类框架，直接官方就建议使用python的flask框架。如果你在训练好模型后，想自己基于flask框架部署模型，下文就可以解决该类需求。 二、使用方法1.flask服务代码1234567891011121314151617181920212223242526# A机器中某路径下，app.py# 导入了 Flask 类。该类的实例将会成为我们的 WSGI 应用。from flask import Flaskfrom flask import request# 创建一个flask类的实例。app = Flask(__name__)# 第一个参数是应用模块或者包的名称。 __name__ 是一个适用于大多数情况的快捷方式。# 有了这个参数， Flask才能知道在哪里可以找到模板和静态文件等东西。# 定义服务请求路径和方式, 使用POST请求# 使用 route() 装饰器来告诉 Flask 触发函数的URL 。@app.route(\"/v1/\", methods=[\"POST\"])# \"/v1/\" 表示请求服务路径# \"POST\" 不同的请求方法， 常见的有 get,postdef example(): \"\"\"flask模型预测\"\"\" text = request.get_json() # 测试请求数据内容 # print(text) \"\"\" 逻辑书写 \"\"\" return text2.flask服务部署1234567891011# A机器中终端执行# 服务调试gunicorn -w 1 -b 0.0.0.0:8001 app:app# -w 代表开启的进程数, 我们只开启一个进程# -b 服务的IP地址和端口# app:app 是指执行的主要对象位置, 在app.py中的app对象# 服务部署nohup gunicorn -w 1 -b 0.0.0.0:8001 app:app &gt; ./ret.log 2&gt;&amp;1 &amp;# nohup command &amp; -- 在后台运行3.部署好的服务请求1234567891011121314151617181920212223# B机器中某路径下，test.py# 也可在同一台机器中测试# coding=utf-8import requests import jsonimport timeurl = \"http://0.0.0.0:8001/v1/\"data = &#123; \"id\": 1234, \"data\": \"这是一条测试数据！\"&#125;start = time.time()res = requests.post(url, json=data, timeout=200)end = time.time()print(\"耗费时间：\", end - start)print(\"返回的结果：\", res.json()) 三、总结如果使用flask框架部署深度学习模型1.在A机器中书写flask服务代码2.在A机器终端部署写好的服务3.在B机器调用部署好的服务","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"AI模型部署-flask和fastapi对比[3-1]","slug":"AI综合/AI模型部署-flask 和fastapi 对比[3-1]","date":"2022-06-07T12:44:29.000Z","updated":"2022-06-07T13:38:37.020Z","comments":true,"path":"AI综合/AI综合/AI模型部署-flask 和fastapi 对比[3-1]/","link":"","permalink":"http://sherwinzhang.com/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask%20%E5%92%8Cfastapi%20%E5%AF%B9%E6%AF%94[3-1]/","excerpt":"","text":"1.基本介绍flask、fastapi都是python语言在Web领域的框架，使用它们可以帮助我们提升效率，节省时间，避免我们在组装”汽车“的时候，还要从”轮子“、”螺丝钉“做起。python语言中，web领域主流框架有：django, flask, tornado, fastapi等本文主要从AI中训练好的模型如何更便捷、高效的部署，所以着重介绍flask和fastapi. 1.1 flask是什么Flask 是一个用 Python 编写的轻量级 Web 应用框架。其被称为“微框架”，因为它使用简单的核心，通过扩展增加其他功能。如文件上传等。官方网站 1.2 fastapi是什么FastAPI是一个高性能Web框架，用于构建API。FastAPI主要特性：快速：非常高的性能，与NodeJS和Go相当快速编码：将功能开发速度提高约200％至300％更少的错误：减少约40％的人为错误直观：强大的编辑器支持，自动补全无处不在，调试时间更少简易：旨在易于使用和学习，减少阅读文档的时间。简短：减少代码重复。稳健：获取可用于生产环境的代码，具有自动交互式文档基于标准：基于并完全兼容 API 的开放标准OpenAPI和JSON SchemaFastAPI要求：Python 3.6+官方网站 2.两者对比 2.1 flask优缺点优点与其他 Web 应用程序框架不同，flask 让你可以完全控制 Web 开发，从而完全控制应用程序和 Web 开发。Flask 允许进行单元测试，并且由于其内置的开发服务器，集成的支持等，因此可以通过对一些扩展进行调整来过渡到 Web 框架。Flask 简单易用，非常适合初学者使用，为开发人员提供了更好地学习和理解它的空间。它还使开发人员可以毫不费力地快速创建应用程序。缺点Flask 的很多模块由第三方开发，容易引起安全漏洞。Flask 具有一个单一的来源，表示它将依次处理每个请求，因此，无论有多少个请求，它仍然会轮流处理它们，这会耗费更多时间。可以将 Flask 用于商业项目。它可以帮助你快速入门，**但是网站高负荷情况下效果不佳。**你可以快速实施 Flask 项目，例如：电子商务系统。Facebook / Twitter机器人。在线社交网络。静态网站。如果你要做一些小型个人项目，比如聊天机器人，或者想实现产品的快速原型，或者喜欢自由的编写代码控制程序的流程，那么可以选择 Flask。 2.2 FastApi优缺点优点**自动类型检查。**这意味着更少的 Bug，即使在深度嵌套的 JSON 请求中，Fast API 也会验证开发人员的数据类型。集众所长，站在巨人的肩膀上。FastAPI 建立在 JSON Schema（用于验证JSON数据结构的工具），OAuth 2.0（用于授权的行业标准协议）和**OpenAPI（这是可公开获得的应用程序编程接口）**之类的标准之上。现代化。FastAPI 使使用称为 graphene-python 的 Python 库轻松构建 GraphQL API 。**快速、高性能。**可以和 NodeJS 和 Go 相提并论。缺点由于 FastAPI 相对较新，因此与其他框架相比，社区较小，第三方的教程相对较少。FastAPI 适用于构建高性能的 API，本身支持异步，如果要构建异步 API，可以优先选择 FastAPI。作者写开发fastapi的缘由：FastApi诞生的缘由 2.3 比较Stack Overflow 2021 开发人员调查最受欢迎的 Web 框架：来源：https://insights.stackoverflow.com/survey/2021/#section-most-loved-dreaded-and-wanted-web-frameworks从社区，性能，灵活性，学习成本，稳定性进行比较。社区活跃程度。Flask框架因为搭建较早，且开发者使用较多，所以社区非常活跃；FastAPI 的社区目前还比较小，因为它相对较新。性能。在性能方面，FastAPI 是领跑者，因为它是面向速度的。灵活性。灵活性是开发人员非常重视的东西，FastAPI 在代码方面是非常灵活的，其不限制代码布局。学习成本。FastAPI &lt; Flask稳定性Flask &gt; api","categories":[{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"}]},{"title":"目前使用不错软件整理","slug":"apple生态/目前使用不错软件整理","date":"2022-06-03T08:44:34.000Z","updated":"2023-04-02T00:08:55.764Z","comments":true,"path":"apple生态/apple生态/目前使用不错软件整理/","link":"","permalink":"http://sherwinzhang.com/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/","excerpt":"","text":"办公滴答清单：计划管理screenbrush:屏幕画笔、标记pdf阅读器：pdf expert做笔记功能，生态能力（ipad做笔记，可以在其他端查看） 图文笔记sublime（替代mac文本文档）typora（markdown书写）搭配ipic使用（截图上传到云端）onenote(学习类笔记)有道云（写随笔、晨间日记的地方）xmind(脑图制作)agenda(带分类窗口和时间线的笔记记录平台) 图片截图软件：snipaste, 截图效果一般，但是贴图功能非常强大 影音视频录制：obs视频剪辑:screenflow 系统bartender:mac bar管理工具，让你的bar没有那么杂乱的软件magnet:屏幕窗口管理：尤其适合多屏用户（分享自己的快捷键）MonitorControl一键调节外置显示器亮度及音量的软件 网络浏览器2023年02月02日 之前Safari(为了苹果生态使用)、chrome(当Safari无法发挥作用时使用)2023年04月02日 之后因为Safari支持不够友好，转换回Chrome。 学习bob:非常好用的翻译软件，支持截图翻译、输入翻译、复制翻译强烈推荐通过 serect key 接入使用123苹果生态 iCloud云盘(存储必要文档) 阅读书籍（苹果生态阅读+笔记同步）","categories":[{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"}],"tags":[{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"}]},{"title":"流言猛如虎","slug":"程序人生/流言猛如虎","date":"2022-05-14T15:41:15.000Z","updated":"2022-06-04T03:48:10.861Z","comments":true,"path":"程序人生/程序人生/流言猛如虎/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E6%B5%81%E8%A8%80%E7%8C%9B%E5%A6%82%E8%99%8E/","excerpt":"","text":"最近发生的几件事情，作为一个旁观者，让我深刻体会到在信息爆炸的今天，流言的可怕性。暂且在此记录，同时告诫自己以后类似情况应对之法。1.本周四下午，突然传出北京市因为疫情原因，新闻发布会将会宣布北京进入静默管理3天。该消息一经流传，就在各大短视频平台迅速蔓延，然后大部分市民都加入了蔬菜等日常物资的抢购潮。但是没过多久，随着疫情新闻发布会的结束，并没有如大家所传需要进入静默期，而是严惩假消息传播的始作俑者。2.最近在刷微信小视频或者抖音的时候，总可以发现北大韦神的日常，有他授课的、日常生活被别人拍摄的。其中有一条是几个教授花数月解不开的难题，被韦神几分钟解决。这个视频经过几天发酵后，最后韦东奕亲自澄清，并没有如此事情，只是流言。我们每个人都处在信息爆炸的今天，说不定你某个无意的举动，经过蝴蝶效应的发酵，就会把你推上风口浪尖。甚至在你受到别人关注的时候，即使很多和你并不相关的事情，都会被杜撰出来。我们暂且不讨论那些杜撰此类消息的人们，为了博人眼球，无所不用其极。我们能做的是，在流言中如何让自己尽可能成为一名“智者”。现在的我们，每天都会接受来自外界的很多信息，其传播之广、传播之快，远超常人想象，在这些未经证实的“消息”被推送到我们面前，我们需要做的是：1.不做任何消息都照单全收的“接收者”，需要加上自己的判断。2.流言止于智者。于2022年05月14日23:29:33家中","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]},{"title":"ERNIE论文导读和翻译","slug":"NLP/ERNIE论文导读和翻译","date":"2022-02-15T03:20:21.000Z","updated":"2022-08-28T14:23:27.772Z","comments":true,"path":"自然语言处理/NLP/ERNIE论文导读和翻译/","link":"","permalink":"http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/ERNIE%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E5%92%8C%E7%BF%BB%E8%AF%91/","excerpt":"","text":"ERNIE: Enhanced Representation through Knowledge Integration文章脉络： 摘要我们提出了一种新的知识增强语言表征模型，称为ERNIE（Enhanced Representation through Knowledge Integration，主要是通过对知识进行整合，达到增强表达的目的）。受BERT（Devlin等人，2018）的掩码策略启发，ERNIE旨在学习由知识掩码策略增强的语言表征，其中包括实体级掩码和短语级掩码。实体级策略要求通常由多个单词组成的实体。短语级策略将由多个单词组成的整个短语作为一个概念单元进行屏蔽。最新实验结果表明，在五个自然语言处理任务（包括自然语言推理，语义相似性，命名实体识别，情感分析和检索问答）上，ERNIE优于其他基准方法。我们还证明了ERNIE在完形填空测试中具有更强大的知识推理能力。 1.简介事实证明，语言表征预训练（Mikolovet等人，2013年；Devlin等人，2018年）对于改善许多自然语言处理任务（如命名实体识别、情感分析和问答）是有效的。其为了获得可靠的单词表示，设计了神经语言模型，学习单词共现，然后通过无监督学习获得单词嵌入。Word2Vec（Mikolov等人，2013）和Glove（Penningtonet等人，2014）中的方法将单词表示为向量，其中相似的单词具有相似的单词表示。这些单词表示为其他深度学习模型中的单词向量提供了初始化。最近，Cove（Mc Cann et al.，2017）、Elmo（Peters et al.，2018）、GPT（Radford et al.，2018）和BERT（Devlin et al.，2018）等许多工作通过不同的策略改进了单词表示，这项研究已经被证明对下游自然语言处理任务更有效。这些研究中的绝大多数只是通过上下文预测缺失的单词，然后进行建模。这些作品没有考虑句子中的先验知识。例如，在”Harry Potter is a series of fantasy novels written by J. K. Rowling”这句话中，“Harry Potter”是一个名字和“J. K. Rowling”是作家。该模型不需要长上下文的帮助，而只要通过单词搭配就可以很容易地预测实体中的遗漏单词Harry Potter。但是，现存的模型无法根据Harry Potter与J. K. Rowling之间的关系预测Harry Potter。 很显然，如果模型学习了有关先验知识的更多信息，则该模型可以获得更可靠的语言表示。本文提出了一种基于知识掩码策略的ERNIE（enhanced representation by knowledge Integration）模型，除基本掩码策略外，还采用了两种知识策略：短语级策略和实体级策略。我们把一个短语或一个实体作为一个单位，通常由几个词组成。在单词表征训练中，同一单元中的所有单词都被屏蔽，而不是只屏蔽一个单词或字符。这样，在训练过程中，短语和实体的先验知识被隐式学习。ERNIE没有直接添加知识嵌入，而是有效地学习了有关知识和较长语义依赖的信息，如实体之间的关系、实体的属性和事件的类型，以指导单词学习。这可以使模型具有更好的泛化性和适应性。为了降低模型的训练成本，ERNIE对异构中文进行了预训练，然后将其应用于5个中文NLP任务。ERNIE对所有这些任务的最新结果进行了改进。在完形填空测试上实验表明，与其他强基线方法相比，ERNIE具有更好的知识边缘推理能力。ERNIE的主要的贡献如下：（1）我们引入了一种新的语言学习处理模型，该模型弱化了语言单位（如符号和实体），以便隐式地从这些单位学习句法和语义信息。（2）ERNIE在各种汉语自然语言处理任务上的表现明显优于先前的最新方法。（3）我们发布了ERNIE和预训练模型的代码，这些代码都可以在如下链接中找到：https://github.com/PaddlePaddle/ERNIE/tree/develop/ernie. 2.相关工作 2.1上下文无关表示词作为连续向量的表示有着悠久的历史。Bengio等人（2003年）提出了一种非常流行的用于估计神经网络语言模型（NNLM）的结构，其中主要包括：一个具有线性投影层和非线性隐层的前馈神经网络用于学习单词向量表示。通过使用大量未标记数据预训练语言模型，然后学习通用语言表示是有效的。传统方法侧重于上下文无关的词语嵌入。例如，Word2Vec（Mikolovet等人，2013年）和Glove（Pennington等人，2014年）等方法将大量文本作为输入，并生成几百维的词向量。它们为词汇表中的每个单词生成一个单词嵌入表示。 2.2上下文感知表示我们知道，一个词在上下文中可能有完全不同的含义。Skip-thought（Kiros et al，2015）提出了一种通用的分布式句子编码器的无监督学习方法。Cove（McCann et al.，2017）表明，和在各种常见NLP任务中仅使用无监督的单词和字符向量相比，添加这些上下文向量可以提高性能。ULMFit（Howard and Ruder，2018）提出了一种有效的迁移学习方法，可应用于NLP中的任何任务。ELMo（Peters et al.，2018）从不同的维度概括了传统的词语嵌入研究。他们建议从语言模型中提取上下文敏感特征。GPT（Radford et al.，2018）通过调整Transformer增强了上下文敏感嵌入。BERT（Devlin et al.，2018）使用两种不同的跟踪任务进行语言建模。Bert随机地在句子中隐藏一定比例的单词，并学习预测那些隐藏的单词。此外，BERT还学习预测两个句子是否相邻。这项任务试图模拟传统语言模型无法捕捉到两个句子之间的关系。因此，这种特殊的预训练方法有助于BERT在各种关键NLP数据集（如GLUE（Wang et al.，2018）和SQUAD（Rajpurkar et al.，2016）等）上远超过其他技术。其他一些研究人员试图根据这些模型添加更多信息。MT-DNN（Liu等人，2019年）将预训练学习和多任务学习相结合，以提高GLUE中多个不同任务的表现（Wang等人，2018年）。GPT-2（Radford et al.，2019）将任务信息添加到预训练过程中，并使其模型适应零样本任务。XLM（Lample and Conneau，2019）将语言嵌入到预训练过程中，在跨语言任务中取得了更好的效果。 2.3 异构数据在非监督的异构数据上预训练语义编码器可以提高迁移学习性能。我们（Ceret al.，2018）采用了来自维基百科、网络新闻、网络QA页面和论坛的异构数据对句子编码。句子编码器（Yanget al.，2018）基于从Reddit对话中提取的查询-响应对数据的响应预测。XLM（Lample和Conneau，2019）将平行语料库引入了BERT，该语料库与掩码语言模型任务联合训练。通过transformer对异构数据进行预训练，XLM在无监督机器翻译任务和分类任务中表现出了优异的性能。 3.方法本节介绍ERNIE及其详细实现。我们首先描述了模型的transformer编码器，然后在第3.2节介绍了知识集成方法。BERT和ERNIE之间的比较如图1所示. 3.1 transformer编码器ERNIE使用多层Transformer （Vaswani et al，2017）作为基本编码器，类似于之前的预训练模型，例如GPT，BERT和XLM。 Transformer 可以通过自注意力机制来捕获句子中每个标记的上下文信息，并生成一系列上下文embedding。对于中文语料库，我们在CJK Unicode范围内的每个字符周围添加空格，并使用WordPiece（Wu等人，2016）标记中文句子。对于给定的token，其输入表示是通过对相应的token、部分和位置进行求和构造的。每个序列的第一个标记是特殊分类嵌入（[CLS]）。 3.2 知识整合我们使用先验知识来增强预训练语言模型。我们没有直接添加知识嵌入，而是提出了一种多阶段知识掩码策略，将短语和实体集成到语言表示中。句子中不同的掩码级别如图2所示。 3.2.1 基本级别的掩码第一个学习阶段是使用基本的掩码，它将一个句子视为一系列基本的语言单元，对于英语，基本的语言单元是单词，对于汉语，基本的语言单元是汉字。在训练过程中，我们随机屏蔽15%的基本语言单元，并使用句子中的其他基本单元作为输入，并训练一个转换器来预测被屏蔽的单元。基于基本层掩码，我们可以得到基本的单词表示。因为它是在基本语义单元随机掩码的基础上训练的，所以高层语义的知识表示很难完全建模。 3.2.2 短语级别的掩码第二阶段是使用短语级掩码。短语是作为概念单位的一小群单词或字符。对于英语，我们使用词汇分析和组块工具来获取句子中短语的边界，并使用一些依赖于语言的切分工具来获取其他语言（如汉语）中的单词/短语信息。在短语级掩码阶段，我们还使用基本语言单元作为训练输入，不像随机基本单元掩码那样，这次我们在句子中多选几个短语，掩码并预测同一短语中的所有基本单元。在这个阶段，短语信息被编码到单词嵌入中。 3.2.3 实体级别的掩码第三阶段是实体级掩码。我们把一些专有名词，如地点、人名、组织、产品等抽象为实体进行屏蔽。实体通常包含句子中的重要信息。在短语屏蔽阶段，我们首先分析一个句子中的命名实体，然后屏蔽和预测实体中所有的空缺。经过这三个阶段的学习，我们可以获得语义信息丰富的表达。 4 实验为了进行更好的比较，ERNIE选取的模型和BERT-base模型相同，也具有12个编码器层、768个隐藏单元和12个注意力头。 4.1 异构语料库的预训练ERNIE采用异构语料库进行预训练。我们构建了混合语料库，包含中文维基百科、百度百科、百度新闻和百度贴吧。句子的数量分别为2100万、5100万、4700万、5400万。百度百科包含以正式语言编写的百科全书文章，这是语言建模的有力基础。百度新闻提供有关电影名称、演员姓名、足球队名称等的最新信息。百度贴吧是一个类似Reddits的开放式论坛，每个帖子都可以被视为一个对话话题。在我们的DLM任务中我们使用了Tieba语料库，我们将在后面讨论这部分内容。我们对汉字进行繁体到简体的转换，对英文字母进行大小写转换。为模型使用了17,964个unicode字符的共享词汇表。 4.2 对话语言模型对话数据对于语义表示很重要，因为相同回复的相应查询语义通常相似。ERNIE在DLM（对话语言模型）任务上对查询-响应对话结构进行建模。如图3所示，我们的方法引入了对话嵌入（dialogue embedding）来识别对话中的角色，这与通用句子编码器的方法不同（Cer等人，2018）。 ERNIE的“对话”嵌入功能与BERT中的token类型嵌入功能相同，不同之处在于ERNIE还可以表示多回合对话（例如QRQ，QRR，QQR，其中Q和R分别代表“查询”和“响应”）。像BERT中的MLM一样，使用掩码来强制使模型预测以查询和响应为条件的缺失词。而且，我们通过用随机选择的句子替换查询或响应来生成假样本。该模型旨在判断多回合对话是真实的还是假的。DLM任务帮助ERNIE学习对话中的隐式关系，这也增强了模型学习的语义表示能力。DLM任务的模型体系结构与MLM任务的模型体系结构兼容，因此它是与MLM任务交替训练的。 4.3 ERNIE在中文自然语言处理任务中的实验ERNIE被用于在自然语言推理、语义相似性判断、命名实体识别、情感分析和问答等5项中文NLP任务上进行实验。 4.3.1 自然语言推理跨语言自然语言推理（XNLI）语料库（Liu等人，2019年）是MultiNLI语料库的众包集合。这些标签用文本蕴涵进行注释，并翻译成包括中文在内的14种语言。标签包含矛盾、中性和修饰。我们遵循BERT中的中文实验（Devlin等人，2018年）。 4.3.2 语义相似性大规模中文问题匹配语料库（LCQMC）（Liu等人，2018）旨在识别两个句子是否具有相同的意图。 数据集中的每一对句子都与一个二进制标签相关联，该二进制标签指示两个句子是否共享相同的意图，并且可以将该任务形式化为预测二进制标签。 4.3.3 命名实时识别MSRA-NER数据集是为姓名识别而设计的，该数据集由Microsoft Research Asia发布。实体包含七种类型，包括人名、地名或组织名称等。此任务可视为序列标记任务。 4.3.4 情感分析CHNSTINCorp（Song bo）是一个旨在判断句子情感的数据集。它包含了一些领域的评论，如酒店、书籍和电子计算机。这项任务的目的是判断句子是积极的还是消极的。 4.3.5 检索问答NLPCC-DBQA数据集的目标(http://tcci.ccf.org.cn/conference/2016/dldoc/evagline2.pdf)选择相应问题的答案。该数据集的评估方法包括MRR（Voorhees，2001）和F1分数。 4.4 实验结果表1给出了5个中文NLP任务的测试结果。可以看出，ERNIE在所有任务上的表现都超过了BERT，在这些NLP任务上创造了最先进的结果。对于XNLI、MSRA-NER、CHNSTINCorp和nlpcc-dbqa（nlpcc：基于知识库的问答，dbqa：基于文档的问答）任务，ERNIE比BERT获得超过1%的绝对精度改进。ERNIE的成功归因于其知识整合策略。 4.5 消融研究为了更好地了解ERNIE，我们将在本节中对ERNIE的每种策略进行消融实验。 4.5.1 知识掩码策略的效果我们从整个语料库中抽取10%的训练数据，以验证知识掩码策略的有效性。结果见表2。我们可以看到，将短语级掩码添加到基线单词级掩码可以提高模型的性能。在此基础上，增加了实体级掩码策略，进一步提高了模型的性能。此外结果还表明，当预训练数据集的大小增加10倍时，XNLI测试集的性能提高了0.8%。 4.5.2 对话语言模型的效果我们还对DLM（对话语言模型）任务进行了消融研究。我们使用10%的具有不同比例的训练语料来说明DLM测试对XNLI开发集的贡献。然后对这些数据集从头开始用ERNIE进行预训练，并对5次随机的微调结果取平均值。详细的实验设置和开发集结果如表3所示，我们可以看到，在这个DLM任务中，开发/测试精度提高了0.7%/1.0%。 4.6 完形填空为了验证ERNIE的知识学习能力，我们使用了几个完形填空测试样本（Taylor，1953）来检验模型。在实验中，我们把命名实体从段落中删除，模型需要推断它是什么。有些情况如图4所示。我们比较了bert和ERNIE的预测。在情况1中，BERT尝试复制出现在上下文中的名称，而ERNIE则记住了文章中提到的有关关系的知识。在情况2和情况5中，BERT可以根据上下文成功学习表现模式，因此可以正确预测命名的实体类型，但是无法使用正确的实体填充插槽。相反，ERNIE可以使用正确的实体填充插槽。在情况3、4、6中，BERT用与句子相关的几个字符填充了空位，但是很难预测语义概念。 ERNIE可以预测除情况4之外的正确实体。尽管ERNIE在情况4中预测了错误的实体，但它可以正确地预测语义类型，并用一个澳大利亚城市填充该位置。总之，这些案例表明ERNIE在基于上下文的知识推理中表现更好。 5.结论本文提出了一种将知识整合到预训练语言模型中的新方法。在5个中文处理任务上的实验表明，该方法在所有这些任务上都优于Bert。我们还确认，知识集成和对异构数据的预训练都能使模型获得更好的语言表示。下一步，我们将会把其他类型的知识集成到语义表示模型中，例如使用句法分析或来自其他任务的弱监督信号。此外，我们还将用其他语言验证这个想法。原文链接：https://arxiv.org/pdf/1904.09223.pdfernie学习地址：https://github.com/PaddlePaddle/ERNIE/blob/develop/README.zh.md","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"}]},{"title":"linux环境下安装Anaconda","slug":"计算机相关/linux环境下安装Anaconda","date":"2021-10-10T00:55:39.000Z","updated":"2022-06-16T00:56:03.776Z","comments":true,"path":"计算机相关/计算机相关/linux环境下安装Anaconda/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Anaconda/","excerpt":"","text":"linux环境下安装Anaconda 1.下载：获取anaconda在清华镜像站的网址，然后在服务器端wget 网址就行了。清华镜像站中anaconda的所有版本的网址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/找到自己想要的那个版本，然后右键 -&gt; 复制链接地址。接下来在服务器端找一个好的目录，wget + 复制好的地址，运行就好。如：1wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh 2.安装下载好之后，在下载目录目录中，出现一个Anaconda3-5.3.1-Linux-x86_64.sh这样子的文件，运行它就好,切换到该文件目录运行1bash Anaconda3-5.3.1-Linux-x86_64.sh根据指示操作：步骤一：按下ENTER继续。步骤二：在按下enter键之后，安装又会停止，并且出现一个“more”的字样，按空格就好。步骤三：接收协议，在这里输入“yes”表示接收协议。步骤四：是否在环境中配置anaconda的环境， 输入yes，不然你自己配环境很麻烦。步骤五：是否安装vscode？这个看个人爱好了，一般我选择否，输入no之后，安装完成。注意：需要退出终端后，再次进入才能输入conda命令。或者不想退出的话，刷新一下环境变量，之后输入conda就有反应了12刷新环境变量命令：source .bash_profile步骤六：输入conda验证是否安装成功，如果出现如下截图内容，说明安装成功接下来就可以愉快的在anaconda中玩耍了。","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"tmux命令总结","slug":"计算机相关/tmux命令总结","date":"2021-10-08T15:21:18.000Z","updated":"2022-06-08T15:21:39.627Z","comments":true,"path":"计算机相关/计算机相关/tmux命令总结/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/tmux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/","excerpt":"","text":"tmux命令总结 1. 是什么tmux即terminal multiplexer（终端复用器），它可以启动一系列终端会话。它解绑了会话和终端窗口。关闭终端窗口再打开，会话并不终止，而是继续运行再执行。将会话与终端窗后彻底分离。 2. 怎么用 2.1 安装12安装：yum install tmux 2.2 入门运行tmux：# tmux新建会话： # tmux new -s SESSION-NAME查看已创建的会话：# tmux ls进入一个已知会话： # tmux a -t SESSION-NAME 或 # tmux attach -t SESSION-NAME暂时离开当前会话：# tmux detach该命令会从当前会话中退出去, 因此才会有稍后重新接入会话这么一说关闭会话：# tmux kill-session -t SESSION-NAME在会话内部或外部执行均可 2.3 进阶 2.3.1 分屏操作很多情况下, 需要在一个会话中运行多个命令，执行多个任务,我们可以在一个会话的多个窗口里组织他们。分屏：分为水平分屏和垂直分屏水平分屏 – 快捷键：先按 ctrl+b, 放开后再按%如何区分水平分屏和垂直分屏，看%和&quot;在键盘中的大概位置，就可以理解垂直分屏 – 快捷键：先按 ctrl+b, 放开后再按 &quot;分屏后的窗口中光标互相切换 – 快捷键：先按ctrl+b, 放开后再按下o (或者直接方向键) 使用快捷键左右分屏 ：Ctrl + b, % (分割当前窗口)上下分屏 ：Ctrl + b, &quot; (分割当前窗口)关闭分屏 ：Ctrl + b, x (关闭所在分屏窗口)显示分屏编号 ：Ctrl + b, q (显示分屏编号)分屏切换 ：Ctrl + b, 方向键 (基本可以自由切换) 2.3.2 切换tmux会话终端快捷键：先按ctrl+b, 放开后再按s 2.3.3 屏中内容上下滚动模式进入tmux翻屏模式：先按 ctrl ＋ｂ，松开，然后再按 [上下翻页：直接通过触摸板控制退出：q 2.3.4 当前窗格全屏显示先按ctrl+b, 放开后再按z：当前窗格全屏显示，再使用一次会变回原来大小 2.3.5 列出当前所有窗口先按ctrl+b, 放开后再按w; 3.其他单独运行tmux命令，即开启一个tmux会话。不能在tmux会话里面再新建会话，会报错：“sessions should be nested with care, unset $TMUX to force”。终端内显示时间：快捷键：先按ctrl+b, 放开后再按t ；退出时间界面：按q键参考资料：https://blog.51cto.com/13683137989/1961188https://zhuanlan.zhihu.com/p/98384704","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"数据分割介绍","slug":"ML/数据分割介绍","date":"2021-06-12T01:14:21.000Z","updated":"2022-06-04T03:47:45.895Z","comments":true,"path":"机器学习/ML/数据分割介绍/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"在机器学习中，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”( testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景:老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？答案是否定的，可能有的同学只会做这10道题却能得高分。回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。可是，我们只有一个包含m个样例的数据集既要训练，又要测试，怎样才能做到呢？答案是:通过对D进行适当的处理，从中产生出训练集S和测试集T。（这个也是我们前面一直在做的事情）。下面我们一起总结一下几种常见的做法：留出法交叉验证法自助法 1 留出法“留出法”(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。大家在使用的过程中，需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。如果从采样( sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为**“分层采样”( stratified sampling)。**例如通过对D进行分层样而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。另一个需注意的问题是，即便在给定训练测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中，这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。因此，单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性( fidelity)。这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试。使用Python实现留出法：1234from sklearn.model_selection import train_test_split#使用train_test_split划分训练集和测试集train_X , test_X, train_Y ,test_Y = train_test_split( X, Y, test_size=0.2,random_state=0)在留出法中，有一个特例，叫：留一法( Leave-One-Out，简称LOO），即每次抽取一个样本做为测试集。显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集一每个子集包含个样本；使用Python实现留一法：123456789101112from sklearn.model_selection import LeaveOneOutdata = [1, 2, 3, 4]loo = LeaveOneOut()for train, test in loo.split(data): print(\"%s %s\" % (train, test))'''结果[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3]'''留一法优缺点：优点：留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，留一法的评估结果往往被认为比较准确。缺点：留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，则需训练1百万个模型，而这还是在未考虑算法调参的情况下。 2 交叉验证法“交叉验证法”( cross validation)先将数据集D划分为k个大小相似的互斥子集，即。每个子集$$D_i$$都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证”(k- fold cross validation)。k最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。下图给出了10折交叉验证的示意图。**与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。**为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有“10次10折交叉验证”。交叉验证实现方法，除了咱们前面讲的GridSearchCV之外，还有KFold, StratifiedKFold KFold和StratifiedKFold1from sklearn.model_selection import KFold,StratifiedKFold用法：将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。注意点：对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本参数说明：n_splits：表示划分几等份shuffle：在每次划分时，是否进行洗牌①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的属性：①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器1234567891011121314151617181920212223242526import numpy as npfrom sklearn.model_selection import KFold,StratifiedKFoldX = np.array([ [1,2,3,4], [11,12,13,14], [21,22,23,24], [31,32,33,34], [41,42,43,44], [51,52,53,54], [61,62,63,64], [71,72,73,74]])y = np.array([1,1,0,0,1,1,0,0])folder = KFold(n_splits = 4, random_state=0, shuffle = False)sfolder = StratifiedKFold(n_splits = 4, random_state = 0, shuffle = False) for train, test in folder.split(X, y): print('train:%s | test:%s' %(train, test)) print(\"\") for train, test in sfolder.split(X, y): print('train:%s | test:%s'%(train, test)) print(\"\")结果：1234567891011121314151617# 第一个for，输出结果为：train:[2 3 4 5 6 7] | test:[0 1]train:[0 1 4 5 6 7] | test:[2 3]train:[0 1 2 3 6 7] | test:[4 5]train:[0 1 2 3 4 5] | test:[6 7]# 第二个for，输出结果为：train:[1 3 4 5 6 7] | test:[0 2]train:[0 2 4 5 6 7] | test:[1 3]train:[0 1 2 3 5 7] | test:[4 6]train:[0 1 2 3 4 6] | test:[5 7]可以看出，sfold进行4折计算时候，是平衡了测试集中，样本正负的分布的；但是fold却没有。 3 自助法我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此**实际评估的模型所使用的训练集比D小，**这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？“自助法”( bootstrapping)是一个比较好的解决方案，它直接以自助采样法( bootstrap sampling)为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D:每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采样中始终不被采到的概率是$$(1-\\frac{1}{m})^m$$，取极限得到即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。于是我们可将D′用作训练集，D\\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称**“包外估计”(out- of-bagestimate）**自助法优缺点：优点：自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。 4 总结综上所述：当我们数据量足够时，选择留出法简单省时，在牺牲很小的准确度的情况下，换取计算的简便；当我们的数据量较小时，我们应该选择交叉验证法，因为此时划分样本集将会使训练数据过少；当我们的数据量特别少的时候，我们可以考虑留一法。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"DeepFM","slug":"ML/DeepFM","date":"2021-04-09T02:24:21.000Z","updated":"2022-11-19T06:23:33.973Z","comments":true,"path":"机器学习/ML/DeepFM/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/DeepFM/","excerpt":"","text":"DeepFM在前面一篇文章中提到，目前遇到特征组合的问题，主流做法主要会分成两类：FM系列、DNN系列。关于DNN相关内容，是深度学习基础知识，本处不展开介绍，直接使用。本文主要介绍FM+DNN的结合体：DeepFM相关内容。文章依旧主要从三方面展开对FM算法介绍When – 什么时候需要考虑DeepFM算法What – 究竟什么是DeepFM算法How – DeepFM怎么使用 1. When什么时候需要考虑DeepFM基于CTR预估的推荐系统，究其根本，其实是学习到用户点击行为背后隐含的特征组合。在各种各样的推荐场景中，低阶特征组合或者高阶的特征组合都会对最终用户的行为产生影响。之前介绍的FM通过对每一维特征的隐变量内积提取特征组合，最终结果还算不错。虽然理论上FM可以对高阶特征组合进行建模，可是因为计算复杂度的原因一般到二阶的特征组合就结束。那么高阶特征该怎么办呢，此时你应该会和很多大牛的想法一样，通过多层的神经网络去解决。 1.1 DNN的局限性以下图片内容参考自张俊林教授在AI大会上的分享我们知道，对于离散特征的处理，一般都是把特征转换为one-hot编码形式，不过把one-hot编码类型的特征输入到DNN中，会导致网络参数过多：就如上图，从输入层到隐藏层，将会产生50亿的参数。解决上面这个问题的方法是：把特征分为不同的field，从one-hot变成dense vector:然后再添加两层全连接层，让其和dense vector进行组合，此时高阶特征的组合就搞定了不过上面方式把低阶和高阶特征组合隐含的体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。又该怎么做呢？ 1.2 模型融合方式此时，我们能想到的就是把DNN与FM进行一个合理的融合：二者的融合总的来说有两种形式，一是串行结构，二是并行结构。融合方式一：并行结构融合方式二：串行结构而我们今天要讲到的DeepFM，就是并行结构中的一种典型代表。 2. What究竟什么是DeepFM算法 2.1 简单介绍DeepFM是2017年华为诺亚方舟实验室发表的一篇论文。论文链接：https://arxiv.org/pdf/1703.04247.pdfDeepFM整体结构：根据上图，我们把图像分成左半部分和右半部分，其实**这也就是DeepFM包含的两部分：DNN部分和FM部分，其中DNN部分负责高阶特征的提取，FM部分负责低阶特征的提取。**这两部分共享同样的输入。 2.2 模型细节接下来，我们从下往上，分别看一下组成DeepFM的各个部分。 a) 架构间参数的传递架构间参数的传递，有几处需要注意，尤其是上面标记红色圈中部分。其中1中，是针对不同特征做的embedding【FM的二阶两两交互计算部分和 deep部分是共享这个embedding结果的】2是FM的一阶计算部分【使用权重直接对原始特征做的一阶计算】3是对应FM的二阶计算阶段，对经过权重embedding的结果做二阶交叉计算4是deep部分的全连接计算部分，使用神经网络进行计算 b) FM计算的过程FM的计算公式，我们在之前文章中讲过，此处拿来直接使用：y^(x)=w0+∑i=1nwixi+∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\hat{y}(x) = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}y^​(x)=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​上公式中，第一项和第二项公式对应上面标圈部分1的内容，即：w0+∑i=1nwixiw_0+\\sum^{n}_{i=1}{w_ix_i}w0​+i=1∑n​wi​xi​公式中第三项公式对应上面标圈部分2的内容，即：∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​ c) DNN部分DNN是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是极其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个特性：1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2）在FM里得到的隐变量VikV_{ik}Vik​现在作为了嵌入层网络的权重。 d) DeepFM预测结果输出最后，上DeepFM中FM和DNN预测结果的输出方式：y^=sigmoid(yFM+yDNN)\\hat{y}=sigmoid(y_{FM}+y_{DNN})y^​=sigmoid(yFM​+yDNN​) 3.HowDeepFM怎么使用推荐百度官方基于paddlepaddle实现框架：链接：https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/deepfm_dygraphPs：参考readme运行一遍就搞定。 参考资料：资料一：https://arxiv.org/pdf/1703.04247.pdf资料二：https://zhuanlan.zhihu.com/p/67795161资料三：http://wiki.baidu.com/pages/viewpage.action?pageId=765563246资料四：https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/deepfm_dygraph","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"因子分解机（Factorization Machines）","slug":"ML/因子分解机（Factorization Machines）","date":"2021-04-08T02:24:21.000Z","updated":"2022-08-10T08:49:13.000Z","comments":true,"path":"机器学习/ML/因子分解机（Factorization Machines）/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88Factorization%20Machines%EF%BC%89/","excerpt":"","text":"因子分解机（Factorization Machines）在推荐系统中，CTR（click-through rate）预估是非常重要的环节，其主要是用于判断一个商品是否被用于推荐。谈到CTR预估，有一个算法不得不提一下，LR（logistic regression）逻辑回归。在推荐系统发展的历史长河中，LR绝对有浓墨重彩的一笔。比如在2020年和微博做算法的同学交流，对方称他们依旧在推荐中使用LR，当然这离不开其非常容易实现大规模实时并行处理的优势。我们知道LR模型其实是一种线性的学习模型，所以它并不可以获取一些高阶特征（非线性）的信息。但是我们在处理多特征的推荐模型时，除了单特征外，有时候还想对一些特征进行组合。关于遇到特征组合的问题，目前主流做法主要分为两类：FM系列、DNN系列。本文将着重介绍FM算法。文章主要从三方面展开对FM算法介绍When – 什么时候需要考虑FM算法What – 究竟什么是FM算法How – FM怎么使用 1. When什么时候需要考虑FM算法需要考虑的情况：情况一：在建模过程中，除了考虑单个特征，还需要考虑特征与特征之间的关联信息时。比如，企业产品个性化定价中，我们除了想知道收入水平、教育水平对用户购买会员的影响，还想知道这两者组合起来对购买会员的影响。情况二：当特征下包含分类比较多的时候，如果通过one-hot处理，就会形成高维的稀疏矩阵，此时直接计算会导致计算量太大，特征权重更新较慢。比如，依旧是企业中个性化定价项目中，特征职业类别会包含很多分类，one-hot编码后，特征空间一下子就会暴增，导致计算量太大，甚至会形成维灾难。FM算法的优势就是对上面两种情况的处理。第一，进行特征组合，通过两两特征组合，引入了交叉项得分；其次，针对维灾难问题，通过引入隐向量，同时对参数矩阵进行矩阵分解，完成对特征的参数估计。 2. What究竟什么是FM算法 2.1 简单介绍因子分解机（Factorization Machines，简称为FM）是2010年由Steffen Rendle提出，是一种基于矩阵分解的机器学习算法。主要用于解决数据稀疏的业务场景下（如推荐业务），特征怎样组合的问题，可以用于求解分类、回归和排序问题。原文：Factorization Machines 2.2 公式推导在引出FM算法前，先看一下线性回归表达式：y=w0+∑i=1nwixiy = w_0+\\sum^{n}_{i=1}{w_ix_i}y=w0​+i=1∑n​wi​xi​其中w0w_0w0​为偏置，wiw_iwi​为每个特征xix_ixi​对应的权重值。我们在前面讨论过线性回归模型最大的缺点就是只能解决单个特征或者需要人工进行特征组合，那么是否可以把特征组合的能力体现在模型的层面呢，显然OK，如下公式：y=w0+∑i=1nwixi+∑i=1n∑j=i+1nwijxixjy = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{w_{ij}x_ix_j}}y=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​wij​xi​xj​上面公式，是把两两组合的特征引入模型了，但是又出现了另一个问题，这个组合的特征泛化能力太弱了，因为如果在训练数据中，xixj=0x_ix_j=0xi​xj​=0,那么wi,j=0w_{i,j}=0wi,j​=0。结果就是wi,jw_{i,j}wi,j​无法通过训练得出。为了求解wi,jw_{i,j}wi,j​，我们对每个特征分量xix_ixi​引入辅助向量vi=(vi1,vi2,...,vik)v_i=(v_{i1},v_{i2},...,v_{ik})vi​=(vi1​,vi2​,...,vik​)。然后，利用vivjTv_iv_j^Tvi​vjT​对wijw_{ij}wij​进行求解（根据矩阵分解思路：对于正定矩阵W，存在矩阵V，使得W=VVTW=VV^TW=VVT）；V=[v11v12⋯v1kv21v22⋯v2k⋮⋮⋱⋮vn1vn2⋯vnk]=[V1V2⋮Vn]V= \\left[ \\begin{matrix} v_{11} &amp; v_{12} &amp; \\cdots &amp; v_{1k} \\\\ v_{21} &amp; v_{22} &amp; \\cdots &amp; v_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ v_{n1} &amp; v_{n2} &amp; \\cdots &amp; v_{nk} \\\\ \\end{matrix} \\right] = \\left[ \\begin{matrix} V_{1} \\\\ V_{2} \\\\ \\vdots \\\\ V_{n} \\\\ \\end{matrix} \\right]V=⎣⎢⎢⎢⎡​v11​v21​⋮vn1​​v12​v22​⋮vn2​​⋯⋯⋱⋯​v1k​v2k​⋮vnk​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎢⎡​V1​V2​⋮Vn​​⎦⎥⎥⎥⎤​那么wijw_{ij}wij​组成的矩阵可以表示成：W^=VVT=[V1V2⋮Vn][V1TV2T⋯VnT]\\hat{W}=VV^T= \\left[ \\begin{matrix} V_{1} \\\\ V_{2} \\\\ \\vdots \\\\ V_{n} \\\\ \\end{matrix} \\right] \\left[ \\begin{matrix} V_{1}^{T} V_{2}^{T} &amp; \\cdots &amp; V_{n}^{T} \\\\ \\end{matrix} \\right]W^=VVT=⎣⎢⎢⎢⎡​V1​V2​⋮Vn​​⎦⎥⎥⎥⎤​[V1T​V2T​​⋯​VnT​​]此时，我们就可以得到FM的表达式：y^(x)=w0+∑i=1nwixi+∑i=1n∑j=i+1n&lt;vi,vj&gt;xixj\\hat{y}(x) = w_0+\\sum^{n}_{i=1}{w_ix_i}+\\sum^{n}_{i=1}{\\sum^{n}_{j=i+1}{&lt;v_i,v_j&gt;x_ix_j}}y^​(x)=w0​+i=1∑n​wi​xi​+i=1∑n​j=i+1∑n​&lt;vi​,vj​&gt;xi​xj​&lt;vi,vj&gt;=∑f=1kvifvjf&lt;v_i,v_j&gt; =\\sum^{k}_{f=1}{v_{if}v_{jf}}&lt;vi​,vj​&gt;=f=1∑k​vif​vjf​其中，n是特征数量， viv_{i}vi​是第iii维特征的隐向量， &lt;,&gt;代表向量点积。隐向量的长度为k&lt;&lt;nk&lt;&lt;nk&lt;&lt;n，包含 kkk个描述特征的因子。同时，直观判断上面表达式的复杂度是O(kn2)O(kn^2)O(kn2)FM现在被广泛应用的其中一个优点是可以通过数学公式化解，把原来表面上看起来O(kn2)O(kn^2)O(kn2)的复杂度降低为O(kn)O(kn)O(kn)。具体的化简过程如下：此时FM的公式就完全搞定。接下来一起看一下如何更新权重值。 2.3 权值求解可以采用随机梯度下降法SGD求解参数(当然其他类似SGD的方法都是可以的)：\\frac{\\partial }{\\partial \\theta}\\hat{y}(X)= \\begin{cases} 1, &amp;if\\ \\theta = w_0\\\\\\ x_i, &amp;if\\ \\theta = w_i\\\\\\ x_i\\sum_{j=1}^n{v_j, fx_j-v_i, fx^2_i}, &amp;if\\ \\theta =v_{i,f}\\\\ \\end{cases}其中，∑j=1nVjfxj\\sum^{n}_{j=1}{V_{jf}x_j}∑j=1n​Vjf​xj​和i无关，可以事先求出。计算过程中，每个梯度都可以在O(1)时间内求出，整体的参数更新时间为O(kn)。经过迭代之后就可以求出结果。 3. HowFM怎么使用使用模式一：通过xlearn搭建模型xlearn官方文档：https://xlearn-doc-cn.readthedocs.io/en/latest/index.html具体举例此处不再赘述，官方文档讲解非常直观。写在最后，产出本篇文档其实主要是为了后面DeepFM文章做铺垫，所以本文讲解重原理轻应用。 参考资料资料一：Factorization Machines资料二：https://zhuanlan.zhihu.com/p/58160982资料三：http://wiki.baidu.com/pages/viewpage.action?pageId=181930125","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"Python中continue和break的区别","slug":"python/Python中continue和break的区别","date":"2020-12-08T02:24:02.000Z","updated":"2023-04-18T23:08:08.839Z","comments":true,"path":"python/python/Python中continue和break的区别/","link":"","permalink":"http://sherwinzhang.com/python/python/Python%E4%B8%ADcontinue%E5%92%8Cbreak%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Python中continue和break的区别在Python中，可以使用continue语句结束当前循环并进入下一次循环，也可以使用break语句完全退出循环。continue语句用于在循环体中的某个条件满足时，跳过本次循环的剩余语句，直接进入下一次循环。以下是一个示例代码：12345pythonCopy codefor i in range(1, 11): if i % 2 == 0: continue # 如果i是偶数，则跳过本次循环 print(i)在上面的代码中，当i为偶数时，continue语句将直接跳过本次循环的剩余语句，进入下一次循环。因此，输出结果只包含奇数1、3、5、7、9。break语句用于在循环体中的某个条件满足时，完全退出循环。以下是一个示例代码：123456pythonCopy codewhile True: s = input('请输入字符串（输入q退出）：') if s == 'q': break # 如果输入q，则退出循环 print(s)在上面的代码中，使用while语句循环读取用户输入的字符串，当用户输入q时，break语句将退出循环。因此，用户可以通过输入q来结束程序。","categories":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/tags/python/"}]},{"title":"python中os._exit()，sys.exit()，exit()的区别是什么？","slug":"python/python 中 os._exit()， sys.exit()， exit() 的区别是什么？","date":"2020-11-08T02:24:02.000Z","updated":"2022-08-16T15:30:47.594Z","comments":true,"path":"python/python/python 中 os._exit()， sys.exit()， exit() 的区别是什么？/","link":"","permalink":"http://sherwinzhang.com/python/python/python%20%E4%B8%AD%20os._exit()%EF%BC%8C%20sys.exit()%EF%BC%8C%20exit()%20%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"python 中 os._exit()， sys.exit()， exit() 的区别是什么？ 1. sys.exit(n)退出程序引发SystemExit异常, 可以捕获异常执行些清理工作.如果有捕获该异常的代码，那么后面的代码还是会执行。一般主程序中使用此退出。举例： import sys try: sys.exit(0) except: print(\"异常捕获\") # 可以执行print语句，因为sys.exit()调用出现了异常 &lt;!--￼0--&gt; 3. exit()/quit()跑出SystemExit异常。一般在交互式shell中退出时使用。","categories":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/tags/python/"}]},{"title":"markdown常见数学公式","slug":"计算机相关/markdown常见数学公式","date":"2020-10-10T00:55:39.000Z","updated":"2022-08-28T08:56:23.389Z","comments":true,"path":"计算机相关/计算机相关/markdown常见数学公式/","link":"","permalink":"http://sherwinzhang.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/markdown%E5%B8%B8%E8%A7%81%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","excerpt":"","text":"1.行内与独行行内公式：将公式插入到本行内，符号：$公式内容$，如：xyzxyzxyz独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，如：$$xyz$$ 2.上标、下标与组合上标符号，符号：^，如：x4x^4x4下标符号，符号：_，如：x1x_1x1​组合符号，符号：{}，如：168O2+2{16}_{8}O{2+}_{2}168​O2+2​ 3.汉字、字体与格式汉字形式，符号：\\mbox{}，如：V_{\\mbox{初始}}字体控制，符号：\\displaystyle，如：x+yy+z\\displaystyle \\frac{x+y}{y+z}y+zx+y​下划线符号，符号：\\underline，如：x+y‾\\underline{x+y}x+y​标签，符号\\tag{数字}，如：\\tag{11}上大括号，符号：\\overbrace{算式}，如：a+b+c+d⏞2.0\\overbrace{a+b+c+d}^{2.0}a+b+c+d​2.0​下大括号，符号：\\underbrace{算式}，如：a+b+c⏟1.0+da+\\underbrace{b+c}_{1.0}+da+1.0b+c​​+d上位符号，符号：\\stacrel{上位符号}{基位符号}，如：x⃗=defx1,…,xn\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}x=defx1​,…,xn​ 4.占位符两个quad空格，符号：\\qquad，如：xyx \\qquad yxyquad空格，符号：\\quad，如：xyx \\quad yxy大空格，符号\\，如：x yx \\ yx y中空格，符号\\:，如：x:yx : yx:y小空格，符号\\,，如：x,yx , yx,y没有空格，符号``，如：xyxyxy紧贴，符号\\!，如：x!yx ! yx!y 5.定界符与组合括号，符号：（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)，如：（）()()()()（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)（）()()()()中括号，符号：[]，如：[x+y][x+y][x+y]大括号，符号：\\{ \\}，如：x+y{x+y}x+y自适应括号，符号：\\left \\right，如：(x)\\left(x\\right)(x)，(xyz)\\left(x{yz}\\right)(xyz)组合公式，符号：{上位公式 \\choose 下位公式}，如：(n+1k)=(nk)+(nk−1){n+1 \\choose k}={n \\choose k}+{n \\choose k-1}(kn+1​)=(kn​)+(k−1n​)组合公式，符号：{上位公式 \\atop 下位公式}，如：∑k0,k1,…&gt;0k0+k1+⋯=nAk0Ak1⋯\\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots∑k0​+k1​+⋯=nk0​,k1​,…&gt;0​​Ak0​​Ak1​​⋯ 6.四则运算加法运算，符号：+，如：x+y=zx+y=zx+y=z减法运算，符号：-，如：x−y=zx-y=zx−y=z加减运算，符号：\\pm，如：x±y=zx \\pm y=zx±y=z减甲运算，符号：\\mp，如：x∓y=zx \\mp y=zx∓y=z乘法运算，符号：\\times，如：x×y=zx \\times y=zx×y=z点乘运算，符号：\\cdot，如：x⋅y=zx \\cdot y=zx⋅y=z星乘运算，符号：\\ast，如：x∗y=zx \\ast y=zx∗y=z除法运算，符号：\\div，如：x÷y=zx \\div y=zx÷y=z斜法运算，符号：/，如：x/y=zx/y=zx/y=z分式表示，符号：\\frac{分子}{分母}，如：x+yy+z\\frac{x+y}{y+z}y+zx+y​分式表示，符号：{分子} \\voer {分母}，如：x+yy+z{x+y} \\over {y+z}y+zx+y​绝对值表示，符号：||，如：∣x+y∣|x+y|∣x+y∣ 7.高级运算平均数运算，符号：\\overline{算式}，如：xyz‾\\overline{xyz}xyz​开二次方运算，符号：\\sqrt，如：x\\sqrt xx​开方运算，符号：\\sqrt[开方数]{被开方数}，如：x+y3\\sqrt[3]{x+y}3x+y​对数运算，符号：\\log，如：log⁡(x)\\log(x)log(x)极限运算，符号：\\lim，如：lim⁡y→0x→∞xy\\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}limy→0x→∞​yx​极限运算，符号：\\displaystyle \\lim，如：lim⁡y→0x→∞xy\\displaystyle \\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}y→0limx→∞​yx​求和运算，符号：\\sum，如：∑y→0x→∞xy\\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}∑y→0x→∞​yx​求和运算，符号：\\displaystyle \\sum，如：∑y→0x→∞xy\\displaystyle \\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}y→0∑x→∞​yx​积分运算，符号：\\int，如：∫0∞xdx\\int^{\\infty}_{0}{xdx}∫0∞​xdx积分运算，符号：\\displaystyle \\int，如：∫0∞xdx\\displaystyle \\int^{\\infty}_{0}{xdx}∫0∞​xdx微分运算，符号：\\partial，如：∂x∂y\\frac{\\partial x}{\\partial y}∂y∂x​矩阵表示，符号：\\begin{matrix} \\end{matrix}，如：\\left[ \\begin{matrix} 1 &amp;2 &amp;\\cdots &amp;4\\5 &amp;6 &amp;\\cdots &amp;8\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\13 &amp;14 &amp;\\cdots &amp;16\\end{matrix} \\right] 8.逻辑运算等于运算，符号：=，如：x+y=zx+y=zx+y=z大于运算，符号：&gt;，如：x+y&gt;zx+y&gt;zx+y&gt;z小于运算，符号：&lt;，如：x+y&lt;zx+y&lt;zx+y&lt;z大于等于运算，符号：\\geq，如：x+y≥zx+y \\geq zx+y≥z小于等于运算，符号：\\leq，如：x+y≤zx+y \\leq zx+y≤z不等于运算，符号：\\neq，如：x+y≠zx+y \\neq zx+y​=z不大于等于运算，符号：\\ngeq，如：x+y≱zx+y \\ngeq zx+y≱z不大于等于运算，符号：\\not\\geq，如：x+y≱zx+y \\not\\geq zx+y​≥z不小于等于运算，符号：\\nleq，如：x+y≰zx+y \\nleq zx+y≰z不小于等于运算，符号：\\not\\leq，如：x+y≰zx+y \\not\\leq zx+y​≤z约等于运算，符号：\\approx，如：x+y≈zx+y \\approx zx+y≈z恒定等于运算，符号：\\equiv，如：x+y≡zx+y \\equiv zx+y≡z 9.集合运算属于运算，符号：\\in，如：x∈yx \\in yx∈y不属于运算，符号：\\notin，如：x∉yx \\notin yx∈/​y不属于运算，符号：\\not\\in，如：x∉yx \\not\\in yx​∈y子集运算，符号：\\subset，如：x⊂yx \\subset yx⊂y子集运算，符号：\\supset，如：x⊃yx \\supset yx⊃y真子集运算，符号：\\subseteq，如：x⊆yx \\subseteq yx⊆y非真子集运算，符号：\\subsetneq，如：x⊊yx \\subsetneq yx⊊y真子集运算，符号：\\supseteq，如：x⊇yx \\supseteq yx⊇y非真子集运算，符号：\\supsetneq，如：x⊋yx \\supsetneq yx⊋y非子集运算，符号：\\not\\subset，如：x⊄yx \\not\\subset yx​⊂y非子集运算，符号：\\not\\supset，如：x⊅yx \\not\\supset yx​⊃y并集运算，符号：\\cup，如：x∪yx \\cup yx∪y交集运算，符号：\\cap，如：x∩yx \\cap yx∩y差集运算，符号：\\setminus，如：x∖yx \\setminus yx∖y同或运算，符号：\\bigodot，如：x⨀yx \\bigodot yx⨀y同与运算，符号：\\bigotimes，如：x⨂yx \\bigotimes yx⨂y实数集合，符号：\\mathbb{R}，如：\\mathbb{R}自然数集合，符号：\\mathbb{Z}，如：\\mathbb{Z}空集，符号：\\emptyset，如：∅\\emptyset∅ 10.数学符号无穷，符号：\\infty，如：∞\\infty∞虚数，符号：\\imath，如：ı\\imathı虚数，符号：\\jmath，如：ȷ\\jmathȷ数学符号，符号\\hat{a}，如：a^\\hat{a}a^数学符号，符号\\check{a}，如：aˇ\\check{a}aˇ数学符号，符号\\breve{a}，如：a˘\\breve{a}a˘数学符号，符号\\tilde{a}，如：a~\\tilde{a}a~数学符号，符号\\bar{a}，如：aˉ\\bar{a}aˉ矢量符号，符号\\vec{a}，如：a⃗\\vec{a}a数学符号，符号\\acute{a}，如：aˊ\\acute{a}aˊ数学符号，符号\\grave{a}，如：aˋ\\grave{a}aˋ数学符号，符号\\mathring{a}，如：a˚\\mathring{a}a˚一阶导数符号，符号\\dot{a}，如：a˙\\dot{a}a˙二阶导数符号，符号\\ddot{a}，如：a¨\\ddot{a}a¨上箭头，符号：\\uparrow，如：↑\\uparrow↑上箭头，符号：\\Uparrow，如：⇑\\Uparrow⇑下箭头，符号：\\downarrow，如：↓\\downarrow↓下箭头，符号：\\Downarrow，如：⇓\\Downarrow⇓左箭头，符号：\\leftarrow，如：←\\leftarrow←左箭头，符号：\\Leftarrow，如：⇐\\Leftarrow⇐右箭头，符号：\\rightarrow，如：→\\rightarrow→右箭头，符号：\\Rightarrow，如：⇒\\Rightarrow⇒底端对齐的省略号，符号：\\ldots，如：1,2,…,n1,2,\\ldots,n1,2,…,n中线对齐的省略号，符号：\\cdots，如：x12+x22+⋯+xn2x_1^2 + x_2^2 + \\cdots + x_n^2x12​+x22​+⋯+xn2​竖直对齐的省略号，符号：\\vdots，如：⋮\\vdots⋮斜对齐的省略号，符号：\\ddots，如：⋱\\ddots⋱ 11.希腊字母字母实现字母实现AAα\\alhpaBBβ\\betaΓ\\Gammaγ\\gammaΔ\\Deltaδ\\deltaEEϵ\\epsilonZZζ\\zetaHHη\\etaΘ\\Thetaθ\\thetaIIι\\iotaKKκ\\kappaΛ\\Lambdaλ\\lambdaMMμ\\muNNν\\nuΞ\\Xiξ\\xiOOο\\omicronΠ\\Piπ\\piPPρ\\rhoΣ\\Sigmaσ\\sigmaTTτ\\tauΥ\\Upsilonυ\\upsilonΦ\\Phiϕ\\phiXXχ\\chiΨ\\Psiψ\\psiΩ\\vω\\omega","categories":[{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"}]},{"title":"朗格朗日乘子法","slug":"ML/初识-朗格朗日乘子法","date":"2019-10-08T02:24:21.000Z","updated":"2022-08-16T15:31:04.773Z","comments":true,"path":"机器学习/ML/初识-朗格朗日乘子法/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%88%9D%E8%AF%86-%E6%9C%97%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/","excerpt":"","text":"朗格朗日乘子法拉格朗日乘子法 (Lagrange multipliers)是一种寻找多元函数在一组约束下的极值的方法.通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为具有 d + k 个变量的无约束优化问题求解。本文希望通过一个直观简单的例子尽力解释拉格朗日乘子法和KKT条件的原理。以包含一个变量一个约束的简单优化问题为例。如图所示，我们的目标函数是f(x)=x2+4x−1f(x)=x^2+4x−1f(x)=x2+4x−1，讨论两种约束条件𝑔(𝑥)g(x)：在满足x≤−1 约束条件下求目标函数的最小值；在满足 x≥−1约束条件g(x)下求目标函数的最小值。我们可以直观的从图中得到，对于约束 1) 使目标值f(x)最小的最优解是x=−2；对于约束 2) 使目标值f(x)最小的最优解是x=−1。下面我们用拉格朗日乘子来求解这个最优解。当没有约束的时候，我们可以直接令目标函数的导数为0，求最优值。可现在有约束，那怎么边考虑约束边求目标函数最优值呢？最直观的办法是把约束放进目标函数里，由于本例中只有一个约束，所以引入一个朗格朗日乘子λ，构造一个新的函数，拉格朗日函数h(x)，h(x)=f(x)+λg(x)h(x)=f(x)+λg(x)h(x)=f(x)+λg(x)该拉格朗日函数h(x)最优解可能在g(x)&lt;0区域中，或者在边界g(x)=0上，下面具体分析这两种情况，当g(x)&lt;0时，也就是最优解在g(x)&lt;0区域中， 对应约束1) x≤−1的情况。此时约束对求目标函数最小值不起作用，等价于λ=0，直接通过条件∇𝑓(𝑥∗)=0，得拉格朗日函数h(x)最优解x=−2。当g(x)=0时，也就是最优解在边界g(x)=0上，对应约束1) x≥−1的情况。此时不等式约束转换为等式约束，也就是在λ&gt;0、约束起作用的情况下，通过求∇𝑓(𝑥∗)+𝜆∇𝑔(𝑥∗)=0，得拉格朗日函数h(x)最优解x=−1。所以整合这两种情况，必须满足λg(x)=0因此约束g(x)最小化f(x)的优化问题，可通过引入拉格朗日因子转化为在如下约束下，最小化拉格朗日函数h(x)，上述约束条件成为KKT条件。该KKT条件可扩展到多个等式约束和不等式约束的优化问题。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"分类中解决类别不平衡问题","slug":"ML/分类中解决类别不平衡问题","date":"2019-08-11T02:28:21.000Z","updated":"2022-08-10T14:58:55.006Z","comments":true,"path":"机器学习/ML/分类中解决类别不平衡问题/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%88%86%E7%B1%BB%E4%B8%AD%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98/","excerpt":"","text":"分类中解决类别不平衡问题在现实环境中，采集的数据（建模样本）往往是比例失衡的。比如网贷数据，逾期人数的比例是极低的（千分之几的比例）；奢侈品消费人群鉴定等。 1.类别不平衡数据集基本介绍在这一节中，我们一起看一下，当遇到数据类别不平衡的时候，我们该如何处理。在Python中，有Imblearn包，它就是为处理数据比例失衡而生的。安装Imblearn包1pip3 install imbalanced-learn第三方包链接：https://pypi.org/project/imbalanced-learn/创造数据集12345678910111213from sklearn.datasets import make_classificationimport matplotlib.pyplot as plt#使用make_classification生成样本数据X, y = make_classification(n_samples=5000, n_features=2, # 特征个数= n_informative（） + n_redundant + n_repeated n_informative=2, # 多信息特征的个数 n_redundant=0, # 冗余信息，informative特征的随机线性组合 n_repeated=0, # 重复信息，随机提取n_informative和n_redundant 特征 n_classes=3, # 分类类别 n_clusters_per_class=1, # 某一个类别是由几个cluster构成的 weights=[0.01, 0.05, 0.94], # 列表类型，权重比 random_state=0)查看各个标签的样本12345#查看各个标签的样本量from collections import CounterCounter(y)# Counter(&#123;2: 4674, 1: 262, 0: 64&#125;)数据集可视化123# 数据集可视化plt.scatter(X[:, 0], X[:, 1], c=y)plt.show()可以看出样本的三个标签中，1，2的样本量极少，样本失衡。下面使用imblearn进行过采样。接下来，我们就要基于以上数据，进行相应的处理。关于类别不平衡的问题，主要有两种处理方式：过采样方法增加数量较少那一类样本的数量，使得正负样本比例均衡。欠采样方法减少数量较多那一类样本的数量，使得正负样本比例均衡。 2.解决类别不平衡数据方法介绍 2.1 过采样方法 2.1.1 什么是过采样方法对训练集里的少数类进行“过采样”（oversampling），即增加一些少数类样本使得正、反例数目接近，然后再进行学习。 2.1.2 随机过采样方法随机过采样是在少数类 中随机选择一些样本，然后**通过复制所选择的样本生成样本集 ，**将它们添加到 中来扩大原始数据集从而得到新的少数类集合 。新的数据集 。通过代码实现随机过采样方法：12345678910111213# 使用imblearn进行随机过采样from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)#查看结果Counter(y_resampled) #过采样后样本结果# Counter(&#123;2: 4674, 1: 4674, 0: 4674&#125;)# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：对于随机过采样，由于需要对少数类样本进行复制来扩大数据集，造成模型训练复杂度加大。另一方面也容易造成模型的过拟合问题，因为随机过采样是简单的对初始样本进行复制采样，这就使得学习器学得的规则过于具体化，不利于学习器的泛化性能，造成过拟合问题。为了解决随机过采样中造成模型过拟合问题，又能保证实现数据集均衡的目的，出现了过采样法代表性的算法SMOTE算法。 2.1.3 过采样代表性算法-SMOTESMOTE全称是Synthetic Minority Oversampling即合成少数类过采样技术。SMOTE算法是对随机过采样方法的一个改进算法，由于随机过采样方法是直接对少数类进行重采用，会使训练集中有很多重复的样本，容易造成产生的模型过拟合问题。而SMOTE算法的基本思想:对每个少数类样本 ，从它的最近邻中随机选择一个样本 （ 是少数类中的一个样本），然后在 和 之间的连线上随机选择一点作为新合成的少数类样本。SMOTE算法合成新少数类样本的算法描述如下：对于少数类中的每一个样本 ，以欧氏距离为标准计算它到少数类样本集 中所有样本的距离，得到其k近邻。根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本 ，从其k近邻中随机选择若干个样本，假设选择的是 。对于每一个随机选出来的近邻 ，分别与 按照如下公式构建新的样本。我们用图文表达的方式，再来描述一下SMOTE算法。先随机选定一个少数类样本 。找出这个少数类样本 的K个近邻（假设K=5），5个近邻已经被圈出。随机从这K个近邻中选出一个样本 （用绿色圈出来了）。4)在少数类样本 和被选中的这个近邻样本 之间的连线上，随机找一点。这个点就是人工合成的新的样本点（绿色正号标出）。SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。代码实现：1234567891011# SMOTE过采样from imblearn.over_sampling import SMOTEX_resampled, y_resampled = SMOTE().fit_resample(X, y)Counter(y_resampled)# 采样后样本结果# [(0, 4674), (1, 4674), (2, 4674)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show() 2.2 欠采样方法 2.2.1 什么是欠采样方法直接对训练集中多数类样本进行“欠采样”（undersampling），即去除一些多数类中的样本使得正例、反例数目接近，然后再进行学习。 2.2.2 随机欠采样方法随机欠采样顾名思义即从多数类 中随机选择一些样样本组成样本集 。然后将样本集 从 中移除。新的数据集 。代码实现：123456789101112# 随机欠采样from imblearn.under_sampling import RandomUnderSamplerrus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)Counter(y_resampled)# 采样后结果[(0, 64), (1, 64), (2, 64)]# 数据集可视化plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)plt.show()缺点：随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但是这也存在一些问题。对于随机欠采样，由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。官网链接：https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"独立同分布","slug":"ML/独立同分布","date":"2019-06-08T02:24:21.000Z","updated":"2022-06-04T03:47:35.264Z","comments":true,"path":"机器学习/ML/独立同分布/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83/","excerpt":"","text":"独立同分布IID(independent and identically distributed) 1 独立同分布(i.i.d.)在概率统计理论中，如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。在西瓜书中解释是：输入空间中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。 2 简单解释 — 独立、同分布、独立同分布（1）独立：每次抽样之间没有关系，不会相互影响举例：给一个骰子，每次抛骰子抛到几就是几，这是独立；如果我要抛骰子两次之和大于8，那么第一次和第二次抛就不独立，因为第二次抛的结果和第一次相关。（2）同分布：每次抽样，样本服从同一个分布举例：给一个骰子，每次抛骰子得到任意点数的概率都是六分之一，这个就是同分布（3）独立同分布：i.i.d.，每次抽样之间独立而且同分布 3 机器学习领域的重要假设IID独立同分布即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。 4 目前发展机器学习并不总要求独立同分布，在不少问题中要求样本数据采样自同一个分布是因为希望用训练数据集得到的模型可以合理的用于测试数据集，使用独立同分布假设能够解释得通。目前一些机器学习内容已经不再囿于独立同分布假设下，一些问题会假设样本没有同分布。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"如何理解无偏估计？","slug":"ML/如何理解无偏估计","date":"2019-05-20T02:28:21.000Z","updated":"2022-08-10T14:58:37.847Z","comments":true,"path":"机器学习/ML/如何理解无偏估计/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/","excerpt":"","text":"如何理解无偏估计？ 1.如何理解无偏估计无偏估计：就是我认为所有样本出现的概率一样。假如有N种样本我们认为所有样本出现概率都是1/N。然后根据这个来计算数学期望。此时的数学期望就是我们平常讲的平均值。数学期望本质就是平均值 2.无偏估计为何叫做“无偏”？它要“估计”什么？首先回答第一个问题：它要“估计”什么？它要估计的是整体的数学期望（平均值）。第二个问题：那为何叫做无偏？有偏是什么？假设这个是一些样本的集合:$$X=x_1, x_2, x_3,…,x_N$$ 我们根据样本估计整体的数学期望（平均值）。因为正常求期望是加权和，什么叫加权和这个就叫加权和。每个样本出现概率不一样，概率大的乘起来就大，这个就产生偏重了（有偏估计）。但是，但是我们不知道某个样本出现的概率啊。比如你从别人口袋里面随机拿了3张钞票。两张是十块钱，一张100元，然后你想估计下他口袋里的剩下的钱平均下来每张多少钱（估计平均值）。然后呢？无偏估计计算数学期望就是认为所有样本出现概率一样大，没有看不起哪个样本。回到求钱的平均值的问题。无偏估计我们认为每张钞票出现概率都是1/2（因为只出现了10和100这两种情况，所以是1/2。如果是出现1 10 100三种情况，每种情况概率则是1/3。哪怕拿到了两张十块钱，我还是认为十块钱出现的概率和100元的概率一样。不偏心。所以无偏估计，所估计的别人口袋每张钱的数学期望（平均值）=10 * 1/2+100 * 1/2。有偏估计那就是偏重那些出现次数多的样本。认为样本的概率是不一样的。我出现了两次十块钱，那么我认为十块钱的概率是2/3，100块钱概率只有1/3. 有偏所估计的别人口袋每张钱的数学期望（平均值）=10 * 2/3+100 * 1/3。 3.为何要用无偏估计？因为现实生活中我不知道某个样本出现的概率啊，就像骰子，我不知道他是不是加过水银。所以我们暂时按照每种情况出现概率一样来算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"模型的保存和加载","slug":"ML/模型的保存和加载","date":"2019-05-09T02:28:21.000Z","updated":"2022-08-10T14:58:45.728Z","comments":true,"path":"机器学习/ML/模型的保存和加载/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/","excerpt":"","text":"模型的保存和加载 1.sklearn模型的保存和加载APIfrom sklearn.externals import joblib保存：joblib.dump(estimator, ‘test.pkl’)加载：estimator = joblib.load(‘test.pkl’) 2.线性回归的模型保存加载案例1234567891011121314151617181920212223242526272829303132333435363738def load_dump_demo(): \"\"\" 模型保存和加载 :return: \"\"\" # 1.获取数据 data = load_boston() # 2.数据集划分 x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=22) # 3.特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-线性回归(岭回归) # # 4.1 模型训练 # estimator = Ridge(alpha=1) # estimator.fit(x_train, y_train) # # # 4.2 模型保存 # joblib.dump(estimator, \"./data/test.pkl\") # 4.3 模型加载 estimator = joblib.load(\"./data/test.pkl\") # 5.模型评估 # 5.1 获取系数等值 y_predict = estimator.predict(x_test) print(\"预测值为:\\n\", y_predict) print(\"模型中的系数为:\\n\", estimator.coef_) print(\"模型中的偏置为:\\n\", estimator.intercept_) # 5.2 评价 # 均方误差 error = mean_squared_error(y_test, y_predict) print(\"误差为:\\n\", error) 3.tips如果你在学习过程中，发现使用上面方法报如下错误：1ImportError: cannot import name 'joblib' from 'sklearn.externals' (/Library/Python/3.7/site-packages/sklearn/externals/__init__.py)这是因为scikit-learn版本在0.21之后，无法使用from sklearn.externals import joblib进行导入，你安装的scikit-learn版本有可能是最新版本。如果需要保存模块，可以使用：12345# 安装pip install joblib# 导入import joblib安装joblib,然后使用joblib.load进行加载；使用joblib.dump进行保存参考：https://scikit-learn.org/stable/modules/model_persistence.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"ROC曲线的绘制","slug":"ML/ROC曲线的绘制","date":"2019-03-11T02:28:21.000Z","updated":"2022-08-10T14:59:09.912Z","comments":true,"path":"机器学习/ML/ROC曲线的绘制/","link":"","permalink":"http://sherwinzhang.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML/ROC%E6%9B%B2%E7%BA%BF%E7%9A%84%E7%BB%98%E5%88%B6/","excerpt":"","text":"ROC曲线的绘制关于ROC曲线的绘制过程，通过以下举例进行说明假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。然后在这6次展示的时候都通过model算出了点击的概率序列。下面看三种情况。 1.曲线绘制 1.1 序列一曲线绘制如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）1100000.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 1.2 序列二曲线绘制如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）与原来的序列一起，得到序列（从概率从高到低排）1010000.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；3）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；4）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 1.3 序列三曲线绘制如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）与原来的序列一起，得到序列（从概率从高到低排）0000110.90.80.70.60.50.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（6:0.9,5:0.8,4:0.7,2:0.6,3:0.5,1:0.4）；2）从概率最大开始取一个点作为正类，取到点6，计算得到TPR=0.0，FPR=0.25；3）从概率最大开始，再取一个点作为正类，取到点5，计算得到TPR=0.0，FPR=0.5；4）再从最大开始取一个点作为正类，取到点4，计算得到TPR=0.0，FPR=0.75;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0.25,0.0),(0.5,0.0),(0.75,0.0),(1.0,0.0),(1.0,0.5),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 2.意义解释如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"}]},{"title":"2019-春节假期生活杂感","slug":"程序人生/2019年春节假期生活杂感","date":"2019-02-05T05:32:15.000Z","updated":"2022-05-14T16:04:47.503Z","comments":true,"path":"程序人生/程序人生/2019年春节假期生活杂感/","link":"","permalink":"http://sherwinzhang.com/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/2019%E5%B9%B4%E6%98%A5%E8%8A%82%E5%81%87%E6%9C%9F%E7%94%9F%E6%B4%BB%E6%9D%82%E6%84%9F/","excerpt":"","text":"又是和往年一样的春节放假，依旧是常规的操作，放假回来，在家懒着，然后腊月底和爸一起去上坟，最后几天贴春联，挂灯笼等操作。【三十而立，面包杂想】现在是腊三十，不管从阳历还是阴历，自己总算要又长一岁了，虚岁要步入三十大关。古代三十而立，在此插入，非常有必要解释一下，到底什么是三十而立12345孔子所说的“三十而立”，是指他在这个时候做事合于礼，言行都很得当。 [1] 言：谦卑 中 传递祥和！行：举止 彬彬有礼！现常用来指人开始有所成就。三十而立：三十岁的时候就可以自立于世。2019-02-05(正月初一)接着上次的内容继续扯东扯西，三十而立，就自己理解，立的内容是两方面吧，一个是精神层面的面包，一个是真实世界的面包。关于精神层面，具体而言，自从从事计算机相关的工作，这个并没有什么大的进步，关于真实世界的面包，可能限于环境的限制，更多的考虑的是这方面的内容吧，有房吗，有车吗。。。有时候醒悟，面包不仅仅一直局限于真实世界的单一口味，还需要充实自己的精神世界。最好的目标彼岸，真实环境，精神层面都有面包收获，这应该是自己期望达到的目标吧。【2019-沉淀再想】这个假期，说实在的，过得不是那么心安理得，因为有很多事情需要自己去完成，但是都没有去行动，比如一直说要去完成的那篇论文，比如说要准备机器学习，再比如说对自己人工智能的充电。就那么没心没肺的过着，但是当回想的时候，还是蛮有愧疚感的。可能春节回家继续办公、学习，这件事情本身就比较反人类，哈哈。既然回家之前把目标确定了，那么就在接下来的这几天努力尽量完成目标，别加深自己的罪恶感。关于假期目标，其实是希望达到高度的折射，也从另一方面说明了2019必须干的事情。12如何沉淀自己人工智能相关领域的知识？需要自己计划好，然后去坚持完成关于2019的基调，需要主要精力去做的事情，都需要自己定位好，然后去严格的执行，只有一步步的去执行，才能在最后获取到相应的高度，加油！！【关于潮流变化】假期回家，被父母每天观看的快手充斥着，自己也为了体验一下究竟什么样产品可以勾住几乎所有人，下载浏览，快手给我带来更大的吃惊之处是，我妈竟然在昨天从快手中提出20元，这个是她这段时间直播的收入，瞬间觉得，这个社会发展的和自己之前的理解有些偏差。这个社会在追求快速发展的同时也失去了一些东西。之前自己是一直不下载观看快手之类的app的，因为觉得里面的内容，大部分都没有营养，看的过程中，也就是当时博君一笑，尔尔。但是当这个东西还可以赚钱，在利益的驱动下，是否会有更多没有营养的内容被鼓励产生。产品的制作，宣传已经没有了底线可言，怎么让产品更符合大众，更能吊起大众的胃口，怎么设计。如此产品，怎么经得起深究思考呢？或者，产本设计者自己就没有设计一个底线，只是在追求利益。","categories":[{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"}],"tags":[{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"}]},{"title":"聊天机器人介绍","slug":"智能客服/11.聊天机器人综合知识","date":"2018-05-17T15:21:18.000Z","updated":"2022-08-25T05:11:32.000Z","comments":true,"path":"智能客服/智能客服/11.聊天机器人综合知识/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/11.%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%BB%BC%E5%90%88%E7%9F%A5%E8%AF%86/","excerpt":"","text":"聊天机器人介绍对话智能：国际视角，国内形势及案例学习实录分享 | 计算未来轻沙龙：对话系统研究进展（视频 + PPT）聊天机器人初学者完全指南万物有灵：人机对话系统解析Bot：带来对话式体验的下一代UI聊天机器人技术的研究进展总结│解密 chatbot 人工智能聊天机器人 技术沙龙盘点 | 聊天机器人的发展状况与分类巨头们都很重视的聊天机器人，你不进来看看吗？为什么聊天机器人从业者都很委屈？聊天机器人多会长成“女孩” 性别和性格会突变这个时代，机器人也要肤白貌美性格好？人工智障 2 : 你看到的AI与智能无关为什么现在的人工智能助理都像人工智障？【独家】百度朱凯华：智能搜索和对话式OS最新技术全面解读（65PPT）专栏 | 聊天机器人：困境和破局甲骨文 聊天机器人，全天候的智能助理AI聊天机器人设计指南 | AIID编译组基于金融-司法领域(兼有闲聊性质)的聊天机器人","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"开放平台接口定义参考","slug":"智能客服/10.开放平台接口定义参考","date":"2018-05-16T15:21:18.000Z","updated":"2022-08-28T14:16:12.544Z","comments":true,"path":"智能客服/智能客服/10.开放平台接口定义参考/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/10.%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%E6%8E%A5%E5%8F%A3%E5%AE%9A%E4%B9%89%E5%8F%82%E8%80%83/","excerpt":"","text":"开放平台定义参考百度智能对话平台UNIT-企业版UNIT智能对话定制与服务平台- 文档中心阿里云智能对话机器人会话API华为云问答机器人API","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"智能客服综合资料","slug":"智能客服/12.综合资料","date":"2018-05-16T15:21:18.000Z","updated":"2022-08-28T14:17:48.951Z","comments":true,"path":"智能客服/智能客服/12.综合资料/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/12.%E7%BB%BC%E5%90%88%E8%B5%84%E6%96%99/","excerpt":"","text":"对话机器人思考(下):复杂多轮对话的关键功能点机器人和你对话时在想什么？检索式chatbot了解一下？最新综述：对话系统之用户模拟器Domain+Intent+Slot真的在理解自然语言吗？AI中台——智能聊天机器人平台的架构与应用|分享实录（附视频）回顾·多轮对话提升自动化流程服务-Chatopera基于CNN和序列标注的对联机器人 | 附数据集 &amp; 开源代码深度长文：NLP的巨人肩膀（上）问答系统冠军之路：用CNN做问答任务的QANet竹间智能CTO翁嘉颀：如何打造主动式对话机器人 | 吃瓜笔记揭开知识库问答KB-QA的面纱5·深度学习上篇CIIS演讲实录丨王昊奋： 从聊天机器人到虚拟生命：AI技术的新机遇从聊天机器人到虚拟生命：AI技术的新机遇聊天机器人“进化论”:从陪你聊到懂你心研学·产品设计 | Chatbot的人格很重要吗？用 TensorFlow 实现智能机器人以 Facebook 的 wit.ai 为例讲解机器人对话平台（Bot Framework）从api.ai工作原理来看构建简单场景chatbot的一般方法利用逻辑回归模型判断用户提问意图一天开发一款聊天机器人分享丨浅谈垂直领域的chatbot学术】联合意图推测和槽位填充的基于注意力的循环神经网络模型赛尔原创 | 聊天机器人中用户出行消费意图识别方法研究“搜你所想”之用户搜索意图识别图灵机器人：带着千亿条语料库，它成为了 QQ 的群聊机器人 | 创业-PaperWeekly 第40期 | 对话系统任务综述与基于POMDP的对话系统让聊天机器人同你聊得更带劲 - 对话策略学习 | 论文访谈间 #21“小会话，大学问” - 如何让聊天机器人读懂对话历史？| 论文访谈间 #03最全盘点| 入侵保险业的聊天机器人推荐｜自己动手做聊天机器人（全套资料）像人一样自然流畅地说话，下一代智能对话系统还有多长的路要走？机器学习利用 Elasticsearch 进行更智能搜索用Rasa NLU构建自己的中文NLU系统基于RASA的task-orient对话系统解析（二）——对话管理核心模块实战 | 让机器人替你聊天，还不被人看出破绽？来，手把手教你训练一个克隆版的你观点 | 如何从一名软件工程师转行做人工智能？未来，AI+多轮对话将怎样玩转智能客服聊天机器人落地及进阶实战 | 公开课速记谁能打败Siri 相关回顾·机器人·人机交互·技术介绍「回顾」猎户星空NLP技术进展及产品应用「回顾」自然语言处理中的多任务学习「回顾」NLP在网络文学领域的应用回顾·音乐垂域的自然语言理解自然语言处理在金融实时事件监测和财务快讯中的应用[论文简介] 探索类人开放域聊天机器人情感分析技术：让智能客服更懂人类情感-阿里云开发者社区快速搭建对话机器人，就用这一招！CrossWOZ，一个大规模跨领域中文任务导向对话数据集技术动态 | 多轮检索式问答数十名工程师作战5天，阿里达摩院连夜研发智能疫情机器人语音AI革命十年，不忍看，不敢看！NLP 技术在微博 feed 流中的应用Facebook 开源 Python 预测工具，用起来太方便了","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"知识库","slug":"智能客服/09.知识库","date":"2018-05-15T15:21:18.000Z","updated":"2022-08-25T05:10:49.000Z","comments":true,"path":"智能客服/智能客服/09.知识库/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/09.%E7%9F%A5%E8%AF%86%E5%BA%93/","excerpt":"","text":"知识库王浩：新一代智能化知识库（PPT可下载）新一代呼叫中心智能知识库什么样？你问我答之「YiBot知识体系运营知多少」揭开知识库的神秘面纱8·非结构化知识源篇智能客服知识库的3件核心工作项目实战｜智能客服（“七鱼”、“小i机器人”）产品分析-今日头条","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"知识图谱","slug":"智能客服/08.知识图谱","date":"2018-05-14T15:21:18.000Z","updated":"2022-08-28T14:16:49.226Z","comments":true,"path":"智能客服/智能客服/08.知识图谱/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/08.%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/","excerpt":"","text":"知识图谱回顾·知识图谱在贝壳找房的从0到1实践基于知识图谱的问答在美团智能交互场景中的应用和演进基于知识图谱的人机对话系统 | 公开课笔记基于知识图谱的问答系统关键技术研究（一）基于知识图谱的问答系统关键技术研究（三）肖仰华 | 基于知识图谱的问答系统当知识图谱遇上聊天机器人聊天机器人对知识图谱有哪些特殊的需求？回顾·如何构建知识图谱？「回顾」旅游知识图谱的构建和应用「回顾」金融知识图谱的应用与探索知识图谱在智能客服中的应用","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://sherwinzhang.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"}]},{"title":"智能客服","slug":"智能客服/07.智能客服","date":"2018-05-13T15:21:18.000Z","updated":"2022-08-25T05:10:22.000Z","comments":true,"path":"智能客服/智能客服/07.智能客服/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/07.%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/","excerpt":"","text":"智能客服十张图解读我国智能客服行业发展现状与前景AI前沿 | 如何让智能客服更有温度？不是所有的智能机器人都能做好客服——浅谈智能客服机器人评价指标新趋势能感知情绪的 IBM 机器人，正打算消灭人工在线客服项目实战｜智能客服（“七鱼”、“小i机器人”）产品分析关于券商智能客服，那些绕不开的坑你问我答之「智能客服评价体系全解读」追一科技券商AI沙龙：智能呼叫的价值落地当Elasticsearch遇见智能客服机器人详解第二代客服机器人 | 聚焦问题解决，客户服务任务一站直达项目实战｜智能客服（“七鱼”、“小i机器人”）产品分析国内的智能客服发展到哪一步了？这里有份追踪报告","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"问答系统","slug":"智能客服/06.问答系统","date":"2018-05-12T15:21:18.000Z","updated":"2022-08-28T14:16:37.882Z","comments":true,"path":"智能客服/智能客服/06.问答系统/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/06.%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"问答系统【问答系统】QA问答系统(Question Answering)揭开知识库问答KB-QA的面纱1·简介篇ROCLING 2019|基于深度语义匹配，上下文相关的问答系统ACL 2019 开源论文 | 基于知识库和大规模网络文本的问答系统","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"问答系统","slug":"问答系统","permalink":"http://sherwinzhang.com/tags/%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"}]},{"title":"对话生成","slug":"智能客服/05.对话生成","date":"2018-05-11T15:21:18.000Z","updated":"2022-08-28T14:16:57.044Z","comments":true,"path":"智能客服/智能客服/05.对话生成/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/05.%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90/","excerpt":"","text":"对话生成专访 | 三角兽首席科学家王宝勋：热度之下的对话生成重磅 | 阿里巴巴年度技术总结：人工智能在搜索的应用和实践干货 | 论文解读：基于动态词表的对话生成研究","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"对话生成","slug":"对话生成","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90/"}]},{"title":"对话管理","slug":"智能客服/04.对话管理","date":"2018-05-10T15:21:18.000Z","updated":"2022-08-28T14:17:05.816Z","comments":true,"path":"智能客服/智能客服/04.对话管理/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/04.%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86/","excerpt":"","text":"对话管理干货｜深度强化学习在面向任务的对话管理中的应用机器人，我们来聊天吧多轮对话之对话管理(Dialog Management)任务导向型对话系统——对话管理模型研究最新进展-2019-12-26AI LIVE | DSTC 8“基于Schema的对话状态追踪”竞赛冠军方案解读对话状态跟踪中关于上下文信息粒度的详细调查 对话状态跟踪一起来看看最新的对话状态追踪(DST)模型最新研究|Amazon AI技术改善了语音识别和对话状态跟踪大家说 | 任务型对话中的状态跟踪（Dialogue State Tracking）对话平台如何应对多领域和服务切换？基于 Schema 的对话状态追踪解决方案一则【字节跳动-李航】一种按序列进行对话状态跟踪的方法基于深度学习方法的对话状态跟踪综述谷歌：提高长文本对话状态跟踪能力 对话策略学习博客 | 一文看懂任务型对话中的对话策略学习（DPL）基于弱监督演示的对话策略学习AI研习丨张伟男：任务型对话系统中策略优化方法回顾知乎专题端到端的任务型对话(一)任务型对话系统公式建模&amp;&amp;实例说明总结|对话系统中的口语理解技术(SLU)（一）总结|对话系统中的口语理解技术(SLU)（二）总结|对话系统中的口语理解技术(SLU)（三）一文看懂任务型对话系统中的状态追踪（DST） https://mp.weixin.qq.com/s/zI1P_KJiBbA_93fwfX6lwg一文看懂任务型对话中的对话策略学习（DPL）总结|对话系统中的自然语言生成技术（NLG）多轮对话状态追踪（DST）–模型介绍篇","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"对话管理","slug":"对话管理","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86/"}]},{"title":"对话系统","slug":"智能客服/03.对话系统","date":"2018-05-09T15:21:18.000Z","updated":"2022-08-28T14:17:13.641Z","comments":true,"path":"智能客服/智能客服/03.对话系统/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/03.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"对话系统对话系统的开源系统&amp;API&amp;SDK&amp;Framework汇总强化学习在智能对话上的应用最新人机对话系统简略综述对话式交互技术原理及流程揭秘-一文看懂人机对话哈工大张伟男：任务型对话系统业界 | 人机对话评测系列之一：任务驱动多轮对话评测标准最新任务型对话数据集大全NeurlPS 2020 | 简约任务型对话，让对话实现不再繁琐-AI好老师：完善育人系统构架 研发育人对话系统微软与清华开源多领域端到端对话系统集成平台ConvLab，帮助研究人员迅速搭建对话系统多轮对话机器人打造（上篇）：着手设计机器人是如何识别你的意图的？|多轮对话机器人打造（中篇）：话题意图识别下一代对话系统中的关键技术认真的聊一聊对话系统（任务型、检索式、生成式对话论文与工具串讲）通用领域对话问答对话系统原理和实践","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"对话系统","slug":"对话系统","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"}]},{"title":"智能客服综合介绍","slug":"智能客服/00.整体目录","date":"2018-05-08T02:24:21.000Z","updated":"2022-08-28T14:17:34.978Z","comments":true,"path":"智能客服/智能客服/00.整体目录/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/00.%E6%95%B4%E4%BD%93%E7%9B%AE%E5%BD%95/","excerpt":"","text":"00.整体目录","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"企业智能客服介绍","slug":"智能客服/02.企业智能客服介绍","date":"2018-05-08T02:24:21.000Z","updated":"2022-08-24T15:53:04.929Z","comments":true,"path":"智能客服/智能客服/02.企业智能客服介绍/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/02.%E4%BC%81%E4%B8%9A%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"阿里巴巴 小蜜阿里小蜜新一代智能对话开发平台技术解析【文章】分享的内容是“新一代智能对话开发平台”（Dialog Studio），中文名叫对话工厂，是小蜜家族中的一个做多轮任务型对话开发产品，目标是以平台的方式赋能更多开发者，让所有的对话者能创造自己的多轮对话机器人。大概分为四个部分，主要包括：为什么做平台？“新一代”对话开发平台“一站式”对话开发平台业务应用阿里小蜜智能对话开发平台的技术探索与场景实践【视频】阿里小蜜：知识结构化推动智能客服升级【文章】介绍阿里小蜜在知识图谱问答（结构化智能问答）方面的工作，基本思路是：通过知识结构化，解决现有智能客服系统在知识管理和语言理解上的痛点，为客户带来高效复用、精准理解、精细管理等方面的显著收益。知识结构化在阿里小蜜中的应用【文章】阿里小蜜体系中知识的结构化以及在实际场景中的应用。本文的主要内容包括以下几点：阿里小蜜的简介知识的结构化基于结构化知识的应用：KBQA 和 EBQA展望和挑战阿里云小蜜对话机器人背后的核心算法对小样本下的语言理解、用户模拟器和基于模型的对话管理的算法研究和落地介绍。对话系统简介自然语言理解平台视角下的对话管理用户模拟器大中台、小前台，阿里小蜜这样突破对话机器人发展瓶颈机器如何猜你所想？阿里小蜜预测平台揭秘阿里小蜜：智能服务技术实践及场景探索云小蜜：在中国移动的落地实践阿里小蜜这一年，经历了哪些技术变迁？论文导读 | 阿里小蜜背后的技术秘密拆开阿里小蜜的内核，看智能人机交互的实现逻辑揭秘阿里小蜜：基于检索模型和生成模型相结合的聊天引擎 | PaperDaily #25干货 | 阿里小蜜-电商领域的智能助理技术实践多语言阿里小蜜——七步构建跨越语言鸿沟的对话机器人阿里小蜜中的情绪回复能力什么是人机对话模型？阿里小蜜团队写了1.5万字数十名工程师作战5天，阿里达摩院连夜研发智能疫情机器人回顾·阿里神马智能对话问答客服场景下的智能化实践通用领域对话问答 非技术推广小蜜家族知多少：人工智能客服如何做到“不智障”？售后智能客服：店小蜜用户体验地图 其他为减少用户电话排队，阿里研发了智能客服调度系统阿里千亿级购物节背后，淘宝智能客服架构演进之路（ps:偏在线客服）资源 | 从搜索到智能客服：阿里开放强化学习技术演进与实践书籍首次披露！阿里线下智能方案进化史 天猫深度揭秘天猫精灵对话引擎天猫精灵如何和我们聊天？ 蚂蚁支付宝换上“读心术”AI客服，就为配合双11剁手的你专访 | 蚂蚁金服MISA：比用户更懂自己的自然语言客服系统COPC高管访谈：蚂蚁金服客服服务及权益保障事业部总监，智能客服业务负责人 —— 丁翌先生蚂蚁金服-“新客服”白皮书（附下载）（支付宝宣传）当人工智能遇上客服，他们是怎样让科幻变成现实？ 闲鱼我其实一直都懂你|闲鱼聊天机器人 携程干货 | “猜你所想，答你所问”，携程智能客服算法实践携程：上万坐席呼叫中心异地双活架构及系统设计携程呼叫中心异地双活——座席服务的高可用AI 在携程智能客服的应用携程度假智能云客服平台携程基于云的软呼叫中心及客服平台架构实践机器学习在酒店呼叫中心自动化中的应用干货 | 携程度假智能客服机器人背后是这么玩的携程“小诗机”背后的机器学习和自然语言处理技术行业智能客服构建探索干货 | 揭秘携程基于融合通讯技术的新一代客服系统干货 | 为了给你更好的体验，携程做了个“一站式”客服机器人干货 | 每天上百万通话，携程电话系统性能测试实践 京东“天枢”智能调度系统，让京东专属客服与您“一拍即合”干货 | 京东JIMI用户未来意图预测技术揭秘京东揭秘 | 技术方案解答智能客服如何双商俱高京东 618：智能机器人 JIMI 的进击之路揭秘 | 技术方案解答智能客服如何双商俱高京东JIMI机器人累计服务上亿用户 开放平台共享人JIMI：用深度学习搞定80%的客服工作京东JIMI 技术架构开放的JIMI，开放式架构PPT|智能客服机器人在售前导购场景中的应用实践 58同城58智能客服QABot问答机器人算法实践回顾·五八同城智能客服系统“帮帮”技术揭秘 ppt 提取码 z6i0 视频提取码: z6i0干货回顾 |58同城—智能语音机器人助力企业提效增收（含视频）58统称智能语音机器人后端架构解析58 同城对话机器人应用实践：本地生活服务场景中的商家智能助手沙龙干货｜基于深度学习的自动问答工具——qa_match开源项目解析直播回顾集锦人机语音对话技术在58同城的应用实践_AI_DataFunTalk_InfoQ精选文章58同城对话机器人应用实践 视频版技术沙龙回顾|AI技术如何打造58同城智能客服商家版“微聊管家”干货总结 | AI技术如何打造智能语音质检系统语音机器人SIP通话场景下如何捕获用户电话按键信号58同城AI Lab技术沙龙 美团「回顾」智变中的美团客服美团对话理解技术及实践基于知识图谱的问答在美团智能交互场景中的应用和演进美团智能客服核心技术与实践 视频版 滴滴智能机器人在滴滴出行场景的技术探索滴滴KDD 2019 论文详解：基于深度学习自动生成客服对话KDD 2019 滴滴论文解读 | 基于深度学习自动生成客服对话摘要智能客服渗透叫车平台易到、滴滴、首汽哪家的客服更聪明？ OPPOQCon-小布助手对话系统工程实践_OPPO数智技术_InfoQ写作社区 视频版 提取码: z6i0对话系统简介与OPPO小布助手的工程实践_人工智能_OPPO小布助手_InfoQ写作社区OPPO小布助手算法系统的探索、实践与思考_人工智能_OPPO小布助手_InfoQ写作社区对话交互：封闭域任务型与开放域闲聊算法技术_人工智能_OPPO小布助手_InfoQ写作社区OPPO小布助手算法系统探索、实践与思考_算法_OPPO数智技术_InfoQ写作社区 腾讯腾讯知文问答引擎在工业级实战中的演化从问答系统到 AI 中间件：云上智能机器人构建之路 云问（拼多多、当当）云问智能客服 电商盛宴的“神助攻”！ 去哪儿智能客服系统在机票售后的应用实践Qunar智能售后服务机器人 平安AI多轮人机对话与对话管理技术探索与实践 Uber从数据预处理到排序算法，全方位解读 Uber 人工智能客服 COTA 饿了么饿了么客服体系 | 虽然天天见，我们却对它一无所知机器学习算法在饿了么供需平衡系统中的应用饿了么客服系统架构演进之路 瓜子瓜子二手车封宇：瓜子IM智能客服系统数据架构设计回顾·对话机器人在瓜子的实践 平安银行平安银行智能金融在客服机器人中台的落地实践平安人寿智能团队：智能问答系统的探索与实践 小冰微软小冰对话机器人架构独家 | 专访微软小冰负责人李笛：智能助手是创造需求，而非仅提高效率聊天机器人语言理解模块开发实践 苏宁苏宁智能机器人平台 贝壳【WOT峰会回顾】人工智能和人工冰释前嫌贝壳智能客服系统的构建与算法迭代 第四范式15年研发经验博士手把手教学：从零开始搭建智能客服 网易深度学习在网易严选智能客服中的应用]( 智能一点回顾·开源节流的智能导购对话机器人实践","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]},{"title":"智能客服入门","slug":"智能客服/01.智能客服入门","date":"2018-05-06T02:24:21.000Z","updated":"2022-08-28T14:15:46.050Z","comments":true,"path":"智能客服/智能客服/01.智能客服入门/","link":"","permalink":"http://sherwinzhang.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/01.%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E5%85%A5%E9%97%A8/","excerpt":"","text":"资料汇总聊天机器人资料汇总英文对话系统汇总必读论文 | 20 篇聊天机器人领域必读论文速递！MSRA周明博士解读：聊天机器人的三大引擎（视频+PPT）【专知荟萃03】知识图谱KG知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载）【专知荟萃04】自动问答QA知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载）【专知荟萃05】聊天机器人Chatbot知识资料全集（入门/进阶/论文/软件/数据/专家等）(附pdf下载)聊天机器人语言理解模型开发实践","categories":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}]}],"categories":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/categories/chatGPT/"},{"name":"计算机相关","slug":"计算机相关","permalink":"http://sherwinzhang.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://sherwinzhang.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"程序人生","slug":"程序人生","permalink":"http://sherwinzhang.com/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sherwinzhang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/categories/python/"},{"name":"AI综合","slug":"AI综合","permalink":"http://sherwinzhang.com/categories/AI%E7%BB%BC%E5%90%88/"},{"name":"apple生态","slug":"apple生态","permalink":"http://sherwinzhang.com/categories/apple%E7%94%9F%E6%80%81/"},{"name":"机器学习","slug":"机器学习","permalink":"http://sherwinzhang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/categories/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"}],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"http://sherwinzhang.com/tags/chatGPT/"},{"name":"Linux","slug":"Linux","permalink":"http://sherwinzhang.com/tags/Linux/"},{"name":"AI","slug":"AI","permalink":"http://sherwinzhang.com/tags/AI/"},{"name":"RS","slug":"RS","permalink":"http://sherwinzhang.com/tags/RS/"},{"name":"随感","slug":"随感","permalink":"http://sherwinzhang.com/tags/%E9%9A%8F%E6%84%9F/"},{"name":"NLP","slug":"NLP","permalink":"http://sherwinzhang.com/tags/NLP/"},{"name":"python","slug":"python","permalink":"http://sherwinzhang.com/tags/python/"},{"name":"software","slug":"software","permalink":"http://sherwinzhang.com/tags/software/"},{"name":"ML","slug":"ML","permalink":"http://sherwinzhang.com/tags/ML/"},{"name":"智能客服","slug":"智能客服","permalink":"http://sherwinzhang.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"},{"name":"知识图谱","slug":"知识图谱","permalink":"http://sherwinzhang.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"问答系统","slug":"问答系统","permalink":"http://sherwinzhang.com/tags/%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"},{"name":"对话生成","slug":"对话生成","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90/"},{"name":"对话管理","slug":"对话管理","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86/"},{"name":"对话系统","slug":"对话系统","permalink":"http://sherwinzhang.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"}]}