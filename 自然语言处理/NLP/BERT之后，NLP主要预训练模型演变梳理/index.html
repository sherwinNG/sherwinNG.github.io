<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><meta name="google-site-verification" content="5CE3B39kLm7q56d4fGxSbLoYsfIupDQFY4MtYXCWIXY"><title>BERT之后，NLP主要预训练模型演变梳理 | sherwinNG&#39;s blog</title><meta name="description" content="BERT之后，NLP主要预训练模型演变梳理 1.背景BERT [1]（Bidirectional Encoder Representation from Transformers）是由 Google AI 于 2018 年 10 月提出的一种基于深度学习的语言表示模型。BERT 发布时，在 11 种不同的自然语言处理（NLP）测试任务中取得最佳效果。截至2022年6月，很多研究者又基于BERT提"><meta property="og:type" content="article"><meta property="og:title" content="BERT之后，NLP主要预训练模型演变梳理"><meta property="og:url" content="http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/index.html"><meta property="og:site_name" content="sherwinNG&#39;s blog"><meta property="og:description" content="BERT之后，NLP主要预训练模型演变梳理 1.背景BERT [1]（Bidirectional Encoder Representation from Transformers）是由 Google AI 于 2018 年 10 月提出的一种基于深度学习的语言表示模型。BERT 发布时，在 11 种不同的自然语言处理（NLP）测试任务中取得最佳效果。截至2022年6月，很多研究者又基于BERT提"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fvn5eaajj20rn0b4761.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fvzus0t0j20pw0df75d.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fw2my1gaj20r703xdgs.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fywvod1fj20vq0a076l.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fzovky2hj20mr13pjwp.jpg"><meta property="article:published_time" content="2022-06-20T13:20:21.000Z"><meta property="article:modified_time" content="2022-08-10T14:36:48.168Z"><meta property="article:author" content="sherwin"><meta property="article:tag" content="NLP"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fvn5eaajj20rn0b4761.jpg"><link rel="canonical" href="http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/index.html"><link rel="alternate" href="/atom.xml" title="sherwinNG&#39;s blog" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1.4.0/dist/gitalk.min.css"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?4931527b0d51b34dfd6b53672f84e252",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><meta name="generator" content="Hexo 4.2.1"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://sherwinzhang.com/about/" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">sherwin</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">遇见更好的自己</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Beijing, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav menu-highlight"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/sherwinNG" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI%E7%BB%BC%E5%90%88/">AI综合</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/apple%E7%94%9F%E6%80%81/">apple生态</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/">程序人生</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/">计算机相关</a><span class="category-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/" rel="tag">ML</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/software/" rel="tag">software</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E6%84%9F/" rel="tag">随感</a><span class="tag-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/AI/" style="font-size:13.67px">AI</a> <a href="/tags/Linux/" style="font-size:13.33px">Linux</a> <a href="/tags/ML/" style="font-size:14px">ML</a> <a href="/tags/NLP/" style="font-size:13px">NLP</a> <a href="/tags/software/" style="font-size:13px">software</a> <a href="/tags/%E9%9A%8F%E6%84%9F/" style="font-size:13.33px">随感</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">二月 2019</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p><p class="item-title"><a href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/" class="title">BERT之后，NLP主要预训练模型演变梳理</a></p><p class="item-date"><time datetime="2022-06-20T13:20:21.000Z" itemprop="datePublished">2022-06-20</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/AI%E7%BB%BC%E5%90%88/">AI综合</a></p><p class="item-title"><a href="/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-fastapi%5B3-3%5D/" class="title">AI模型部署-fastapi[3-3]</a></p><p class="item-date"><time datetime="2022-06-07T13:34:29.000Z" itemprop="datePublished">2022-06-07</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/AI%E7%BB%BC%E5%90%88/">AI综合</a></p><p class="item-title"><a href="/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask%5B3-2%5D/" class="title">AI模型部署-flask[3-2]</a></p><p class="item-date"><time datetime="2022-06-07T13:04:29.000Z" itemprop="datePublished">2022-06-07</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/AI%E7%BB%BC%E5%90%88/">AI综合</a></p><p class="item-title"><a href="/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-flask%20%E5%92%8Cfastapi%20%E5%AF%B9%E6%AF%94%5B3-1%5D/" class="title">AI模型部署-flask和fastapi对比[3-1]</a></p><p class="item-date"><time datetime="2022-06-07T12:44:29.000Z" itemprop="datePublished">2022-06-07</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/apple%E7%94%9F%E6%80%81/">apple生态</a></p><p class="item-title"><a href="/apple%E7%94%9F%E6%80%81/apple%E7%94%9F%E6%80%81/%E7%9B%AE%E5%89%8D%E4%BD%BF%E7%94%A8%E4%B8%8D%E9%94%99%E8%BD%AF%E4%BB%B6%E6%95%B4%E7%90%86/" class="title">目前使用不错软件整理</a></p><p class="item-date"><time datetime="2022-06-03T08:44:34.000Z" itemprop="datePublished">2022-06-03</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert之后nlp主要预训练模型演变梳理"><span class="toc-number">1.</span> <span class="toc-text">BERT之后，NLP主要预训练模型演变梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1背景"><span class="toc-number">1.1.</span> <span class="toc-text">1.背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2bert基本介绍"><span class="toc-number">1.2.</span> <span class="toc-text">2.BERT基本介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3ernie"><span class="toc-number">1.3.</span> <span class="toc-text">3.ERNIE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4roberta"><span class="toc-number">1.4.</span> <span class="toc-text">4.RoBERTa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5deberta"><span class="toc-number">1.5.</span> <span class="toc-text">5.DeBERTa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6综述"><span class="toc-number">1.6.</span> <span class="toc-text">6.综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7参考资料"><span class="toc-number">1.7.</span> <span class="toc-text">7.参考资料</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#附件"><span class="toc-number">1.8.</span> <span class="toc-text">附件</span></a></li></ol></li></ol></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-NLP/BERT之后，NLP主要预训练模型演变梳理" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">BERT之后，NLP主要预训练模型演变梳理</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/" class="article-date"><time datetime="2022-06-20T13:20:21.000Z" itemprop="datePublished">2022-06-20</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/NLP/" rel="tag">NLP</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span></span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 2.5k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 8(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><h1 id="bert之后nlp主要预训练模型演变梳理"><a class="markdownIt-Anchor" href="#bert之后nlp主要预训练模型演变梳理"></a> BERT之后，NLP主要预训练模型演变梳理</h1><h2 id="1背景"><a class="markdownIt-Anchor" href="#1背景"></a> 1.背景</h2><p>BERT [1]（Bidirectional Encoder Representation from Transformers）是由 Google AI 于 2018 年 10 月提出的一种基于深度学习的语言表示模型。BERT 发布时，在 11 种不同的自然语言处理（NLP）测试任务中取得最佳效果。</p><p>截至2022年6月，很多研究者又基于BERT提出了很多新的模型，本文旨在梳理基于BERT模型优化后部分预训练模型，以便读者能够更快掌握BERT相关内容，为后期工作中使用BERT相关模型提供便捷性。</p><h2 id="2bert基本介绍"><a class="markdownIt-Anchor" href="#2bert基本介绍"></a> 2.BERT基本介绍</h2><p>BERT 主要的模型结构是 Transformer 的编码器部分。Transformer[2] 是由 Ashish 等于 2017年提出的，用于Google机器翻译，包含编码器（Encoder）和解码器（Decoder） 两部分。其中 BERT-base 与 BERT-large 模型分别采用了 12 层与 24 层的 Transformer 编码器作为模型网络层。相比于传统用于 NLP 任务的循环神经网络 （RNN）及长短时记忆网络（LSTM）等，Transformer 拥有更强大的文本编码能力，也能更高效地利用 GPU 等高性能设备完成大规模训练工作。</p><p>基于 BERT 模型的自然语言处理任务通过两个过程来实现：</p><ul><li>在预训练的过程中，首先利用大规模没有标注过的文本语料，例如百科知识、网页新闻等，通过充分的自监督训练，有效学习文本的语言特征，得到深层次的文本向量表示，形成相应文本的预训练模型。</li><li>在微调过程中，直接将预训练过程中完成收敛的网络参数（即嵌入层和网络层）作为起始模型，根据具体的下游任务（如分类、序列标注等），输入人工标注好的数据集，完成模型的进一步拟合与收敛。从而得到一个可用的深度学习模型来实现特定的自然语言处理任务，例如文本分类、序列标注等。</li></ul><p>自 BERT 发布以来，基于“预训练-微调”的两阶段方法逐渐成为自然语言处理研究的主流。</p><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fvn5eaajj20rn0b4761.jpg" alt="image-20220621150352751"></p><blockquote><p>图片来源：参考文献[1]</p></blockquote><h2 id="3ernie"><a class="markdownIt-Anchor" href="#3ernie"></a> 3.ERNIE</h2><p>ERNIE[3]（Enhanced Representation through Knowledge Integration）是百度（清华几乎在同一时间[2019]也发布了ERNIE版本，不过现在社会上谈起ERNIE，大多指百度版ERNIE）在2019年4月基于BERT模型做的进一步优化，在中文的NLP任务上得到了state-of-the-art的结果。其主要是通过对知识进行整合，达到增强表达的目的。受BERT的掩码策略启发，ERNIE旨在学习由知识掩码策略增强的语言表征，其中包括实体级掩码和短语级掩码。实体级策略通常由多个单词组成的实体。短语级策略将由多个单词组成的整个短语作为一个概念单元进行屏蔽。</p><p>ERNIE和BERT的区别：<br><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fvzus0t0j20pw0df75d.jpg" alt="image-20220621151607598"></p><blockquote><p>图片来源：参考文献[3]</p></blockquote><p>在使用先验知识来增强预训练语言模型时ERNIE并没有直接添加知识嵌入，而是使用了一种多阶段知识掩码策略，将短语和实体集成到语言表示中。句子中不同的掩码级别如下图：<br><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fw2my1gaj20r703xdgs.jpg" alt="image-20220621151848304"></p><blockquote><p>图片来源：参考文献[3]</p></blockquote><ul><li><p>基本级别的掩码</p><ul><li>第一个学习阶段是使用基本的掩码，它将一个句子视为一系列基本的语言单元，对于英语，基本的语言单元是单词，对于汉语，基本的语言单元是汉字。在训练过程中，我们随机屏蔽15%的基本语言单元，并使用句子中的其他基本单元作为输入，并训练一个转换器来预测被屏蔽的单元。基于基本层掩码，我们可以得到基本的单词表示。因为它是在基本语义单元随机掩码的基础上训练的，所以高层语义的知识表示很难完全建模。</li></ul></li><li><p>短语级别的掩码</p><ul><li>第二阶段是使用短语级掩码。短语是作为概念单位的一小群单词或字符。对于英语，我们使用词汇分析和组块工具来获取句子中短语的边界，并使用一些依赖于语言的切分工具来获取其他语言（如汉语）中的单词/短语信息。在短语级掩码阶段，我们还使用基本语言单元作为训练输入，不像随机基本单元掩码那样，这次我们在句子中多选几个短语，掩码并预测同一短语中的所有基本单元。在这个阶段，短语信息被编码到单词嵌入中。</li></ul></li><li><p>实体级别的掩码</p><ul><li>第三阶段是实体级掩码。我们把一些专有名词，如地点、人名、组织、产品等抽象为实体进行屏蔽。实体通常包含句子中的重要信息。在短语屏蔽阶段，我们首先分析一个句子中的命名实体，然后屏蔽和预测实体中所有的空缺。</li></ul></li></ul><p>经过这三个阶段的学习，我们可以获得语义信息丰富的表达。</p><p>经过实验结果表明，在五个自然语言处理任务（包括自然语言推理，语义相似性，命名实体识别，情感分析和检索问答）上，ERNIE优于当时其他基准方法。此外ERNIE在完形填空测试中具有更强大的知识推理能力。</p><h2 id="4roberta"><a class="markdownIt-Anchor" href="#4roberta"></a> 4.RoBERTa</h2><p>RoBERTa[4]是Facebook和华盛顿大学于2019年7月在论文《RoBERTa: A Robustly Optimized BERT Pretraining Approach》中提出的。文章在 BERT模型的基础上提出了 BERT 模型的改进版 RoBERTa，使其获得了更好的自然语言任务处理效果，并在 GLUE，SQuAD，RACE 三个榜上取得最好的SOTA。</p><p>RoBERTa主要在<strong>三方面</strong>对之前提出的BERT做了改进：</p><ul><li><strong>其一</strong>是模型的具体细节层面，改进了优化函数；</li><li><strong>其二</strong>是训练策略层面，改用了动态掩码的方式训练模型，证明了NSP（Next Sentence Prediction）训练策略的不足，采用了更大的batch size；</li><li><strong>其三</strong>是数据层面，一方面使用了更大的数据集，另一方面是使用字节级别的BPE（Bytes-level BEP ）来处理文本数据。</li></ul><h2 id="5deberta"><a class="markdownIt-Anchor" href="#5deberta"></a> 5.DeBERTa</h2><p>DeBERTa[5]（Decoding-enhanced BERT with Disentangled Attention）是微软发表于ICLR2021上的预训练语言模型。2021年1月DeBERTa在SuperGLUE这项自然语言理解基准任务上**「超越人类」**，以90.3分夺冠。</p><p>DeBERTa从两方面改进了BERT预训练的方法：</p><ul><li>自注意力**「解耦」**机制<ul><li>用2个向量分别表示content 和 position，即word本身的文本内容和位置。word之间的注意力权重则使用word内容之间和位置之间的解耦矩阵。这是因为word之间的注意力不仅取决于其文本内容，还依赖于两者的相对位置。</li></ul></li></ul><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fywvod1fj20vq0a076l.jpg" alt="image-20220621165704274"></p><blockquote><p>图片来源：参考文献[6]</p></blockquote><ul><li>用Enhanced Mask Decoder(<strong>「EMD」</strong>)强化预训练输出层<ul><li>原始的BERT存在预训练和微调不一致问题。预训练阶段，隐层最终的输出输入到softmax预测被mask掉的token，而微调阶段则是将隐层最终输出输入到特定任务的decoder。这个decoder根据具体任务不同可能是一个或多个特定的decoder，如果输出是概率，那么还需要加上一个softmax层。为消除这种不一致性，DeBERTa将MLM与其他下游任务同等对待，并将原始BERT中输出层的softmax替换为**「增强后的mask decoder(EMD)」**，EMD包含一个或多个Transformer层和一个softmax输出层。至此，结合了BERT和EMD的DeBERTa成为了一个encoder-decoder模型。</li></ul></li></ul><p>使用这两种技术，新的预训练语言模型DeBERTa在许多下游NLP任务上的表现都优于RoBERTa和BERT。DeBERTa这项工作展示了探索自注意的词表征解耦以及使用任务特定解码器改进预训练语言模型的潜力。</p><h2 id="6综述"><a class="markdownIt-Anchor" href="#6综述"></a> 6.综述</h2><p>本文针对BERT系列部分典型模型进行梳理，希望为大家梳理出在BERT提出后，整体的优化脉络。同时基于BERT的优化方向可以总结为如下：</p><ul><li><p>首先，大量的研究者通过对 BERT 的两个预训练目标进行改进提升模型对文本特征的学习能力，如：ERNIE、RoBERTa、DeBERTa等。对于预训练目标的优化改进是最常见同时也是效果最好的改造方式，所以本文在前面介绍中，也主要梳理了该方向的主要模型。</p></li><li><p>其次，针对特定领域的显性知识，研究者提出在预训练模型中融合外部知识的方法，进一步丰富了模型所学习的文本特征，如用于专利文本的 PatentBERT：。</p></li></ul><p>这两种路线提升了模型的特征学习能力，但是并没有对预训练模型内部结构进行实质性的改进。</p><ul><li><p>部分研究者从 Transformer 神经网络出发，对其内部结构进行了改进，从而扩展了模型的应用场景，如：BART。</p></li><li><p>最后，针对 BERT 模型参数量过大导致普通的硬件设备无法有效训练和加载的问题，大量的研究者提出模型压缩的方法，进而提升了 BERT 模型的易用性，如：ALBERT。</p></li></ul><h2 id="7参考资料"><a class="markdownIt-Anchor" href="#7参考资料"></a> 7.参考资料</h2><p>bert [1] <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>transformer[2] <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.03762.pdf</a></p><p>ernie [3] <a href="https://arxiv.org/pdf/1904.09223.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09223.pdf</a></p><p>roberta [4] <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.11692.pdf</a></p><p>deberta[5] <a href="https://arxiv.org/pdf/2006.03654.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2006.03654.pdf</a></p><p>deberta[6] <a href="https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_ca" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/?lang=fr_ca</a></p><hr><hr><h2 id="附件"><a class="markdownIt-Anchor" href="#附件"></a> 附件</h2><p>BERT模型优化改进路线总结：</p><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3fzovky2hj20mr13pjwp.jpg" alt="image-20220621172319969"></p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://sherwinzhang.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/BERT%E4%B9%8B%E5%90%8E%EF%BC%8CNLP%E4%B8%BB%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%BC%94%E5%8F%98%E6%A2%B3%E7%90%86/" title="BERT之后，NLP主要预训练模型演变梳理" target="_blank" rel="external">http://sherwinzhang.com/自然语言处理/NLP/BERT之后，NLP主要预训练模型演变梳理/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://sherwinzhang.com/about/" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://sherwinzhang.com/about/" target="_blank"><span class="text-dark">sherwin</span><small class="ml-1x">遇见更好的自己</small></a></h3><div>布道AI。</div></div></figure></div></div></div></article><section id="comments"></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="next"><a href="/AI%E7%BB%BC%E5%90%88/AI%E7%BB%BC%E5%90%88/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-fastapi%5B3-3%5D/" title="AI模型部署-fastapi[3-3]"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,wechat,facebook,twitter" data-mobile-sites="weibo,wechat"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/sherwinNG" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li></ul><div class="copyright">&copy; 2022 sherwin<div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script src="//cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"fc07e4b629725a40a42a",clientSecret:"878e913fac9e1949c0a62f732a1a077338f66cae",repo:"sherwinNG.github.io",owner:"sherwinNG",admin:["sherwinNG"],id:md5(location.pathname),distractionFreeMode:!0,language:"zh-CN",enableHotKey:"true"});gitalk.render("comments")</script></body></html>